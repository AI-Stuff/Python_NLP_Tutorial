{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text mining (nlp) with python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Ties de Kok ([Personal Website](http://www.tiesdekok.com))  \n",
    "**Last updated:** 30 Oct 2017  \n",
    "**Python version:** Python 3.5  \n",
    "**License:** MIT License  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Some features (like the ToC) will only work if you run the notebook or if you use nbviewer by clicking this link: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Introduction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code examples to get you started with Natural Language Processing (NLP) / Text Mining for Research and Data Science purposes.  \n",
    "\n",
    "In the large scheme of things there are roughly 4 steps:  \n",
    "\n",
    "1. Identify a data source  \n",
    "2. Gather the data  \n",
    "3. Process the data  \n",
    "4. Analyze the data  \n",
    "\n",
    "This notebook only discusses step 3 and 4. If you want to learn more about step 2 see my [Python tutorial](https://github.com/TiesdeKok/LearnPythonforResearch). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: companion slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was designed to accompany a PhD course session on NLP techniques in Accounting Research.  \n",
    "The slides of this session are publically availabe here: [Slides](http://www.tiesdekok.com/AccountingNLP_Slides/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Elements / topics that are discussed in this notebook: *\n",
    "\n",
    "\n",
    "<img style=\"float: left\" src=\"https://i.imgur.com/c3aCZLA.png\" width=\"50%\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Table of Contents*  <a id='toc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Primer on NLP tools](#tool_primer)     \n",
    "* [Process + Clean text](#proc_clean)   \n",
    "    * [Normalization](#normalization)\n",
    "        * [Deal with unwanted characters](#unwanted_char)\n",
    "        * [Sentence segmentation](#sentence_seg)   \n",
    "        * [Word tokenization](#word_token)\n",
    "        * [Lemmatization & Stemming](#lem_and_stem) \n",
    "    * [Language modeling](#lang_model) \n",
    "        * [Part-of-Speech tagging](#pos_tagging) \n",
    "        * [Uni-Gram & N-Grams](#n_grams) \n",
    "        * [Stop words](#stop_words) \n",
    "* [Direct feature extraction](#feature_extract) \n",
    "    * [Feature search](#feature_search) \n",
    "        * [Entity recognition](#entity_recognition) \n",
    "        * [Pattern search](#pattern_search) \n",
    "    * [Text evaluation](#text_eval) \n",
    "        * [Language](#language) \n",
    "        * [Dictionary counting](#dict_counting) \n",
    "        * [Readability](#readability) \n",
    "* [Represent text numerically](#text_numerical) \n",
    "    * [Bag of Words](#bows) \n",
    "        * [TF-IDF](#tfidf) \n",
    "    * [Word Embeddings](#word_embed) \n",
    "        * [Word2Vec](#Word2Vec) \n",
    "* [Statistical models](#stat_models) \n",
    "    * [\"Traditional\" machine learning](#trad_ml) \n",
    "        * [Supervised](#trad_ml_supervised) \n",
    "            * [Na√Øve Bayes](#trad_ml_supervised_nb) \n",
    "            * [Support Vector Machines (SVM)](#trad_ml_supervised_svm) \n",
    "        * [Unsupervised](#trad_ml_unsupervised) \n",
    "            * [Latent Dirichilet Allocation (LDA)](#trad_ml_unsupervised_lda) \n",
    "            * [pyLDAvis](#trad_ml_unsupervised_pyLDAvis) \n",
    "* [Model Selection and Evaluation](#trad_ml_eval) \n",
    "* [Neural Networks](#nn_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Primer on NLP tools</span><a id='tool_primer'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many tools available for NLP purposes.  \n",
    "The code examples below are based on what I personally like to use, it is not intended to be a comprehsnive overview.  \n",
    "\n",
    "Besides build-in Python functionality I will use / demonstrate the following packages:\n",
    "\n",
    "**Standard NLP libraries**:\n",
    "1. `Spacy` and the higher-level wrapper `Textacy` \n",
    "2. `NLTK` and the higher-level wrapper `TextBlob`\n",
    "\n",
    "*Note: besides installing the above packages you also often have to download (model) data . Make sure to check the documentation!*\n",
    "\n",
    "**Standard machine learning library**:\n",
    "\n",
    "1. `scikit learn`\n",
    "\n",
    "**Specific task libraries**:\n",
    "\n",
    "There are many, just a couple of examples:\n",
    "\n",
    "1. `pyLDAvis` for visualizing LDA)\n",
    "2. `langdetect` for detecting languages\n",
    "3. `fuzzywuzzy` for fuzzy text matching\n",
    "4. `textstat` to calculate readability statistics\n",
    "5. `Gensim` for topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Get some example data</span><a id='example_data'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many example datasets available to play around with, see for example this great repository:  \n",
    "https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that I will use for most of the examples is the \"Reuter_50_50 Data Set\" that is used for author identification experiments. \n",
    "\n",
    "See the details here: https://archive.ics.uci.edu/ml/datasets/Reuter_50_50  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't follow what I am doing here? Please see my [Python tutorial](https://github.com/TiesdeKok/LearnPythonforResearch) (although the `zipfile` and `io` operations are not very relevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, zipfile, io, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Download and extract the zip file with the data *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('C50test'):\n",
    "    r = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\")\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load the data into memory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_dict = {'test' : 'C50test', 'train' : 'C50train'}\n",
    "text_dict = {'test' : {}, 'train' : {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for label, folder in folder_dict.items():\n",
    "    authors = os.listdir(folder)\n",
    "    for author in authors:\n",
    "        text_files = os.listdir(os.path.join(folder, author))\n",
    "        for file in text_files:\n",
    "            with open(os.path.join(folder, author, file), 'r') as text_file:\n",
    "                text_dict[label].setdefault(author, []).append(' '.join(text_file.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the text comes pre-split per sentence, for the sake of example I undo this through `' '.join(text_file.readlines()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain\\'s Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997. The shares fell 6p to 781p on the news.\\n \"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers.  \\n Dermott Carr, an analyst at Nikko said, \"the market is going to hang onto them for the moment but until we get a decision they will be held back.\"\\n Whatever the MMC decides many analysts expect Lang to defer a decision until after the next general election which will be called by May 22.\\n \"They will probably try to defer the decision until after the election. I don\\'t think they want the negative PR of having a large number of people fired,\" said Wakley.  \\n If the deal does not go through, analysts calculate the maximum loss to Bass of 60 million, with most sums centred on the 30-40 million range.\\n \"It\\'s a maxiumum loss of 60 million for Bass if they fail and, unlike Allied, you would have to compare it to the perceived upside of doing the deal,\" said Wakley.\\n Bass said at the time of the deal it would take a one-off charge of 75 million stg for restructuring the combined business, resulting in expected annual cost savings of 90 million stg within three years.  \\n Under the terms of the complex deal, if Bass cannot combine C-T with its own brewing business within 16 months, it has the option to put its whole shareholding to Carlsberg for 110 million stg and Carlsberg has an option to put 15 percent of C-T to Allied Domecq, which would reimburse Bass 30 million stg.\\n Bass is also entitled to receive 50 percent of all profits earnied by C-T until the merger is complete, which should give it some 30-35 million stg in a full year. Carlsberg has agreed to contribute its interests and 20 million stg in exchange for a 20 percent share in the combined Bass Breweries and Carlsberg-Tetley business.\\n C-T was a joint venture between Allied Domecq and Carlsberg formed in 1992 by the merger of their UK brewing and wholesaleing businesses.\\n -- London Newsroom +44 171 542 6437\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Process + Clean text</span><a id='proc_clean'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the text into a NLP representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the text directly, but if want to use packages like `spacy` and `textblob` we first have to convert the text into a corresponding object.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all text in the \"test\" sample to a `spacy` `doc` object using `parser()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_text = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    spacy_text[author] = [parser(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spacy_text['TimFarrand'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply basic `nltk` operations directly to the text so we don't need to convert first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all text in the \"test\" sample to a `TextBlob` object using `TextBlob()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textblob_text = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    textblob_text[author] = [TextBlob(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(textblob_text['TimFarrand'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Normalization</span><a id='normalization'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text normalization** describes the task of transforming the text into a different (more comparable) form.  \n",
    "\n",
    "This can imply many things, I will show a couple of things below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Deal with unwanted characters</span><a id='unwanted_char'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will often notice that there are characters that you don't want in your text.  \n",
    "\n",
    "Let's look at this sentence for example:\n",
    "\n",
    "> \"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain\\'s Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\"\n",
    "\n",
    "You notice that there are some `\\` and `\\n` in there. These are used to define how a string should be displayed, if we print this text we get:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
      " Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\n"
     ]
    }
   ],
   "source": [
    "print(text_dict['test']['TimFarrand'][0][:298])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to analyze text we often don't care about the visual representation. They might actually cause problems!  \n",
    "\n",
    "** So how do we remove them? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases it is sufficient to simply use the `.replace()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts. Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:298].replace('\\n', '').replace('\\\\', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, however, the problem arrises because of encoding / decoding problems.  \n",
    "\n",
    "In those cases you can usually do something like:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"This is some  text that has to be cleaned! it's annoying!\"\n"
     ]
    }
   ],
   "source": [
    "problem_sentence = 'This is some \\\\u03c0 text that has to be cleaned\\\\u2026! it\\\\u0027s annoying!'\n",
    "print(problem_sentence.encode().decode('unicode_escape').encode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Sentence segmentation</span><a id='sentence_seg'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation means the task of splitting up the piece of text by sentence.  \n",
    "\n",
    "You could do this by splitting on the `.` symbol, but dots are used in many other cases as well so it is not very robust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts\",\n",
       " '\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997',\n",
       " ' The shares fell 6p to 781p on the news',\n",
       " '\\n \"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers',\n",
       " '  \\n Dermott Carr, an analyst at Nikko said, \"the mark']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:550].split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to use a more sophisticated implementation such as the one by `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_paragraph = spacy_text['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
       "  ,\n",
       " Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997.,\n",
       " The shares fell 6p to 781p on the news.\n",
       "  ,\n",
       " \"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers.  \n",
       "  ,\n",
       " Dermott Carr, an analyst at Nikko said, \"the market is going to hang onto them for the moment but until we get a decision they will be held back.\"\n",
       "  ]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = [s for s in example_paragraph.sents]\n",
    "sentence_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the returned object is still a `spacy` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to all texts (for use later on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_sentences = {}\n",
    "for author, text_list in spacy_text.items():\n",
    "    spacy_sentences[author] = [list(text.sents) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
       "  ,\n",
       " Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997.,\n",
       " The shares fell 6p to 781p on the news.\n",
       "  ]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentences['TimFarrand'][0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Word tokenization</span><a id='word_token'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenization means to split the sentence (or text) up into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
       " "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][0][0]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word is called a `token` in this context (hence `tokenization`), using `spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares,\n",
       " in,\n",
       " brewing,\n",
       " -,\n",
       " to,\n",
       " -,\n",
       " leisure,\n",
       " group,\n",
       " Bass,\n",
       " Plc,\n",
       " are,\n",
       " likely,\n",
       " to,\n",
       " be,\n",
       " held]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [token for token in example_sentence]\n",
    "token_list[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Lemmatization & Stemming</span><a id='lem_and_stem'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you want to convert a word (i.e. token) into a more general representation.  \n",
    "\n",
    "For example: convert \"car\", \"cars\", \"car's\", \"cars'\" all into the word `car`.\n",
    "\n",
    "This is generally done through lemmatization / stemming (different approaches trying to achieve a similar goal).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spacy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Space offers build-in functionality for lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['share',\n",
       " 'in',\n",
       " 'brewing',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'leisure',\n",
       " 'group',\n",
       " 'bass',\n",
       " 'plc',\n",
       " 'be',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'be',\n",
       " 'hold']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = [token.lemma_ for token in example_sentence]\n",
    "lemmatized[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the NLTK libary we can also use the more aggressive Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['share',\n",
       " 'in',\n",
       " 'brew',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'leisur',\n",
       " 'group',\n",
       " 'bass',\n",
       " 'plc',\n",
       " 'are',\n",
       " 'like',\n",
       " 'to',\n",
       " 'be',\n",
       " 'held']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed = [stemmer.stem(token.text) for token in example_sentence]\n",
    "stemmed[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares  |  share  |  share\n",
      "in  |  in  |  in\n",
      "brewing  |  brewing  |  brew\n",
      "-  |  -  |  -\n",
      "to  |  to  |  to\n",
      "-  |  -  |  -\n",
      "leisure  |  leisure  |  leisur\n",
      "group  |  group  |  group\n",
      "Bass  |  bass  |  bass\n",
      "Plc  |  plc  |  plc\n",
      "are  |  be  |  are\n",
      "likely  |  likely  |  like\n",
      "to  |  to  |  to\n",
      "be  |  be  |  be\n",
      "held  |  hold  |  held\n"
     ]
    }
   ],
   "source": [
    "for original, lemma, stem in zip(token_list[:15], lemmatized[:15], stemmed[:15]):\n",
    "    print(original, ' | ', lemma, ' | ', stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my experience it is usually best to use lemmatization instead of a stemmer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Language modeling</span><a id='lang_model'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is inherently structured in complex ways, we can often use some of this underlying structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Part-of-Speech tagging</span><a id='pos_tagging'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech tagging refers to the identification of words as nouns, verbs, adjectives, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Shares, 'NOUN'),\n",
       " (in, 'ADP'),\n",
       " (brewing, 'NOUN'),\n",
       " (-, 'PUNCT'),\n",
       " (to, 'ADP'),\n",
       " (-, 'PUNCT'),\n",
       " (leisure, 'NOUN'),\n",
       " (group, 'NOUN'),\n",
       " (Bass, 'PROPN'),\n",
       " (Plc, 'PROPN')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = [(token, token.pos_) for token in example_sentence]\n",
    "pos_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Uni-Gram & N-Grams</span><a id='n_grams'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously a sentence is not a random collection of words, the sequence of words has information value.  \n",
    "\n",
    "A simple way to incorporate some of this sequence is by using what is called `n-grams`.  \n",
    "An `n-gram` is nothing more than a a combination of `N` words into one token (a uni-gram token is just one word).  \n",
    "\n",
    "So we can convert `\"Sentence about flying cars\"` into a list of bigrams:\n",
    "\n",
    "> Sentence-about, about-flying, flying-cars  \n",
    "\n",
    "See my slide on N-Grams for a more comprehensive example: [click here](http://www.tiesdekok.com/AccountingNLP_Slides/#14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `NLTK`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are-likely', 'likely-to', 'to-be', 'be-held', 'held-back']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_list = ['-'.join(x) for x in nltk.bigrams([token.text for token in example_sentence])]\n",
    "bigram_list[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Stop words</span><a id='stop_words'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what you are trying to do it is possible that there are many words that don't add any information value to the sentence.  \n",
    "\n",
    "The primary example are stop words.  \n",
    "\n",
    "Sometimes you can improve the accuracy of your model by removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_stop_words = [token for token in example_sentence if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares, brewing, -, -, leisure, group, Bass, Plc, likely, held]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares, in, brewing, -, to, -, leisure, group, Bass, Plc]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note* we can also remove punctuation in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares, in, brewing, to, leisure]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token for token in example_sentence if not token.is_punct][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap everything into one function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I will primarily use `SpaCy` directly. However, I also recommend to check out the high-level wrapper `Textacy`.\n",
    "\n",
    "See their GitHub page for details: https://github.com/chartbeat-labs/textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick `Textacy` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_text = text_dict['test']['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = textacy.preprocess_text(example_text, lowercase=True, fix_unicode=True, no_punct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Basic SpaCy text processing function **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split into sentences\n",
    "2. Apply lemmatizer and remove top words\n",
    "3. Clean up the sentence using `textacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text_custom(text):\n",
    "    sentences = list(parser(text).sents)\n",
    "    lemmatized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        lemmatized_sentences.append([token.lemma_ for token in sentence if not token.is_stop | token.is_punct | token.is_space])\n",
    "    return [parser(' '.join(sentence)) for sentence in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 108 ms, total: 1min 47s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spacy_text_clean = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    lst = []\n",
    "    for text in text_list:\n",
    "        lst.append(process_text_custom(text))\n",
    "    spacy_text_clean[author] = lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are quite a lot of sentences (~52K) so this takes a bit of time (~ 2 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 53538\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for author, texts in spacy_text_clean.items():\n",
    "    for text in texts:\n",
    "        count += len(text)\n",
    "print('Number of sentences:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[share brewing leisure group bass plc likely hold britain 's trade industry secretary ian lang decide allow propose merge brewer carlsberg tetley say analyst,\n",
       " earlier lang announce bass deal refer monoplies mergers commission report march 24 1997,\n",
       " share fall 6p 781p news]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_text_clean['TimFarrand'][0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Direct feature extraction</span><a id='feature_extract'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have pre-processed our text into something that we can use for direct feature extraction or to convert it to a numerical representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Feature search</span><a id='feature_search'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Entity recognition</span><a id='entity_recognition'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful / relevant to extract entities that are mentioned in a piece of text.   \n",
    "\n",
    "SpaCy is quite powerful in extracting entities, however, it doesn't work very well on lowercase text.  \n",
    "\n",
    "Given that \"token.lemma\\_\" removes capitalization I will use `spacy_sentences` for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers.  \n",
       " "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][0][3]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(March, 'DATE'), (John Wakley, 'PERSON'), (Lehman Brothers, 'ORG')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, i.label_) for i in parser(example_sentence.text).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "British pub-to-hotel group Greenalls Plc on Thursday reported a 48 percent rise in profits before exceptional items to 148.7 million pounds ($246.4 million), driven by its acquisition of brewer Boddington in November 1995.\n",
       " \""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][4][0]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(British, 'NORP'),\n",
       " (Greenalls Plc, 'ORG'),\n",
       " (Thursday, 'DATE'),\n",
       " (48 percent, 'PERCENT'),\n",
       " (148.7 million pounds, 'QUANTITY'),\n",
       " ($246.4 million, 'MONEY'),\n",
       " (Boddington, 'ORG'),\n",
       " (November 1995, 'DATE')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, i.label_) for i in parser(example_sentence.text).ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Pattern search</span><a id='pattern_search'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the build-in `re` (regular expression) library you can pattern match nearly anything you want.  \n",
    "\n",
    "I will not go into details about regular expressions but see here for a tutorial:  \n",
    "https://regexone.com/references/python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: Use [Pythex.org](https://pythex.org/) to try out your regular expression\n",
    "\n",
    "Example on Pythex: <a href=\"https://pythex.org/?regex=IDNUMBER: (\\d\\d\\d-\\w\\w)&test_string=Ties de Kok (IDNUMBER: 123-AZ). Rest of Text.\" target='_blank'>click here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string_1 = 'Ties de Kok (#IDNUMBER: 123-AZ). Rest of text...'\n",
    "string_2 = 'Philip Joos (#IDNUMBER: 663-BY). Rest of text...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = r'#IDNUMBER: (\\d\\d\\d-\\w\\w)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123-AZ\n",
      "663-BY\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(pattern, string_1)[0])\n",
    "print(re.findall(pattern, string_2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a sentence contains the word 'million' return True, otherwise return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyst forecast pretax profit range 218 232 million stg restructure cost 206 million time\n",
      "restructure cost 35 million anticipate bulk 25 million stem closure small production plant france\n",
      "cadbury 's u.s. drink business turn 112 million stg trading profit 59 million half 1995 entirely contribution dr pepper\n",
      "campbell estimate uk beverage contribute 47 million stg operating profit 50 million time\n",
      "broadly analyst expect pretty flat performance group 's confectionery business consensus forecast 110 million stg operate profit\n",
      "average analyst calculate beverage chip trading profit 150 million\n",
      "sale 51 percent stake coca cola amp schweppes beverages ccsb operation coca cola enterprises june 620 million stg analyst want clear statement strategy company\n",
      "far analyst company say shareholder expect return investment emerge market large far 75 million russian plant\n",
      "cadbury announce investment 20 million stg build new plant wrocoaw poland 1993 joint venture china cost 20 million\n",
      "net debt 1.34 billion end 1995 fall 510 million end 1996 result ccsb sale provide acquisition\n"
     ]
    }
   ],
   "source": [
    "for sen in spacy_text_clean['TimFarrand'][2]:\n",
    "    TERM = 'million'\n",
    "    contains = True if re.search('million', sen.text) else False\n",
    "    if contains:\n",
    "        print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Text evaluation</span><a id='text_eval'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides feature search there are also many ways to analyze the text as a whole.  \n",
    "\n",
    "Let's, for example, evaluate the following paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"soft drink confectionery group cadbury schweppes plc expect report solid percent rise half profit wednesday face question performance 7up soft drink u.s. main question success relaunch 7up brand say mark duffy food manufacturing analyst sbc warburg competitor sprite own coca cola see agressive marketing push rank fast grow brand u.s. cadbury 's dr pepper analyst forecast pretax profit range 218 232 million stg restructure cost 206 million time dividend 5.1 penny expect 4.9p restructure cost 35 m\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_paragraph = ' '.join([x.text for x in spacy_text_clean['TimFarrand'][2]])\n",
    "example_paragraph[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Language</span><a id='language'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `langdetect` package it is easy to detect the language of a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(example_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Readability</span><a id='readability'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `textstat` package we can compute various readability metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/shivam5992/textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.02\n",
      "12.9\n",
      "19.8\n",
      "16.72\n",
      "25.4\n",
      "10.4\n",
      "120\n",
      "8.333333333333334\n",
      "29.942741116751268\n",
      "12th and 13th grade\n"
     ]
    }
   ],
   "source": [
    "print(textstat.flesch_reading_ease(example_paragraph))\n",
    "print(textstat.smog_index(example_paragraph))\n",
    "print(textstat.flesch_kincaid_grade(example_paragraph))\n",
    "print(textstat.coleman_liau_index(example_paragraph))\n",
    "print(textstat.automated_readability_index(example_paragraph))\n",
    "print(textstat.dale_chall_readability_score(example_paragraph))\n",
    "print(textstat.difficult_words(example_paragraph))\n",
    "print(textstat.linsear_write_formula(example_paragraph))\n",
    "print(textstat.gunning_fog(example_paragraph))\n",
    "print(textstat.text_standard(example_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Term (dictionary) counting</span><a id='dict_counting'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common techniques that researchers currently use (at least in Accounting research) are simple metrics based on counting words in a dictionary.  \n",
    "This technique is, for example, very prevalent in sentiment analysis (counting positive and negative words).  \n",
    "\n",
    "In essence this technique is very simple to program:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dictionary = ['soft', 'first', 'most', 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft 3\n",
      "first 0\n",
      "most 0\n",
      "be 8\n"
     ]
    }
   ],
   "source": [
    "for word in word_dictionary:\n",
    "    print(word, example_paragraph.count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = ['great', 'increase']\n",
    "neg = ['bad', 'decrease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = '''According to Trump everything is great, great, \n",
    "and great even though his popularity is seeing a decrease.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "pos_count = 0\n",
    "for word in pos:\n",
    "    pos_count += sentence.lower().count(word)\n",
    "print(pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "neg_count = 0\n",
    "for word in neg:\n",
    "    neg_count += sentence.lower().count(word)\n",
    "print(neg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count / (neg_count + pos_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the total number of words is also easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parser(example_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the count per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_count_dict = {}\n",
    "for word in pos:\n",
    "    pos_count_dict[word] = sentence.lower().count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'great': 3, 'increase': 0}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Represent text numerically</span><a id='text_numerical'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Bag of Words</span><a id='bows'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn includes the `CountVectorizer` and `TfidfVectorizer` function.  \n",
    "\n",
    "For details, see the documentation:  \n",
    "[TF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)  \n",
    "[TFIDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "Note 1: these functions also already includes a lot of preprocessing options (e.g. ngrames, remove stop words, accent stripper).\n",
    "\n",
    "Note 2: example based on the following website [click here](http://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_1 = \"The sky is blue.\"\n",
    "doc_2 = \"The sun is bright today.\"\n",
    "doc_3 = \"The sun in the sky is bright.\"\n",
    "doc_4 = \"We can see the shining sun, the bright sun.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate term frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tf = vectorizer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blue', 'bright', 'shining', 'sky', 'sun', 'today']\n",
      "[1 0 0 1 0 0]\n",
      "[0 1 0 0 1 1]\n",
      "[0 1 0 1 1 0]\n",
      "[0 1 1 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "for doc_tf_vector in tf.toarray():\n",
    "    print(doc_tf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">TF-IDF</span><a id='tfidf'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = transformer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.78528828  0.          0.          0.6191303   0.          0.        ]\n",
      "[ 0.          0.47380449  0.          0.          0.47380449  0.74230628]\n",
      "[ 0.          0.53256952  0.          0.65782931  0.53256952  0.        ]\n",
      "[ 0.          0.36626037  0.57381765  0.          0.73252075  0.        ]\n"
     ]
    }
   ],
   "source": [
    "for doc_vector in tfidf.toarray():\n",
    "    print(doc_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More elaborate example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_paragraphs = []\n",
    "for author, value in spacy_text_clean.items():\n",
    "    for article in value:\n",
    "        clean_paragraphs.append(' '.join([x.text for x in article]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_large = transformer.fit_transform(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors: 2500\n",
      "Number of words in dictionary: 24141\n"
     ]
    }
   ],
   "source": [
    "print('Number of vectors:', len(tfidf_large.toarray()))\n",
    "print('Number of words in dictionary:', len(tfidf_large.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2500x24141 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 444634 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Word Embeddings</span><a id='word_embed'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Word2Vec</span><a id='Word2Vec'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple example below is from:  https://medium.com/@mishra.thedeepak/word2vec-in-minutes-gensim-nlp-python-6940f4e00980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = brown.sents()\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('brown_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('brown_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find words most similar to 'mother':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('father', 0.9751995801925659), ('husband', 0.9580737352371216), ('wife', 0.9531918168067932), ('son', 0.9327206611633301), ('voice', 0.9201934337615967), ('boy', 0.9148358106613159), ('friend', 0.9091534614562988), ('ache', 0.8969202041625977), ('parents', 0.8916678428649902), ('maid', 0.8907431960105896)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"mother\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the odd one out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garden\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"pizza pasta garden fries\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve vector representation of the word \"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.24233282e+00,   1.73863158e-01,  -6.12607360e-01,\n",
       "        -5.31224608e-01,   2.60317028e-01,  -3.56246114e-01,\n",
       "        -5.06218493e-01,  -1.15960002e-01,   3.04902345e-01,\n",
       "        -1.21463396e-01,   4.93332326e-01,  -9.75235820e-01,\n",
       "         3.44474047e-01,   9.77401040e-04,   5.05028106e-02,\n",
       "        -3.87262404e-01,   4.93362129e-01,   7.00087488e-01,\n",
       "        -6.27336025e-01,  -4.63026613e-01,   2.79739406e-02,\n",
       "         1.45691419e+00,   4.07162786e-01,  -4.19379741e-01,\n",
       "        -8.41612220e-01,   8.46711546e-02,   3.03834379e-01,\n",
       "        -5.89724183e-01,  -5.50288737e-01,   1.14675418e-01,\n",
       "        -9.28169414e-02,  -5.00818849e-01,   1.74140222e-02,\n",
       "         2.45587930e-01,   9.37732458e-02,  -6.30766377e-02,\n",
       "         3.79322648e-01,  -9.30945396e-01,   1.81099728e-01,\n",
       "         4.46529061e-01,   5.09826422e-01,  -4.00113940e-01,\n",
       "        -3.06686193e-01,   5.83700202e-02,  -1.30845475e+00,\n",
       "        -8.19562197e-01,  -1.43999264e-01,  -1.79302439e-01,\n",
       "        -9.88642037e-01,   6.19562924e-01,  -5.98924696e-01,\n",
       "        -3.26148927e-01,  -2.68154591e-01,   1.32927846e-03,\n",
       "         4.15733218e-01,   4.20322359e-01,   2.24591553e-01,\n",
       "        -8.06021392e-02,   1.66282967e-01,  -5.05886197e-01,\n",
       "         4.11779553e-01,   2.37013131e-01,   9.44843650e-01,\n",
       "         1.08043969e+00,   1.97366968e-01,  -2.09960312e-01,\n",
       "        -2.96899788e-02,   6.43389523e-01,  -9.92119789e-01,\n",
       "        -5.22915125e-02,  -4.15121198e-01,   6.58638895e-01,\n",
       "        -4.61580336e-01,  -1.06919587e+00,  -3.75133425e-01,\n",
       "        -2.02061430e-01,   1.24140203e+00,   2.50428259e-01,\n",
       "         6.01192236e-01,   4.85432506e-01,   1.26407454e-02,\n",
       "         7.29153931e-01,  -1.80993602e-01,  -9.56271172e-01,\n",
       "         1.91430658e-01,   5.62396646e-01,  -1.07690930e+00,\n",
       "        -7.97812045e-01,  -8.85272324e-01,  -1.71307661e-02,\n",
       "        -4.96744901e-01,   7.43289739e-02,  -8.02996099e-01,\n",
       "        -3.25119644e-01,   1.37371510e-01,  -6.58412039e-01,\n",
       "        -4.42930400e-01,  -6.37149692e-01,  -3.13979797e-02,\n",
       "        -1.62613422e-01], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['human']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Statistical models</span><a id='stat_models'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">\"Traditional\" machine learning</span><a id='trad_ml'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library to use for machine learning is scikit-learn ([\"sklearn\"](http://scikit-learn.org/stable/index.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span>Supervised</span><a id='trad_ml_supervised'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the data into a pandas dataframe (so that we can input it easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for author, value in spacy_text_clean.items():\n",
    "    for article in value:\n",
    "        article_list.append((author, ' '.join([x.text for x in article])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_df = pd.DataFrame(article_list, columns=['author', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>AlanCrosby</td>\n",
       "      <td>czech consumer inflation continue gradual down...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>LydiaZajc</td>\n",
       "      <td>toronto stock end mixed light dealing monday w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>KirstinRidley</td>\n",
       "      <td>telecom analyst london amuse monday portugal t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>EricAuchard</td>\n",
       "      <td>domestic trouble at&amp;amp;t corp contend new thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229</th>\n",
       "      <td>NickLouth</td>\n",
       "      <td>regional bell telephone company report strong ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                               text\n",
       "1738     AlanCrosby  czech consumer inflation continue gradual down...\n",
       "710       LydiaZajc  toronto stock end mixed light dealing monday w...\n",
       "419   KirstinRidley  telecom analyst london amuse monday portugal t...\n",
       "2092    EricAuchard  domestic trouble at&amp;t corp contend new thr...\n",
       "2229      NickLouth  regional bell telephone company report strong ..."
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the sample into a training and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(article_df.text, article_df.author, test_size=0.20, random_state=3561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 500\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple function to train (i.e. fit) and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Accuracy on training set:\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(\"Accuracy on testing set:\")\n",
    "    print(clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span>Na√Øve Bayes estimator</span><a id='trad_ml_supervised_nb'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english'\n",
    "                            )),\n",
    "        \n",
    "    ('clf', MultinomialNB(alpha = 1,\n",
    "                          fit_prior = True\n",
    "                          )\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and show evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.8475\n",
      "Accuracy on testing set:\n",
      "0.7\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.89      0.73      0.80        11\n",
      "       AlanCrosby       0.53      0.90      0.67        10\n",
      "   AlexanderSmith       1.00      0.78      0.88         9\n",
      "  BenjaminKangLim       0.60      0.60      0.60        10\n",
      "    BernardHickey       0.88      0.78      0.82         9\n",
      "      BradDorfman       0.57      0.80      0.67         5\n",
      " DarrenSchuettler       0.75      0.60      0.67        10\n",
      "      DavidLawder       0.58      0.78      0.67         9\n",
      "    EdnaFernandes       1.00      0.30      0.46        10\n",
      "      EricAuchard       0.67      0.29      0.40        14\n",
      "   FumikoFujisaki       1.00      1.00      1.00        13\n",
      "   GrahamEarnshaw       0.71      1.00      0.83        10\n",
      " HeatherScoffield       0.67      0.60      0.63        10\n",
      "       JanLopatka       0.54      0.70      0.61        10\n",
      "    JaneMacartney       0.62      0.29      0.40        17\n",
      "     JimGilchrist       0.92      1.00      0.96        11\n",
      "   JoWinterbottom       0.43      1.00      0.60         3\n",
      "         JoeOrtiz       0.89      0.89      0.89         9\n",
      "     JohnMastrini       0.67      0.33      0.44        12\n",
      "     JonathanBirt       0.57      1.00      0.73         8\n",
      "      KarlPenhaul       0.89      0.80      0.84        10\n",
      "        KeithWeir       0.86      0.50      0.63        12\n",
      "   KevinDrawbaugh       0.56      0.71      0.63         7\n",
      "    KevinMorrison       0.64      0.69      0.67        13\n",
      "    KirstinRidley       0.67      0.89      0.76         9\n",
      "KouroshKarimkhany       0.80      0.80      0.80        10\n",
      "        LydiaZajc       0.80      0.89      0.84         9\n",
      "   LynneO'Donnell       0.80      0.80      0.80        10\n",
      "  LynnleyBrowning       0.83      1.00      0.91        10\n",
      "  MarcelMichelson       0.78      0.78      0.78         9\n",
      "     MarkBendeich       0.50      0.38      0.43         8\n",
      "       MartinWolk       0.86      0.67      0.75         9\n",
      "     MatthewBunce       0.92      0.92      0.92        12\n",
      "    MichaelConnor       0.83      0.83      0.83         6\n",
      "       MureDickie       0.46      0.55      0.50        11\n",
      "        NickLouth       0.64      0.90      0.75        10\n",
      "  PatriciaCommins       1.00      0.62      0.76        13\n",
      "    PeterHumphrey       0.39      0.85      0.54        13\n",
      "       PierreTran       0.86      0.75      0.80         8\n",
      "       RobinSidel       1.00      0.90      0.95        10\n",
      "     RogerFillion       1.00      0.67      0.80         9\n",
      "      SamuelPerry       0.67      0.75      0.71         8\n",
      "     SarahDavison       0.83      0.56      0.67         9\n",
      "      ScottHillis       0.50      0.73      0.59        11\n",
      "      SimonCowell       0.88      0.78      0.82         9\n",
      "         TanEeLyn       1.00      0.21      0.35        14\n",
      "   TheresePoletti       0.59      1.00      0.74        10\n",
      "       TimFarrand       0.44      0.88      0.58         8\n",
      "       ToddNissen       0.77      0.71      0.74        14\n",
      "     WilliamKazer       0.50      0.11      0.18         9\n",
      "\n",
      "      avg / total       0.74      0.70      0.69       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_bayes_results.pkl']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'naive_bayes_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict out of sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_y, example_X = y_train[33], X_train[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: JoeOrtiz\n",
      "Predicted author: AlexanderSmith\n"
     ]
    }
   ],
   "source": [
    "print('Actual author:', example_y)\n",
    "print('Predicted author:', clf.predict([example_X])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span>Support Vector Machines (SVM)</span><a id='trad_ml_supervised_svm'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_svm = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english'\n",
    "                            )),\n",
    "        \n",
    "    ('clf', SVC(kernel='rbf' ,\n",
    "                C=10, gamma=0.3)\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* The SVC estimator is very sensitive to the hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and show evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.998\n",
      "Accuracy on testing set:\n",
      "0.814\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.91      0.91      0.91        11\n",
      "       AlanCrosby       0.83      1.00      0.91        10\n",
      "   AlexanderSmith       1.00      0.67      0.80         9\n",
      "  BenjaminKangLim       0.58      0.70      0.64        10\n",
      "    BernardHickey       1.00      0.89      0.94         9\n",
      "      BradDorfman       0.80      0.80      0.80         5\n",
      " DarrenSchuettler       0.90      0.90      0.90        10\n",
      "      DavidLawder       0.46      0.67      0.55         9\n",
      "    EdnaFernandes       1.00      0.90      0.95        10\n",
      "      EricAuchard       0.73      0.57      0.64        14\n",
      "   FumikoFujisaki       1.00      1.00      1.00        13\n",
      "   GrahamEarnshaw       0.91      1.00      0.95        10\n",
      " HeatherScoffield       0.90      0.90      0.90        10\n",
      "       JanLopatka       0.78      0.70      0.74        10\n",
      "    JaneMacartney       0.60      0.35      0.44        17\n",
      "     JimGilchrist       1.00      1.00      1.00        11\n",
      "   JoWinterbottom       1.00      1.00      1.00         3\n",
      "         JoeOrtiz       0.75      1.00      0.86         9\n",
      "     JohnMastrini       0.73      0.67      0.70        12\n",
      "     JonathanBirt       0.73      1.00      0.84         8\n",
      "      KarlPenhaul       1.00      0.90      0.95        10\n",
      "        KeithWeir       0.92      0.92      0.92        12\n",
      "   KevinDrawbaugh       0.86      0.86      0.86         7\n",
      "    KevinMorrison       0.79      0.85      0.81        13\n",
      "    KirstinRidley       1.00      0.89      0.94         9\n",
      "KouroshKarimkhany       1.00      1.00      1.00        10\n",
      "        LydiaZajc       0.90      1.00      0.95         9\n",
      "   LynneO'Donnell       0.89      0.80      0.84        10\n",
      "  LynnleyBrowning       1.00      1.00      1.00        10\n",
      "  MarcelMichelson       0.78      0.78      0.78         9\n",
      "     MarkBendeich       0.64      0.88      0.74         8\n",
      "       MartinWolk       0.80      0.89      0.84         9\n",
      "     MatthewBunce       1.00      0.92      0.96        12\n",
      "    MichaelConnor       1.00      1.00      1.00         6\n",
      "       MureDickie       0.75      0.55      0.63        11\n",
      "        NickLouth       0.86      0.60      0.71        10\n",
      "  PatriciaCommins       1.00      0.92      0.96        13\n",
      "    PeterHumphrey       0.59      0.77      0.67        13\n",
      "       PierreTran       0.70      0.88      0.78         8\n",
      "       RobinSidel       1.00      1.00      1.00        10\n",
      "     RogerFillion       0.89      0.89      0.89         9\n",
      "      SamuelPerry       0.75      0.75      0.75         8\n",
      "     SarahDavison       0.88      0.78      0.82         9\n",
      "      ScottHillis       0.62      0.45      0.53        11\n",
      "      SimonCowell       1.00      0.89      0.94         9\n",
      "         TanEeLyn       0.83      0.71      0.77        14\n",
      "   TheresePoletti       0.71      1.00      0.83        10\n",
      "       TimFarrand       1.00      0.75      0.86         8\n",
      "       ToddNissen       0.75      0.64      0.69        14\n",
      "     WilliamKazer       0.29      0.56      0.38         9\n",
      "\n",
      "      avg / total       0.83      0.81      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf_svm, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_results.pkl']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf_svm, 'svm_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict out of sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_y, example_X = y_train[33], X_train[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: JoeOrtiz\n",
      "Predicted author: JoeOrtiz\n"
     ]
    }
   ],
   "source": [
    "print('Actual author:', example_y)\n",
    "print('Predicted author:', clf_svm.predict([example_X])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span>Model Selection and Evaluation</span><a id='trad_ml_eval'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `TfidfVectorizer` and `SVC()` estimator take a lot of hyperparameters.  \n",
    "\n",
    "It can be difficult to figure out what the best parameters are.\n",
    "\n",
    "We can use `GridSearchCV` to help figure this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the options that should be tried out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lala\n"
     ]
    }
   ],
   "source": [
    "print('lala')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_search = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "parameters = { 'vect__stop_words': ['english'],\n",
    "                'vect__strip_accents': ['unicode'],\n",
    "              'vect__max_features' : [1500],\n",
    "              'vect__ngram_range': [(1,1), (2,2) ],\n",
    "             'clf__gamma' : [0.2, 0.3, 0.4], \n",
    "             'clf__C' : [8, 10, 12],\n",
    "              'clf__kernel' : ['rbf']\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=6,\n",
       "       param_grid={'vect__stop_words': ['english'], 'clf__C': [8, 10, 12], 'clf__gamma': [0.2, 0.3, 0.4], 'clf__kernel': ['rbf'], 'vect__strip_accents': ['unicode'], 'vect__max_features': [1500], 'vect__ngram_range': [(1, 1), (2, 2)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score, average=micro), verbose=0)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(clf_search, param_grid=parameters, scoring=make_scorer(f1_score, average='micro'), n_jobs=6)\n",
    "grid.fit(X_train, y_train)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* if you are on a powerful unix system you can set n_jobs to the number of available threads to speed up the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'vect__stop_words': 'english', 'clf__C': 10, 'clf__gamma': 0.3, 'clf__kernel': 'rbf', 'vect__strip_accents': 'unicode', 'vect__max_features': 1500, 'vect__ngram_range': (1, 1)} with a score of 0.78\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.91      0.91      0.91        11\n",
      "       AlanCrosby       0.83      1.00      0.91        10\n",
      "   AlexanderSmith       1.00      0.67      0.80         9\n",
      "  BenjaminKangLim       0.58      0.70      0.64        10\n",
      "    BernardHickey       1.00      0.89      0.94         9\n",
      "      BradDorfman       0.80      0.80      0.80         5\n",
      " DarrenSchuettler       0.90      0.90      0.90        10\n",
      "      DavidLawder       0.46      0.67      0.55         9\n",
      "    EdnaFernandes       1.00      0.90      0.95        10\n",
      "      EricAuchard       0.73      0.57      0.64        14\n",
      "   FumikoFujisaki       1.00      1.00      1.00        13\n",
      "   GrahamEarnshaw       0.91      1.00      0.95        10\n",
      " HeatherScoffield       0.90      0.90      0.90        10\n",
      "       JanLopatka       0.78      0.70      0.74        10\n",
      "    JaneMacartney       0.60      0.35      0.44        17\n",
      "     JimGilchrist       1.00      1.00      1.00        11\n",
      "   JoWinterbottom       1.00      1.00      1.00         3\n",
      "         JoeOrtiz       0.75      1.00      0.86         9\n",
      "     JohnMastrini       0.73      0.67      0.70        12\n",
      "     JonathanBirt       0.73      1.00      0.84         8\n",
      "      KarlPenhaul       1.00      0.90      0.95        10\n",
      "        KeithWeir       0.92      0.92      0.92        12\n",
      "   KevinDrawbaugh       0.86      0.86      0.86         7\n",
      "    KevinMorrison       0.79      0.85      0.81        13\n",
      "    KirstinRidley       1.00      0.89      0.94         9\n",
      "KouroshKarimkhany       1.00      1.00      1.00        10\n",
      "        LydiaZajc       0.90      1.00      0.95         9\n",
      "   LynneO'Donnell       0.89      0.80      0.84        10\n",
      "  LynnleyBrowning       1.00      1.00      1.00        10\n",
      "  MarcelMichelson       0.78      0.78      0.78         9\n",
      "     MarkBendeich       0.64      0.88      0.74         8\n",
      "       MartinWolk       0.80      0.89      0.84         9\n",
      "     MatthewBunce       1.00      0.92      0.96        12\n",
      "    MichaelConnor       1.00      1.00      1.00         6\n",
      "       MureDickie       0.75      0.55      0.63        11\n",
      "        NickLouth       0.86      0.60      0.71        10\n",
      "  PatriciaCommins       1.00      0.92      0.96        13\n",
      "    PeterHumphrey       0.59      0.77      0.67        13\n",
      "       PierreTran       0.70      0.88      0.78         8\n",
      "       RobinSidel       1.00      1.00      1.00        10\n",
      "     RogerFillion       0.89      0.89      0.89         9\n",
      "      SamuelPerry       0.75      0.75      0.75         8\n",
      "     SarahDavison       0.88      0.78      0.82         9\n",
      "      ScottHillis       0.62      0.45      0.53        11\n",
      "      SimonCowell       1.00      0.89      0.94         9\n",
      "         TanEeLyn       0.83      0.71      0.77        14\n",
      "   TheresePoletti       0.71      1.00      0.83        10\n",
      "       TimFarrand       1.00      0.75      0.86         8\n",
      "       ToddNissen       0.75      0.64      0.69        14\n",
      "     WilliamKazer       0.29      0.56      0.38         9\n",
      "\n",
      "      avg / total       0.83      0.81      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "y_true, y_pred = y_test, grid.predict(X_test)\n",
    "print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span>Unsupervised</span><a id='trad_ml_unsupervised'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span>Latent Dirichilet Allocation (LDA)</span><a id='trad_ml_unsupervised_lda'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizer (using countvectorizer for the sake of example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english', max_df=0.8)\n",
    "tf_large = vectorizer.fit_transform(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "n_top_words = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10,\n",
    "                                learning_method='online',\n",
    "                                n_jobs=1)\n",
    "lda_fitted = lda.fit_transform(tf_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_top_words(model, feature_names, n_top_words):\n",
    "    out_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        out_list.append((topic_idx+1, \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])))\n",
    "    out_df = pd.DataFrame(out_list, columns=['topic_id', 'top_words'])\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df = save_top_words(lda, vectorizer.get_feature_names(), n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>top_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kong hong china chinese deng party beijing peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>million percent share analyst sale quarter pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>stock gold gm bre canada toronto share busang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>new market company plan group business add 000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>bank thomson banking financial nomura firm com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>company corp service computer new apple softwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>market percent rate china bank crown billion a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>tonne 000 price export million crop source tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>china official beijing chinese people air boei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>government ford car company czech strike new p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id                                          top_words\n",
       "0         1  kong hong china chinese deng party beijing peo...\n",
       "1         2  million percent share analyst sale quarter pro...\n",
       "2         3  stock gold gm bre canada toronto share busang ...\n",
       "3         4  new market company plan group business add 000...\n",
       "4         5  bank thomson banking financial nomura firm com...\n",
       "5         6  company corp service computer new apple softwa...\n",
       "6         7  market percent rate china bank crown billion a...\n",
       "7         8  tonne 000 price export million crop source tra...\n",
       "8         9  china official beijing chinese people air boei...\n",
       "9        10  government ford car company czech strike new p..."
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span>pyLDAvis</span><a id='trad_ml_unsupervised_pyLDAvis'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el45995470601851163684455707973\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el45995470601851163684455707973_data = {\"plot.opts\": {\"ylab\": \"PC2\", \"xlab\": \"PC1\"}, \"lambda.step\": 0.01, \"mdsDat\": {\"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"x\": [-0.16014843169161172, -0.015733314860300458, 0.012237512089620038, 0.22620480300612059, 0.0855443257571877, -0.03419599075198165, 0.19860081750892267, -0.07804954040105674, -0.12189224324734417, -0.11256793740955631], \"y\": [-0.007948935261780733, -0.12308070173785869, -0.05386788282406487, 0.03338768655119422, -0.02327818504080228, -0.12292660534536136, 0.06303648083069814, 0.10578542943684255, 0.22182982586679612, -0.09293711247566261], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"Freq\": [21.924676123982795, 14.437502411531844, 12.920509742141192, 9.698102721272521, 8.993743859719967, 8.146444734311078, 7.756998362688669, 7.555307125735093, 4.54933373995015, 4.017381178666697]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.5125, 1.5084, 1.5082, 1.5071, 1.5042, 1.5035, 1.5011, 1.4998, 1.4972, 1.4968, 1.4925, 1.4883, 1.4818, 1.4714, 1.4531, 1.441, 1.4372, 1.4302, 1.4298, 1.4289, 1.3814, 1.3794, 1.3701, 1.3531, 1.3528, 1.3173, 1.3105, 1.3041, 1.2949, 1.2931, 1.2817, 1.2705, 1.2693, 1.2042, 1.2258, 1.2618, 1.1216, 1.0052, 1.2011, 1.1256, 0.932, 1.0621, 1.0264, 1.1099, 1.0591, 1.1892, 0.7733, 0.9613, 0.7734, 0.9572, 0.7107, 0.967, 0.9738, 0.3203, 0.7374, 0.5732, 0.1704, 0.4674, 0.4331, 0.8045, -0.2446, 1.9333, 1.932, 1.9319, 1.9315, 1.9301, 1.9299, 1.9297, 1.9297, 1.9292, 1.9291, 1.9283, 1.9276, 1.9276, 1.9274, 1.9273, 1.9266, 1.9255, 1.9255, 1.9242, 1.9241, 1.9214, 1.9211, 1.9195, 1.9191, 1.9189, 1.913, 1.9125, 1.9096, 1.9088, 1.9088, 1.9027, 1.8915, 1.8981, 1.894, 1.8695, 1.8915, 1.8796, 1.8038, 1.5481, 1.6837, 1.8164, 1.4182, 1.4938, 1.8069, 0.8807, 1.332, 1.4986, 0.5951, 1.0805, 0.7484, 0.7368, 1.1751, 0.4753, 0.774, -0.0548, 0.6363, -0.2295, 0.7703, 0.1447, -0.0283, -0.3404, -0.779, 0.4904, 2.0406, 2.0397, 2.0364, 2.0169, 1.9935, 1.9503, 1.9264, 1.9161, 1.9132, 1.8865, 1.8493, 1.8416, 1.8121, 1.8016, 1.7997, 1.7949, 1.7922, 1.7757, 1.7756, 1.7751, 1.7492, 1.7199, 1.7029, 1.6728, 1.6713, 1.654, 1.6524, 1.6475, 1.6364, 1.6354, 1.5855, 1.5682, 1.3985, 1.4916, 0.6953, 1.1546, 1.1384, 1.1771, 0.4365, 0.9695, 1.1391, 1.0389, 1.1427, 0.7658, 0.5682, 0.6505, 0.7336, 0.6289, 0.043, 0.5975, 0.7156, 0.4401, 1.0104, 0.5929, 0.3169, 0.693, 0.5608, 0.6269, 0.8216, 0.6681, 0.4565, -0.674, 0.2557, -0.7043, 0.323, 2.3309, 2.3303, 2.3299, 2.3285, 2.3272, 2.3261, 2.3256, 2.3252, 2.3239, 2.3239, 2.3228, 2.3216, 2.3209, 2.3128, 2.3107, 2.3032, 2.2981, 2.2893, 2.2818, 2.2791, 2.2692, 2.2504, 2.2029, 2.1628, 2.1469, 2.1345, 2.1305, 2.13, 2.1081, 2.1044, 2.0697, 2.0512, 1.9895, 1.7552, 1.8471, 1.6609, 1.7812, 1.5438, 1.9775, 1.8511, 1.36, 1.5837, 1.1868, 1.2885, 1.3279, 1.6848, 0.5097, 1.181, 0.3965, 0.1576, 0.3724, 1.1813, 1.2398, 2.4063, 2.4052, 2.4021, 2.3982, 2.3968, 2.3904, 2.3791, 2.3581, 2.348, 2.3387, 2.3257, 2.3229, 2.3221, 2.3096, 2.3017, 2.2772, 2.2696, 2.2689, 2.2652, 2.2387, 2.2291, 2.2248, 2.2016, 2.1704, 2.1615, 2.1329, 2.0938, 2.065, 2.0237, 1.9977, 1.9219, 1.9097, 1.8604, 1.7934, 1.3562, 1.8939, 1.8457, 1.5334, 1.6054, 1.4968, 0.6424, 0.2038, -0.1615, 1.1226, 0.7189, 0.9967, 1.0518, 0.2987, 0.4934, -0.8484, 0.3131, 2.5055, 2.5043, 2.502, 2.5017, 2.4994, 2.4972, 2.4964, 2.4956, 2.4954, 2.4915, 2.4603, 2.4528, 2.4515, 2.4495, 2.4472, 2.4383, 2.3815, 2.3745, 2.3742, 2.3565, 2.3249, 2.3144, 2.3098, 2.2968, 2.2847, 2.2738, 2.2311, 2.2287, 2.1734, 2.0979, 2.0399, 2.0266, 1.9792, 1.9583, 1.9469, 1.8488, 1.5349, 1.4795, 1.7894, 1.5738, 1.517, 1.3429, 1.4213, 0.9818, 0.5684, 1.128, 0.6153, -0.1706, 0.5991, 0.1975, 1.5936, 0.3975, 0.7452, 2.5537, 2.551, 2.5508, 2.5507, 2.5504, 2.5501, 2.5496, 2.5494, 2.5486, 2.5475, 2.5473, 2.5469, 2.5456, 2.5452, 2.5431, 2.5415, 2.5374, 2.531, 2.5246, 2.5169, 2.5062, 2.5037, 2.4992, 2.4986, 2.4986, 2.4712, 2.4508, 2.4423, 2.4323, 2.4267, 2.3711, 2.3511, 2.2644, 2.0711, 1.7252, 1.6264, 1.8935, 2.3718, 1.1689, 1.4575, 2.1387, 1.6814, 1.4605, 1.3106, 1.1609, 1.4246, 1.3393, 2.1068, 1.807, 0.4571, 0.4011, -0.4076, 2.5665, 2.5609, 2.526, 2.5176, 2.501, 2.4651, 2.4522, 2.374, 2.3456, 2.3022, 2.2709, 2.1839, 2.109, 2.0773, 2.0727, 2.072, 2.046, 1.8774, 1.8506, 1.7978, 1.7972, 1.7927, 1.7792, 1.7636, 1.7367, 1.7367, 1.7244, 1.7234, 1.7127, 1.7068, 1.6977, 1.6806, 1.4298, 1.5925, 1.4945, 1.604, 1.5128, 1.5353, 0.967, 1.4813, 1.0129, 1.2468, 0.8746, 1.1378, 1.2946, 0.6198, 0.9694, 0.7659, 0.3606, 0.7541, 0.6087, 0.4066, 0.6337, 0.8166, 1.1142, 3.0891, 3.0872, 3.0854, 3.0854, 3.0844, 3.0842, 3.083, 3.0818, 3.0815, 3.0779, 3.0708, 3.0631, 3.0181, 3.0168, 2.9755, 2.9675, 2.9548, 2.8963, 2.8642, 2.8198, 2.816, 2.7162, 2.7113, 2.6055, 2.5736, 2.5711, 2.4538, 2.4473, 2.4333, 2.4251, 2.1582, 1.9866, 1.735, 2.1411, 1.752, 2.2806, 1.1266, 0.4062, 1.8343, 0.9416, 1.1349, 1.9367, 0.622, 1.0636, 0.9681, 0.8621, 3.2128, 3.2124, 3.2116, 3.2109, 3.2107, 3.2089, 3.2079, 3.2066, 3.2065, 3.2048, 3.2047, 3.2046, 3.1693, 3.1605, 3.1594, 3.1288, 3.0934, 3.0108, 2.9664, 2.955, 2.8679, 2.7051, 2.3421, 2.238, 2.199, 2.1917, 2.1552, 2.1523, 2.1316, 2.1257, 2.0652, 1.86, 1.8608, 2.065, 1.8392, 1.9565, 1.6272, 0.8687, 1.1298, 0.2602, 1.0344, 0.4972, 0.229, -0.0252, 1.484, 0.8371, -0.019, -0.5215], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.0001, -6.9611, -6.9792, -7.1025, -7.2092, -6.9408, -6.9577, -4.7621, -4.1873, -7.0986, -5.7506, -7.2127, -5.3408, -5.6934, -6.9718, -4.202, -6.1382, -6.2083, -7.0463, -6.6142, -6.8949, -7.2713, -6.8049, -5.4412, -7.1378, -6.4141, -7.1315, -7.0543, -7.2015, -6.0948, -4.9453, -4.1622, -5.1985, -3.7768, -4.9192, -5.5138, -3.4864, -3.4935, -5.32, -4.772, -3.9555, -4.7265, -4.5723, -5.0285, -5.0447, -5.6896, -4.2732, -5.0551, -4.535, -5.1756, -4.6517, -5.2357, -5.3178, -4.2115, -4.9978, -4.9299, -4.6092, -4.9506, -5.0362, -5.2054, -5.2105, -5.0339, -5.5388, -4.6498, -5.5864, -5.6263, -5.8626, -6.0598, -5.9738, -5.4407, -6.1702, -6.2316, -6.3623, -6.3846, -6.0494, -4.7183, -5.5123, -6.6226, -6.6097, -4.4832, -6.7516, -6.7247, -6.9036, -5.2813, -6.2994, -5.8602, -6.4295, -4.3314, -6.0931, -6.852, -6.4487, -5.6578, -4.6741, -5.5004, -5.601, -5.5957, -6.0204, -5.8011, -5.5147, -4.3206, -5.1899, -5.8248, -4.2857, -4.8846, -5.8339, -3.651, -4.8488, -5.4969, -4.3708, -5.0395, -4.9079, -4.9282, -5.3932, -5.0005, -5.1974, -4.9423, -5.2381, -5.009, -5.3128, -5.2178, -5.3367, -5.3868, -5.387, -5.4447, -5.9853, -6.0743, -6.4503, -5.1137, -6.2809, -5.6175, -5.9824, -6.3514, -6.476, -6.587, -6.5541, -6.7397, -5.3198, -6.2036, -6.3062, -6.6859, -6.6531, -6.162, -5.6299, -6.4275, -5.234, -6.0616, -6.442, -6.7278, -6.6927, -7.0734, -6.8474, -6.686, -6.8451, -6.9114, -6.3116, -6.4479, -5.4077, -6.1685, -4.2706, -5.4305, -5.4164, -5.6487, -4.343, -5.368, -5.6381, -5.489, -5.672, -5.1693, -4.9076, -5.0458, -5.164, -5.0571, -4.4888, -5.0674, -5.2561, -5.0291, -5.5623, -5.2472, -5.0456, -5.3241, -5.2976, -5.395, -5.5232, -5.478, -5.4122, -5.1726, -5.4006, -5.3123, -5.5198, -4.7603, -5.018, -4.2617, -5.4966, -5.3062, -5.8332, -5.983, -6.0212, -6.1828, -6.1829, -6.2942, -6.4089, -6.2877, -2.9902, -2.9965, -5.7592, -6.1265, -4.9324, -5.635, -5.613, -5.3829, -5.6826, -6.0388, -6.3139, -6.2239, -6.4626, -6.3876, -6.0475, -6.4208, -4.5638, -4.3269, -5.5472, -5.0951, -3.2279, -4.6959, -4.3269, -4.8325, -4.2362, -5.4741, -5.1877, -4.4827, -5.0218, -4.8351, -5.0021, -5.2316, -5.5254, -5.0211, -5.3739, -5.3391, -5.3181, -5.3635, -5.4828, -5.4869, -4.5017, -4.8071, -5.6505, -6.0174, -6.1275, -5.0736, -5.5216, -5.2593, -5.0181, -5.8147, -6.3008, -6.3368, -6.0043, -4.6536, -5.0377, -5.6554, -4.726, -4.7734, -6.1223, -5.4257, -5.7083, -6.3754, -5.9838, -6.517, -4.9323, -6.3715, -4.8147, -5.3503, -5.7844, -6.1818, -4.7152, -5.1562, -4.9369, -5.03, -4.1746, -5.5429, -5.5881, -5.3337, -5.4624, -5.4055, -4.8333, -4.7621, -4.6933, -5.2858, -5.1554, -5.3645, -5.4239, -5.3662, -5.3958, -5.347, -5.4228, -4.3172, -4.8858, -5.4405, -5.4697, -5.678, -6.0814, -5.9322, -4.7207, -4.5119, -6.1369, -5.4152, -6.2686, -5.8763, -5.3527, -5.365, -4.373, -4.8162, -6.1605, -5.4888, -6.1492, -5.432, -5.9879, -6.2177, -6.2943, -6.0051, -5.4681, -3.0, -4.7908, -5.4754, -5.5225, -5.1312, -5.1043, -4.9547, -4.9038, -4.9093, -4.9094, -4.4368, -4.6718, -5.165, -4.9542, -5.0149, -4.943, -5.0471, -4.8926, -4.794, -5.0181, -4.854, -4.7024, -4.9317, -4.849, -5.2066, -5.0351, -5.1441, -4.7875, -5.4465, -5.4621, -5.22, -5.5363, -5.6012, -5.6737, -5.6435, -5.8022, -5.9428, -5.9572, -5.9582, -6.0244, -6.1613, -5.8508, -5.3747, -5.9709, -5.003, -5.2861, -5.117, -5.8898, -5.9059, -5.9154, -5.1836, -5.58, -4.7141, -5.2284, -5.0803, -6.0253, -6.1359, -4.9289, -4.9371, -4.9798, -4.8331, -4.2625, -4.1095, -4.8635, -5.6826, -3.8143, -4.3225, -5.3441, -4.9189, -4.845, -4.7969, -4.6818, -4.9837, -4.9514, -5.45, -5.3787, -5.2785, -5.3341, -5.3735, -4.9202, -4.2462, -5.3363, -4.8345, -4.981, -5.1308, -4.8179, -5.8216, -5.2058, -5.9347, -5.9045, -5.9463, -4.6817, -5.0763, -6.2905, -4.0853, -6.0559, -4.7823, -5.2422, -6.0643, -5.4194, -5.6875, -6.2148, -6.1275, -5.4046, -6.6587, -6.8494, -6.7293, -6.3642, -5.7152, -4.6742, -4.5873, -3.3498, -5.0446, -4.6131, -5.2378, -4.9691, -5.1289, -3.5316, -4.9768, -4.2182, -4.6424, -4.1086, -4.6508, -4.8438, -4.4267, -4.797, -4.6521, -4.5269, -4.749, -4.9221, -4.9018, -4.965, -4.9857, -5.0361, -3.151, -4.2544, -4.741, -4.734, -4.9192, -4.9702, -5.0891, -5.3345, -5.3702, -5.3658, -5.141, -4.5977, -4.918, -5.3667, -5.1579, -5.3797, -5.6264, -4.5926, -4.6902, -5.5687, -5.4303, -5.5769, -4.8773, -5.7289, -6.028, -5.3822, -4.388, -4.0283, -4.4338, -5.1966, -3.5278, -4.2994, -3.7681, -5.0002, -4.7174, -5.1708, -4.4722, -4.2018, -4.9892, -4.8384, -4.9461, -5.1988, -5.0743, -5.1391, -5.1393, -5.1543, -3.5954, -3.5713, -4.1389, -4.0831, -3.5757, -4.8094, -4.9415, -5.1438, -5.0596, -5.3608, -5.3547, -5.3488, -4.1439, -4.6382, -4.9025, -4.4959, -4.2951, -3.9087, -5.0811, -5.5453, -5.4569, -5.2, -5.2103, -6.0957, -5.4227, -6.3536, -6.334, -5.9921, -6.208, -6.4516, -4.9496, -3.558, -4.4852, -5.8832, -5.1431, -5.5386, -4.8309, -4.1123, -4.5265, -4.2716, -4.7007, -4.5493, -4.6585, -4.6332, -5.0371, -4.9652, -4.9849, -5.0202], \"Freq\": [2490.0, 2479.0, 2676.0, 3429.0, 5566.0, 3436.0, 975.0, 2220.0, 4203.0, 2038.0, 4990.0, 1698.0, 1700.0, 5385.0, 1545.0, 1255.0, 1772.0, 566.0, 564.0, 552.0, 971.0, 686.0, 3218.0, 1852.0, 1059.0, 825.0, 2187.0, 770.0, 3773.0, 494.0, 271.94657310388084, 104.01878391914761, 102.15486557146077, 90.29804149521782, 81.16334188207017, 106.15354679710788, 104.36685915469634, 937.8141642897556, 1666.3250567786852, 90.6523461551029, 349.00748166347313, 80.8772537694466, 525.7731764868315, 369.5445436498798, 102.9068092876463, 1642.0312665018325, 236.8666793960436, 220.83068336678463, 95.52481667623476, 147.15420781062787, 111.14102573996908, 76.27706535058871, 121.59829753849245, 475.54836114686987, 87.17396573774013, 179.75011251540394, 87.71837075908313, 94.76299144148895, 81.79048351304243, 247.3548636650451, 780.8463111300188, 1708.6236357248938, 606.1742923166474, 2511.995842241183, 801.4983170249847, 442.22853988364153, 3358.512155790322, 3334.815110428029, 536.8131151495044, 928.600870656341, 2101.009645142413, 971.7646003589621, 1133.8050208741756, 718.4651176361106, 706.9861468506075, 370.9512055853645, 1529.1803605756522, 699.6179229416571, 1176.9832786017982, 620.223754418217, 1047.2703566598154, 584.0130203958506, 538.0204998744531, 1626.5377824788225, 740.8728777153957, 792.9283439436743, 1092.7966402138793, 776.7280251983607, 712.9939705273612, 601.9875065488143, 598.944591163595, 470.6024729064275, 284.0385935618955, 690.925354943823, 270.83724320031627, 260.2294901760981, 205.4617506098786, 168.69735217906896, 183.84387350251288, 313.3137932174899, 151.06728502082606, 142.06832485520803, 124.66163589448193, 121.90902487736412, 170.46205364860828, 645.2117707925391, 291.6701795063625, 96.09161961927065, 97.33617292950572, 816.2322152241584, 84.46221718083, 86.76157528507534, 72.55112416008556, 367.4581449995055, 132.747876992608, 205.9679703072551, 116.5623856770219, 949.9888042639838, 163.1719975664757, 76.39561937116122, 114.33954899751109, 252.1647324147644, 674.3555038896186, 295.14125063902316, 266.8964935458475, 268.3082427869337, 175.4717787643129, 218.50772235795208, 290.9503403269344, 960.3733357885776, 402.6318346132684, 213.3847165934266, 994.3950286815312, 546.3595554063481, 211.43997869840953, 1875.8970092732382, 566.25031093421, 296.186363187687, 913.3555866518782, 467.97971277607263, 533.7807509970385, 523.0741506992065, 328.54447207679294, 486.5808424485476, 399.6212002297361, 515.741643834014, 383.68218431403085, 482.4585369249958, 356.0655715212525, 391.54022972078394, 347.6534836029208, 330.64094307992855, 330.60579040560515, 312.04417622798474, 162.64763105371597, 148.79413910638252, 102.16397509829166, 388.8356407001276, 121.02017880270532, 234.94151308316455, 163.11709453504557, 112.78091700407757, 99.57278590836266, 89.11207963081797, 92.09110573157895, 76.49397311978743, 316.4324252955236, 130.74109916649962, 118.00219055452004, 80.72014572790668, 83.41253625875137, 136.30739986049116, 232.04411863615582, 104.52110492269387, 344.758408218955, 150.68996804684318, 103.00969623885022, 77.40262252409646, 80.17161917711847, 54.78536283935473, 68.6812584003253, 80.71288228924793, 68.83529911509915, 64.420057370819, 117.36505215737664, 102.40817925148215, 289.8026066186439, 135.42360791164364, 903.4700272713001, 283.26486112898664, 287.2770193023669, 227.73535491287564, 840.4014651471688, 301.51748466391916, 230.1474086360449, 267.1596107887799, 222.48737888294747, 367.7918225689464, 477.85459085133357, 416.1724532117493, 369.7732796586812, 411.4885948290799, 726.3693042900313, 407.2491893230347, 337.22786129410144, 423.1623773275868, 248.2712143726912, 340.2313241004905, 416.2414788947052, 315.050761230443, 323.52928776745586, 293.501200122763, 258.192907992285, 270.1254839027719, 288.497484719499, 366.5870743537655, 291.8639626594064, 318.7985808775961, 259.07112777202514, 415.5663475738626, 321.1740615711721, 684.2299615733535, 199.02398951540488, 240.7626989268778, 142.13483759579694, 122.35741067607869, 117.77126715176259, 100.19649479924917, 100.19481252882, 89.63971128243199, 79.92128507392549, 90.22672522907935, 2440.141862051071, 2424.703907147989, 153.0508755858589, 106.00373388121585, 349.8748696344583, 173.28500875700507, 177.14770007532582, 222.9683209460294, 165.23196144731182, 115.71613440819148, 87.88517732156932, 96.16744527225909, 75.74373622667815, 81.64649278845495, 114.71520925203572, 78.97903813671597, 505.839003680495, 641.0471666634195, 189.19335788223862, 297.336382049496, 1923.8212444591525, 443.2334100202017, 641.0400755361055, 386.63340481682894, 701.8902682460074, 203.54676664696055, 271.05196805948844, 548.5395969510465, 319.94626252923683, 385.6284437204671, 326.3230480765491, 259.4030297802916, 193.3653953237298, 320.16742950760204, 225.0028887254635, 232.97265223074587, 237.90803913422545, 227.35172035613533, 201.77050833025686, 200.9512110439797, 499.1307651063509, 367.7608592732599, 158.23952955340522, 109.6385301316876, 98.2114154041409, 281.73125323617165, 179.9967276229668, 233.99807411963388, 297.82787053021843, 134.27541888410173, 82.58376104144111, 79.6574546598002, 111.08818001905114, 428.79182569950444, 292.0349593290671, 157.4558019460863, 398.85018875974697, 380.3809745419733, 98.72364711871967, 198.12254808432243, 149.34745951658766, 76.64637771721476, 113.38537393651657, 66.52523640608607, 324.5022210127767, 76.94591230310004, 364.9984770277281, 213.63175879372466, 138.40103128553068, 93.01472476747587, 403.1628739904939, 259.38963651053825, 322.9966556903197, 294.28300598147916, 692.2833064600902, 176.2176576404283, 168.42758178535487, 217.21869364838676, 190.97375807136527, 202.1544829291471, 358.24964214588124, 384.6982320028323, 412.1154499766535, 227.88151322827017, 259.6052014766978, 210.62791989376808, 198.48278946619553, 210.27087398901264, 204.12688427317542, 214.34086192655477, 198.7032602872439, 543.7170485949576, 307.9239022862307, 176.81653775449468, 171.7334014199276, 139.43309513736574, 93.15248703540898, 108.13861090201036, 363.2043272789106, 447.54381561557017, 88.12209172900094, 181.35144930645532, 77.24568032995411, 114.35731801291463, 193.03965731131308, 190.6772794637516, 514.1840109316531, 330.0989870098013, 86.06649680309826, 168.4837085031274, 87.04414003075718, 178.3366613875128, 102.28647192424923, 81.28215117095255, 75.2875656540735, 100.53643189155294, 172.00101189580718, 2029.6592925343293, 338.5965771746001, 170.7557492550682, 162.8926464246697, 240.91450774600884, 247.47741894619242, 287.4134632458181, 302.42411183238585, 300.77209761090757, 300.72210826236335, 482.4448612495451, 381.40735275272, 232.90567884793884, 287.5713677178272, 270.6219078856134, 290.7963953369837, 262.0492053085073, 305.8448240019558, 337.50934986560185, 269.7568313721659, 317.87072804266853, 369.8993783519179, 294.0923772645099, 319.45002614411044, 223.40390720912882, 265.20407451020526, 237.8264235601531, 323.49374799100934, 167.36671604154395, 164.76742549340284, 209.90213592954214, 152.98907957853592, 143.37835561878993, 133.34235986725247, 137.43583638990734, 117.26723243969715, 101.88642604250704, 100.43055224912787, 100.32868205167054, 93.90503945507567, 81.88605551657099, 111.6976612778649, 179.82652349388817, 99.05913213684892, 260.77171822040066, 196.48388012649502, 232.6756024327256, 107.42669189175632, 105.7139452469051, 104.72033372130073, 217.68130414687346, 146.45119211334327, 348.1050317679907, 208.1438235765049, 241.36755950557898, 93.81761178241968, 83.99780643154607, 280.8175313389038, 278.5253246228546, 266.89183013764944, 309.0646673558155, 546.8122995158506, 637.2583559118232, 299.80877355283303, 132.1657603702577, 856.0712481347341, 514.9825534597775, 185.3991538373261, 283.637575612722, 305.39285595879005, 320.44324506129914, 359.5359540449164, 265.8565706544125, 274.58843468822107, 166.78107962498476, 179.09714673131222, 197.97884369361344, 187.2675882256975, 180.0276156204055, 275.9293951782335, 541.3713757776451, 182.00582108307208, 300.6133027247045, 259.64334480724045, 223.50917435983345, 305.6429098988058, 112.02073897607232, 207.37601005293632, 100.04428485638489, 103.1139041182877, 98.88908916916304, 350.2177395664926, 236.04856068336073, 70.092237682062, 635.9011915678965, 88.62396205647272, 316.71256461443636, 199.96524922612375, 87.88452715707592, 167.4832490655758, 128.0960318679811, 75.60464650680255, 82.50143611373323, 169.97716585029036, 48.50367635573029, 40.0825102798185, 45.19342907800737, 65.11198762043378, 124.59892672715162, 352.8585035453631, 384.89809100909196, 1326.800929916687, 243.63284832768986, 375.1208867918904, 200.83525298087167, 262.76050417661554, 223.9412682976521, 1106.195349921674, 260.74179877531725, 556.7519928356394, 364.25782864292205, 621.22843736302, 361.2182163745373, 297.81015242762936, 451.9532392950068, 312.08327890517353, 360.76170587272406, 408.8848639932738, 327.4361167443278, 275.398634704583, 281.0515774143391, 263.8221436809249, 258.42593234708596, 245.71270622359464, 974.6462767086178, 323.3116593016013, 198.74152883156592, 200.1340398219988, 166.30827339837793, 158.03796180453247, 140.3204518328359, 109.7837276134934, 105.93033289769087, 106.40147357167548, 133.21868514637995, 229.3593448999732, 166.50629718266518, 106.3041109611639, 130.99294401199205, 104.93451597978495, 81.99175163830375, 230.53890263351278, 209.11354817854698, 86.86413934052223, 99.757104419436, 86.15211835021094, 173.4281992835088, 74.00595781083403, 54.87562116734986, 104.67251551329319, 282.8696159441435, 405.33445432251347, 270.2256481494118, 126.02124887933434, 668.6532096478824, 309.09423842489866, 525.7931316417157, 153.36577313323124, 203.48478218692972, 129.3139405278344, 260.04436153490644, 340.77866482625853, 155.06305073856365, 180.29238345438193, 161.8932374645171, 125.74381203124777, 142.41913221241964, 133.47603487352012, 133.4463626948494, 131.4694863859409, 551.861696912943, 565.3323270293864, 320.4537856128901, 338.8477962686095, 562.8530572559116, 163.8967326782461, 143.61714446935528, 117.31531394208042, 127.62428890228793, 94.42893345875778, 95.01103017684466, 95.57433817471701, 318.8790458149471, 194.49966307574564, 149.32780398205506, 224.26022716695257, 274.1224490603512, 403.41419101676826, 124.91055323263446, 78.5226444313949, 85.78366536339244, 110.90627506766818, 109.77361993660467, 45.285984297512634, 88.76462878676479, 34.99185887481195, 35.68300852435737, 50.230208915849644, 40.47379143310339, 31.72302611084804, 142.4679088223475, 572.8942319777749, 226.65548258394628, 56.006605445474115, 117.39790980505985, 79.05032155669052, 160.42116080146226, 329.0918934087482, 217.5041762926576, 280.6497482214414, 182.71683315743473, 212.59570542283313, 190.59135284984094, 195.4890474631474, 130.52988552069377, 140.25252069828304, 137.51668350702806, 132.75235276125125], \"Total\": [2490.0, 2479.0, 2676.0, 3429.0, 5566.0, 3436.0, 975.0, 2220.0, 4203.0, 2038.0, 4990.0, 1698.0, 1700.0, 5385.0, 1545.0, 1255.0, 1772.0, 566.0, 564.0, 552.0, 971.0, 686.0, 3218.0, 1852.0, 1059.0, 825.0, 2187.0, 770.0, 3773.0, 494.0, 273.3245068216199, 104.97812275551922, 103.1108525573926, 91.24979719575356, 82.25180957473044, 107.6607144284548, 106.10330809705336, 954.572637755457, 1700.5274094646024, 92.55611881477694, 357.85262567235776, 83.27613060914628, 544.9275324237359, 387.0039349240208, 109.75473647388026, 1772.7581103628945, 256.6980655709834, 240.99036479465715, 104.29034537230984, 160.79298211894078, 127.34602809946689, 87.57622590902426, 140.92209248432104, 560.5360602400311, 102.78632759096557, 219.60159902809872, 107.89852368731802, 117.31095353507327, 102.18434339490874, 309.6152877556819, 988.5427859088, 2187.5730605477656, 777.019394419416, 3436.522940202132, 1073.0021644442998, 571.1293549116525, 4990.307921521608, 5566.776018187237, 736.6649495042204, 1374.2345542003468, 3773.3026745744646, 1532.4116993625166, 1852.8617468350553, 1080.0144905044297, 1118.1838746910378, 515.1255773579408, 3218.685978901415, 1220.2655380787955, 2477.1574786408687, 1086.1498393948632, 2346.6687653038557, 1012.8381014830097, 926.6812745438424, 5385.376906125238, 1616.5219437748924, 2038.7777072153278, 4203.590563185251, 2220.041437720386, 2109.0211975234374, 1228.217642285401, 3488.862490305256, 471.5687961347823, 284.9920102578541, 693.3133629768818, 271.8872590027497, 261.6044117255572, 206.58780089480874, 169.64529271462345, 184.88636388768805, 315.2591294350439, 152.01866118328203, 143.07737811326197, 125.62454243419009, 122.85869988730809, 171.82333252750703, 650.4009398244498, 294.2231806271848, 97.04555093532564, 98.30272974468754, 825.399300470919, 85.41794234435622, 87.97913509154705, 73.59101449601381, 373.31355438132744, 134.92749382328677, 209.3871167841999, 119.19939053618036, 971.9655340249877, 167.42869657990687, 78.44794546477038, 117.41238338178002, 260.53399761329405, 704.5642028815129, 306.34435236756104, 278.16522530116396, 286.57486323701346, 183.3411012762364, 231.03246727371996, 331.84701981732957, 1414.5787632703293, 517.8625263036357, 240.33538689022342, 1667.8759300489, 849.671031933958, 240.43039339378683, 5385.376906125238, 1035.2695814852013, 458.40970166824826, 3488.862490305256, 1100.2659450966407, 1749.2709772800301, 1734.2071785435746, 702.7195224223199, 2095.369504259219, 1276.532331953834, 3773.3026745744646, 1406.5282692175574, 4203.590563185251, 1141.5349342154527, 2346.6687653038557, 2477.1574786408687, 3218.685978901415, 4990.307921521608, 1323.5214108789914, 163.59324490102037, 149.78823613320702, 103.18959318427721, 400.4657187622436, 127.58646880322546, 258.62334530703816, 183.89835096192058, 128.46714258176374, 113.7498219684564, 104.56183731863902, 112.14590299989709, 93.87168598173508, 399.96591129183275, 166.9968653090005, 151.00823630662032, 103.79974253259086, 107.5496717813302, 178.6700433212677, 304.2074791008704, 137.08743881371635, 464.0638316371734, 208.86173384154063, 145.23027331394792, 112.46031223609585, 116.65769537217604, 81.10895974108229, 101.83951217700347, 120.27321071180631, 103.72128649399664, 97.16460696675284, 186.07437407484957, 165.19308911876178, 553.9615150308681, 235.8397104810385, 3488.862490305256, 691.0138455996089, 712.2008711196567, 543.169788138247, 4203.590563185251, 885.1151648147384, 570.1861040538766, 731.6142921209272, 549.2185452189425, 1323.5214108789914, 2095.369504259219, 1680.6405962225003, 1374.2345542003468, 1698.1060575999809, 5385.376906125238, 1734.2071785435746, 1276.090359894693, 2109.0211975234374, 699.565870031997, 1455.4187013940045, 2346.6687653038557, 1219.3390550759927, 1429.1344744670914, 1213.5733141550406, 878.7476705805054, 1071.8581863634324, 1414.5787632703293, 5566.776018187237, 1749.2709772800301, 4990.307921521608, 1451.6808753544863, 416.5446030885771, 322.1248498510184, 686.5511828592416, 199.97189780916614, 242.22294134606622, 143.14854058666057, 123.29864865366136, 118.72185203843527, 101.13747377273592, 101.13763332292689, 90.5792451597126, 80.86064300157085, 91.34948563061343, 2490.6303794939395, 2479.9533188317973, 157.72470013415258, 109.79192122402243, 365.58319536068973, 182.4245297521225, 187.01159224520438, 237.71736079647428, 179.49982737854825, 131.82509462661153, 104.21446900083383, 115.8628129725993, 92.39516523070543, 100.0006163126514, 140.56659611268114, 98.92248965340646, 635.9238170730588, 834.3352041899523, 250.84458993148442, 419.30440939710473, 3429.153729470909, 720.7389981486958, 1255.6988751720976, 671.5101770934349, 1545.6534572704022, 290.51883883433646, 438.98404819288794, 1451.6808753544863, 676.9856801650615, 1213.5733141550406, 927.5901421506352, 708.9039869727055, 369.8140874621744, 1983.0692168047892, 712.2008711196567, 1615.9081864510251, 2095.369504259219, 1615.4350787167537, 638.4504612346054, 599.7380825639644, 500.2883789173784, 369.030486777536, 159.28443682981634, 110.79103435963525, 99.37929529118287, 286.9256091476047, 185.39905394648633, 246.13584933529214, 316.4580518126222, 144.0040409198104, 89.72220715012753, 86.79043695658343, 121.1253593593898, 473.415920001474, 325.00448932914156, 179.56852866786448, 458.32902613277105, 437.4286555711037, 113.94751898920018, 234.81628820806992, 178.71549287921374, 92.11472936135789, 139.4736366376868, 84.41999137534346, 415.4883840877856, 101.37465253205987, 500.04027163104803, 301.22129133675526, 203.38980499135985, 140.29464369059107, 655.9434588408777, 427.1940161475097, 558.8644813921501, 544.436819725591, 1983.0692168047892, 294.85219561962964, 295.74209701846746, 521.2029658912546, 426.4084989882215, 503.16018612554586, 2095.369504259219, 3488.862490305256, 5385.376906125238, 824.597110528954, 1406.5282692175574, 864.4204087724057, 770.8989208326657, 1734.2071785435746, 1385.719735942004, 5566.776018187237, 1615.4350787167537, 544.8601992182063, 308.944478578345, 177.80321004995582, 172.74543084267125, 140.5770286159118, 94.12779591450324, 109.35950980750106, 367.5831818446828, 453.03065044942184, 89.55175579687969, 190.1255261317305, 81.59259863776562, 120.95149185234021, 204.59044003916037, 202.5441285909409, 551.0730204612433, 374.46509834429503, 98.31551257909825, 192.52529485054671, 101.23895127302006, 214.07610451763094, 124.07950766619713, 99.06141017741355, 92.95832556743795, 125.63996408069761, 217.29821295015608, 2676.1668612408616, 447.50097396791716, 238.50926008467246, 245.37473406878553, 384.57928915669703, 400.331592376178, 487.49123590880373, 523.7885224737817, 526.9134125762138, 581.1565145283007, 1276.090359894693, 1066.3528442577294, 477.62159625351586, 731.6142921209272, 728.7431502186547, 931.9554662438869, 776.487761255256, 1406.5282692175574, 2346.6687653038557, 1071.8581863634324, 2109.0211975234374, 5385.376906125238, 1983.0692168047892, 3218.685978901415, 557.2282760617034, 2187.5730605477656, 1385.719735942004, 324.4274868015481, 168.30621530680196, 165.7134107598449, 211.13469544080695, 153.9317694541019, 144.30969709270337, 134.27614720129716, 138.4305009561597, 118.2011452664706, 102.81565843297317, 101.36419984087242, 101.30381559352551, 94.94466419743702, 82.82310603700591, 113.2111491658908, 182.55322197307876, 100.97591741928645, 267.52583095157763, 202.8752609885851, 242.0889019481634, 112.98227902624562, 111.45191605788644, 110.90360105156032, 230.68093291991565, 155.19881754847898, 379.11623279721545, 231.3720576762392, 270.5865834974084, 106.23239040281392, 95.64998806920839, 338.04939078611005, 342.0748392372681, 357.4667488984954, 502.23550205130977, 1255.6988751720976, 1615.4350787167537, 581.872442326857, 158.98368584623276, 3429.153729470909, 1545.6534572704022, 281.56786332303795, 680.5015723329358, 913.8299597993338, 1113.9460583245184, 1451.6808753544863, 824.597110528954, 927.5901421506352, 261.51823932247305, 378.9948113502121, 1615.9081864510251, 1616.5219437748924, 3488.862490305256, 280.49110197243294, 553.4455757338252, 192.67228247309131, 320.9170727949072, 281.822742109367, 251.4538080879245, 348.31086536164946, 138.05259993328644, 262.92642922489387, 132.46462487248198, 140.87838421648533, 147.37468642453558, 562.5475550502389, 391.38665932222256, 116.74549367961575, 1059.9722366709395, 151.61391807472742, 641.3023762441729, 415.90821261532244, 192.70253014928562, 367.45435184309883, 282.2969749746612, 168.89053069621016, 187.1946436550085, 396.18671833535126, 113.05525570694671, 94.57710076632358, 106.75045796887014, 155.44577109593905, 299.22855694714474, 855.1771764976893, 948.9291149289519, 4203.590563185251, 655.9434588408777, 1113.9460583245184, 534.5370857856879, 766.1625724544242, 638.4504612346054, 5566.776018187237, 784.5883991911445, 2676.1668612408616, 1385.719735942004, 3429.153729470909, 1532.4116993625166, 1080.0144905044297, 3218.685978901415, 1566.7788382985304, 2220.041437720386, 3773.3026745744646, 2038.7777072153278, 1983.0692168047892, 2477.1574786408687, 1852.8617468350553, 1511.5592719379408, 1067.2782164170685, 975.7010727322956, 324.28119729309526, 199.68788263834978, 201.09670971283077, 167.27285320599876, 158.9856175652384, 141.32811508155464, 110.71218852321353, 106.84968583155573, 107.71647730732431, 135.83274027172828, 235.6479314275493, 178.9472324057085, 114.40353085714953, 146.9129117778177, 118.63111690170558, 93.88165693434162, 279.8744189344966, 262.13529195494175, 113.83688641388066, 131.22442902492304, 125.22601295148021, 253.32985904130044, 120.16229386758948, 91.99219345250316, 175.90070943936934, 534.5370857856879, 770.8989208326657, 521.2029658912546, 245.0718073683388, 1698.1060575999809, 931.9554662438869, 2038.7777072153278, 396.18671833535126, 775.7342644232442, 290.57128061094875, 1852.8617468350553, 4990.307921521608, 544.436819725591, 1545.6534572704022, 1143.9417101719323, 398.5269223869799, 1680.6405962225003, 1012.8381014830097, 1114.099979487763, 1220.2655380787955, 552.8305774898649, 566.5684858115205, 321.4011621528655, 340.082936045923, 564.9994642008375, 164.83045693574206, 144.57909678856385, 118.24539833578702, 128.65711501372078, 95.35661073272725, 95.94578331965003, 96.52728373860229, 333.62184042301703, 205.29068584269646, 157.7973151388458, 244.33416018066478, 309.43622412248874, 494.5634756876687, 160.09487884803252, 101.79396435633856, 121.32670050057555, 184.5966233950919, 262.6529878279485, 120.23983049215485, 245.0718073683388, 97.31855615456138, 102.93075554695059, 145.3014175064551, 119.5345842855335, 94.24087524336035, 449.6340169234717, 2220.041437720386, 877.616981863249, 176.80028508927074, 464.45843296586355, 278.14948560648105, 784.5883991911445, 3436.522940202132, 1749.2709772800301, 5385.376906125238, 1616.5219437748924, 3218.685978901415, 3773.3026745744646, 4990.307921521608, 736.6649495042204, 1511.5592719379408, 3488.862490305256, 5566.776018187237], \"Term\": [\"kong\", \"hong\", \"bank\", \"china\", \"percent\", \"share\", \"tonne\", \"stock\", \"market\", \"price\", \"million\", \"000\", \"quarter\", \"company\", \"chinese\", \"beijing\", \"profit\", \"gold\", \"gm\", \"bre\", \"computer\", \"deng\", \"billion\", \"1996\", \"rate\", \"apple\", \"sale\", \"export\", \"analyst\", \"canada\", \"dividend\", \"pretax\", \"kmart\", \"abnormal\", \"pence\", \"quaker\", \"snapple\", \"earning\", \"quarter\", \"exceptional\", \"penny\", \"sears\", \"fourth\", \"margin\", \"item\", \"profit\", \"fiscal\", \"expectation\", \"chemical\", \"stg\", \"cereal\", \"jones\", \"retailer\", \"net\", \"consensus\", \"earn\", \"offset\", \"outlook\", \"waste\", \"income\", \"revenue\", \"sale\", \"half\", \"share\", \"result\", \"acquisition\", \"million\", \"percent\", \"cent\", \"pound\", \"analyst\", \"rise\", \"1996\", \"growth\", \"increase\", \"forecast\", \"billion\", \"1997\", \"expect\", \"fall\", \"business\", \"1995\", \"strong\", \"company\", \"report\", \"price\", \"market\", \"stock\", \"group\", \"cost\", \"new\", \"microsoft\", \"online\", \"software\", \"mci\", \"digital\", \"wireless\", \"windows\", \"aol\", \"communications\", \"attorney\", \"web\", \"calif\", \"netscape\", \"cigarette\", \"internet\", \"cable\", \"robotics\", \"nynex\", \"apple\", \"modem\", \"sprint\", \"bskyb\", \"intel\", \"subscriber\", \"satellite\", \"device\", \"computer\", \"fcc\", \"optus\", \"morris\", \"distance\", \"network\", \"tobacco\", \"pc\", \"phone\", \"telecommunications\", \"3com\", \"telecom\", \"service\", \"technology\", \"user\", \"corp\", \"customer\", \"tv\", \"company\", \"amp\", \"television\", \"new\", \"offer\", \"base\", \"industry\", \"bid\", \"plan\", \"product\", \"analyst\", \"deal\", \"market\", \"long\", \"business\", \"expect\", \"billion\", \"million\", \"executive\", \"film\", \"lloyd\", \"insurer\", \"insurance\", \"mutual\", \"sun\", \"club\", \"pension\", \"st\", \"zealand\", \"prudential\", \"star\", \"life\", \"society\", \"care\", \"melbourne\", \"england\", \"regulation\", \"australian\", \"traditional\", \"australia\", \"young\", \"housing\", \"owner\", \"royal\", \"amro\", \"mortgage\", \"compensation\", \"franchise\", \"heart\", \"study\", \"float\", \"job\", \"team\", \"new\", \"national\", \"britain\", \"london\", \"market\", \"pay\", \"home\", \"fund\", \"build\", \"executive\", \"plan\", \"add\", \"pound\", \"000\", \"company\", \"industry\", \"financial\", \"group\", \"member\", \"big\", \"business\", \"large\", \"time\", \"british\", \"work\", \"investment\", \"service\", \"percent\", \"base\", \"million\", \"people\", \"tung\", \"handover\", \"deng\", \"legislature\", \"jiang\", \"colony\", \"provisional\", \"patten\", \"hwa\", \"chee\", \"corruption\", \"mao\", \"zemin\", \"kong\", \"hong\", \"xiaoping\", \"liberty\", \"territory\", \"colonial\", \"democratic\", \"democracy\", \"freedom\", \"elect\", \"hk\", \"flag\", \"chamber\", \"governor\", \"midnight\", \"xinhua\", \"political\", \"party\", \"civil\", \"communist\", \"china\", \"leader\", \"beijing\", \"law\", \"chinese\", \"death\", \"july\", \"people\", \"rule\", \"british\", \"right\", \"future\", \"reform\", \"government\", \"britain\", \"tell\", \"plan\", \"official\", \"economic\", \"public\", \"ford\", \"chrysler\", \"uaw\", \"apec\", \"skoda\", \"russia\", \"russian\", \"gas\", \"truck\", \"norilsk\", \"moscow\", \"gazprom\", \"encryption\", \"car\", \"vehicle\", \"republic\", \"strike\", \"plant\", \"nickel\", \"automaker\", \"auto\", \"steel\", \"detroit\", \"soviet\", \"worker\", \"tender\", \"union\", \"southern\", \"energy\", \"motor\", \"czech\", \"contract\", \"minister\", \"production\", \"government\", \"vote\", \"labour\", \"oil\", \"supply\", \"agreement\", \"plan\", \"new\", \"company\", \"talk\", \"deal\", \"country\", \"export\", \"industry\", \"state\", \"percent\", \"official\", \"thomson\", \"csf\", \"alcatel\", \"lagardere\", \"aerospatiale\", \"cnb\", \"suez\", \"franc\", \"nomura\", \"gec\", \"itt\", \"markets\", \"hilton\", \"natwest\", \"yen\", \"banking\", \"loan\", \"bzw\", \"scandal\", \"paris\", \"suspend\", \"sa\", \"mgam\", \"lending\", \"probe\", \"investigation\", \"bank\", \"french\", \"client\", \"privatisation\", \"defence\", \"credit\", \"finance\", \"asset\", \"japan\", \"debt\", \"financial\", \"firm\", \"securities\", \"fund\", \"merger\", \"source\", \"capital\", \"deal\", \"business\", \"investment\", \"group\", \"company\", \"government\", \"billion\", \"security\", \"sale\", \"state\", \"boeing\", \"hwang\", \"plane\", \"korean\", \"seoul\", \"airbus\", \"jail\", \"xinjiang\", \"pyongyang\", \"activist\", \"747\", \"cathay\", \"airlines\", \"embassy\", \"riot\", \"airport\", \"wang\", \"airline\", \"flight\", \"korea\", \"smoke\", \"jet\", \"guerrilla\", \"textile\", \"kill\", \"air\", \"aircraft\", \"cargo\", \"aviation\", \"arrest\", \"police\", \"human\", \"taiwan\", \"south\", \"beijing\", \"official\", \"north\", \"colombia\", \"china\", \"chinese\", \"washington\", \"states\", \"united\", \"trade\", \"people\", \"talk\", \"right\", \"pacific\", \"war\", \"tell\", \"report\", \"new\", \"inflation\", \"crown\", \"economist\", \"yuan\", \"index\", \"deficit\", \"shanghai\", \"casino\", \"currency\", \"dealer\", \"enterprise\", \"klaus\", \"central\", \"economy\", \"shenzhen\", \"rate\", \"fix\", \"point\", \"domestic\", \"factor\", \"dollar\", \"prague\", \"bureau\", \"wage\", \"import\", \"strengthen\", \"buying\", \"finish\", \"42\", \"budget\", \"investor\", \"foreign\", \"market\", \"czech\", \"trade\", \"trader\", \"trading\", \"economic\", \"percent\", \"exchange\", \"bank\", \"state\", \"china\", \"rise\", \"growth\", \"billion\", \"high\", \"stock\", \"analyst\", \"price\", \"government\", \"expect\", \"1996\", \"week\", \"issue\", \"tonne\", \"crop\", \"cocoa\", \"coffee\", \"ivory\", \"corn\", \"grain\", \"bean\", \"harvest\", \"lme\", \"zinc\", \"coast\", \"port\", \"rain\", \"exporter\", \"sugar\", \"farmer\", \"metal\", \"producer\", \"arrival\", \"season\", \"weather\", \"output\", \"shipment\", \"grade\", \"factory\", \"trader\", \"export\", \"oil\", \"copper\", \"000\", \"source\", \"price\", \"import\", \"total\", \"quality\", \"1996\", \"million\", \"production\", \"chinese\", \"world\", \"april\", \"add\", \"1995\", \"10\", \"1997\", \"bre\", \"gold\", \"busang\", \"toronto\", \"gm\", \"freeport\", \"indonesian\", \"barrick\", \"wmx\", \"strathcona\", \"walsh\", \"ounce\", \"conrail\", \"csx\", \"hughes\", \"mining\", \"canadian\", \"canada\", \"indonesia\", \"surcharge\", \"bankruptcy\", \"norfolk\", \"deposit\", \"services\", \"copper\", \"hire\", \"bidding\", \"rich\", \"resource\", \"consultant\", \"project\", \"stock\", \"shareholder\", \"test\", \"board\", \"site\", \"exchange\", \"share\", \"base\", \"company\", \"report\", \"billion\", \"analyst\", \"million\", \"cent\", \"week\", \"new\", \"percent\"]}, \"topic.order\": [2, 6, 4, 1, 10, 5, 9, 7, 8, 3], \"token.table\": {\"Term\": [\"000\", \"000\", \"000\", \"000\", \"000\", \"000\", \"000\", \"000\", \"000\", \"000\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"1995\", \"1995\", \"1995\", \"1995\", \"1995\", \"1995\", \"1995\", \"1995\", \"1995\", \"1995\", \"1996\", \"1996\", \"1996\", \"1996\", \"1996\", \"1996\", \"1996\", \"1996\", \"1996\", \"1996\", \"1997\", \"1997\", \"1997\", \"1997\", \"1997\", \"1997\", \"1997\", \"1997\", \"1997\", \"1997\", \"3com\", \"3com\", \"42\", \"42\", \"42\", \"42\", \"42\", \"42\", \"747\", \"abnormal\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"activist\", \"add\", \"add\", \"add\", \"add\", \"add\", \"add\", \"add\", \"add\", \"add\", \"add\", \"aerospatiale\", \"agreement\", \"agreement\", \"agreement\", \"agreement\", \"agreement\", \"agreement\", \"agreement\", \"agreement\", \"air\", \"air\", \"airbus\", \"aircraft\", \"aircraft\", \"aircraft\", \"airline\", \"airline\", \"airlines\", \"airport\", \"airport\", \"alcatel\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"amro\", \"amro\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"aol\", \"apec\", \"apple\", \"apple\", \"apple\", \"april\", \"april\", \"april\", \"april\", \"april\", \"april\", \"april\", \"april\", \"april\", \"april\", \"arrest\", \"arrest\", \"arrival\", \"arrival\", \"arrival\", \"arrival\", \"asset\", \"asset\", \"asset\", \"asset\", \"asset\", \"asset\", \"attorney\", \"australia\", \"australia\", \"australia\", \"australia\", \"australia\", \"australia\", \"australia\", \"australia\", \"australian\", \"australian\", \"australian\", \"australian\", \"auto\", \"auto\", \"auto\", \"auto\", \"automaker\", \"automaker\", \"automaker\", \"aviation\", \"aviation\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"banking\", \"banking\", \"banking\", \"banking\", \"bankruptcy\", \"bankruptcy\", \"bankruptcy\", \"barrick\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"bean\", \"beijing\", \"beijing\", \"beijing\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"bidding\", \"bidding\", \"bidding\", \"bidding\", \"bidding\", \"bidding\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"boeing\", \"bre\", \"britain\", \"britain\", \"britain\", \"britain\", \"britain\", \"britain\", \"britain\", \"british\", \"british\", \"british\", \"british\", \"british\", \"british\", \"british\", \"british\", \"bskyb\", \"budget\", \"budget\", \"budget\", \"budget\", \"budget\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"bureau\", \"bureau\", \"bureau\", \"bureau\", \"bureau\", \"bureau\", \"bureau\", \"bureau\", \"busang\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"buying\", \"buying\", \"buying\", \"buying\", \"buying\", \"bzw\", \"bzw\", \"cable\", \"cable\", \"calif\", \"canada\", \"canada\", \"canada\", \"canada\", \"canada\", \"canada\", \"canada\", \"canadian\", \"canadian\", \"canadian\", \"canadian\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"car\", \"car\", \"car\", \"car\", \"car\", \"care\", \"care\", \"care\", \"care\", \"cargo\", \"cargo\", \"casino\", \"casino\", \"casino\", \"casino\", \"cathay\", \"cent\", \"cent\", \"cent\", \"cent\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"cereal\", \"cereal\", \"chamber\", \"chamber\", \"chee\", \"chemical\", \"chemical\", \"china\", \"china\", \"china\", \"china\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"chrysler\", \"cigarette\", \"cigarette\", \"civil\", \"civil\", \"civil\", \"civil\", \"client\", \"client\", \"client\", \"client\", \"client\", \"club\", \"club\", \"club\", \"club\", \"cnb\", \"coast\", \"coast\", \"coast\", \"cocoa\", \"coffee\", \"colombia\", \"colombia\", \"colombia\", \"colonial\", \"colonial\", \"colony\", \"communications\", \"communications\", \"communist\", \"communist\", \"communist\", \"communist\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"compensation\", \"compensation\", \"compensation\", \"compensation\", \"compensation\", \"computer\", \"computer\", \"computer\", \"computer\", \"conrail\", \"conrail\", \"conrail\", \"conrail\", \"consensus\", \"consensus\", \"consultant\", \"consultant\", \"consultant\", \"consultant\", \"consultant\", \"consultant\", \"contract\", \"contract\", \"contract\", \"contract\", \"contract\", \"contract\", \"contract\", \"contract\", \"contract\", \"copper\", \"copper\", \"copper\", \"corn\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"corruption\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"crop\", \"crown\", \"crown\", \"crown\", \"crown\", \"csf\", \"csx\", \"csx\", \"csx\", \"csx\", \"currency\", \"currency\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"czech\", \"czech\", \"czech\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"dealer\", \"dealer\", \"death\", \"death\", \"death\", \"death\", \"debt\", \"debt\", \"debt\", \"debt\", \"debt\", \"defence\", \"defence\", \"defence\", \"defence\", \"defence\", \"deficit\", \"deficit\", \"deficit\", \"deficit\", \"deficit\", \"democracy\", \"democracy\", \"democratic\", \"democratic\", \"deng\", \"deng\", \"deposit\", \"deposit\", \"deposit\", \"deposit\", \"detroit\", \"detroit\", \"device\", \"device\", \"digital\", \"distance\", \"distance\", \"distance\", \"dividend\", \"dollar\", \"dollar\", \"dollar\", \"dollar\", \"dollar\", \"dollar\", \"dollar\", \"dollar\", \"dollar\", \"domestic\", \"domestic\", \"domestic\", \"domestic\", \"domestic\", \"domestic\", \"domestic\", \"domestic\", \"domestic\", \"earn\", \"earn\", \"earn\", \"earn\", \"earning\", \"earning\", \"earning\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economist\", \"economist\", \"economist\", \"economy\", \"economy\", \"economy\", \"economy\", \"economy\", \"economy\", \"economy\", \"economy\", \"elect\", \"elect\", \"elect\", \"elect\", \"embassy\", \"encryption\", \"encryption\", \"energy\", \"energy\", \"energy\", \"energy\", \"england\", \"england\", \"england\", \"enterprise\", \"enterprise\", \"enterprise\", \"enterprise\", \"exceptional\", \"exceptional\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"export\", \"export\", \"export\", \"export\", \"exporter\", \"exporter\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factory\", \"factory\", \"factory\", \"factory\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"farmer\", \"farmer\", \"farmer\", \"fcc\", \"fcc\", \"film\", \"finance\", \"finance\", \"finance\", \"finance\", \"finance\", \"finance\", \"finance\", \"finance\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"fiscal\", \"fiscal\", \"fiscal\", \"fiscal\", \"fiscal\", \"fix\", \"fix\", \"fix\", \"fix\", \"fix\", \"fix\", \"flag\", \"flag\", \"flag\", \"flag\", \"flag\", \"flight\", \"flight\", \"float\", \"float\", \"float\", \"float\", \"float\", \"ford\", \"forecast\", \"forecast\", \"forecast\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"fourth\", \"fourth\", \"fourth\", \"fourth\", \"fourth\", \"franc\", \"franc\", \"franchise\", \"franchise\", \"franchise\", \"freedom\", \"freedom\", \"freedom\", \"freeport\", \"french\", \"french\", \"french\", \"french\", \"french\", \"french\", \"fund\", \"fund\", \"fund\", \"fund\", \"fund\", \"fund\", \"fund\", \"fund\", \"fund\", \"future\", \"future\", \"future\", \"future\", \"future\", \"future\", \"future\", \"future\", \"future\", \"future\", \"gas\", \"gas\", \"gas\", \"gazprom\", \"gazprom\", \"gec\", \"gec\", \"gm\", \"gm\", \"gold\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"governor\", \"governor\", \"grade\", \"grade\", \"grade\", \"grade\", \"grade\", \"grain\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"growth\", \"growth\", \"growth\", \"growth\", \"growth\", \"guerrilla\", \"guerrilla\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"handover\", \"harvest\", \"heart\", \"heart\", \"heart\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"hilton\", \"hilton\", \"hire\", \"hire\", \"hire\", \"hire\", \"hire\", \"hire\", \"hk\", \"hk\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"hong\", \"hong\", \"hong\", \"hong\", \"housing\", \"housing\", \"housing\", \"housing\", \"hughes\", \"hughes\", \"human\", \"human\", \"hwa\", \"hwang\", \"import\", \"import\", \"import\", \"import\", \"income\", \"income\", \"income\", \"income\", \"income\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"index\", \"index\", \"indonesia\", \"indonesia\", \"indonesia\", \"indonesia\", \"indonesia\", \"indonesia\", \"indonesian\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"inflation\", \"inflation\", \"insurance\", \"insurance\", \"insurer\", \"intel\", \"intel\", \"internet\", \"internet\", \"investigation\", \"investigation\", \"investigation\", \"investigation\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investor\", \"investor\", \"investor\", \"investor\", \"investor\", \"investor\", \"investor\", \"investor\", \"investor\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"item\", \"item\", \"item\", \"itt\", \"itt\", \"itt\", \"ivory\", \"jail\", \"japan\", \"japan\", \"japan\", \"japan\", \"japan\", \"japan\", \"japan\", \"japan\", \"jet\", \"jet\", \"jet\", \"jiang\", \"job\", \"job\", \"job\", \"job\", \"job\", \"job\", \"jones\", \"jones\", \"jones\", \"jones\", \"july\", \"july\", \"july\", \"july\", \"july\", \"july\", \"july\", \"july\", \"july\", \"kill\", \"kill\", \"kill\", \"kill\", \"klaus\", \"klaus\", \"klaus\", \"klaus\", \"kmart\", \"kong\", \"kong\", \"kong\", \"kong\", \"korea\", \"korea\", \"korean\", \"labour\", \"labour\", \"labour\", \"labour\", \"labour\", \"lagardere\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"law\", \"law\", \"law\", \"law\", \"law\", \"law\", \"law\", \"leader\", \"leader\", \"leader\", \"leader\", \"leader\", \"leader\", \"legislature\", \"lending\", \"lending\", \"lending\", \"lending\", \"liberty\", \"liberty\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"lloyd\", \"lme\", \"lme\", \"loan\", \"loan\", \"loan\", \"loan\", \"london\", \"london\", \"london\", \"london\", \"london\", \"london\", \"london\", \"london\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"mao\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"markets\", \"markets\", \"mci\", \"melbourne\", \"melbourne\", \"melbourne\", \"member\", \"member\", \"member\", \"member\", \"member\", \"member\", \"member\", \"merger\", \"merger\", \"merger\", \"merger\", \"merger\", \"metal\", \"metal\", \"metal\", \"metal\", \"mgam\", \"mgam\", \"microsoft\", \"midnight\", \"midnight\", \"midnight\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"mining\", \"mining\", \"minister\", \"minister\", \"minister\", \"minister\", \"minister\", \"minister\", \"minister\", \"minister\", \"modem\", \"morris\", \"morris\", \"mortgage\", \"mortgage\", \"mortgage\", \"moscow\", \"moscow\", \"motor\", \"motor\", \"motor\", \"mutual\", \"mutual\", \"mutual\", \"national\", \"national\", \"national\", \"national\", \"national\", \"national\", \"national\", \"national\", \"national\", \"national\", \"natwest\", \"natwest\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"netscape\", \"network\", \"network\", \"network\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nickel\", \"nickel\", \"nomura\", \"nomura\", \"nomura\", \"norfolk\", \"norfolk\", \"norfolk\", \"norfolk\", \"norilsk\", \"norilsk\", \"north\", \"north\", \"north\", \"north\", \"north\", \"north\", \"nynex\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"official\", \"official\", \"official\", \"official\", \"official\", \"official\", \"official\", \"official\", \"official\", \"official\", \"offset\", \"offset\", \"offset\", \"oil\", \"oil\", \"oil\", \"oil\", \"online\", \"optus\", \"optus\", \"ounce\", \"outlook\", \"outlook\", \"outlook\", \"outlook\", \"output\", \"output\", \"output\", \"owner\", \"owner\", \"owner\", \"owner\", \"owner\", \"pacific\", \"pacific\", \"pacific\", \"pacific\", \"pacific\", \"pacific\", \"pacific\", \"pacific\", \"paris\", \"paris\", \"paris\", \"paris\", \"party\", \"party\", \"party\", \"party\", \"party\", \"patten\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pc\", \"pc\", \"pence\", \"penny\", \"penny\", \"penny\", \"pension\", \"pension\", \"pension\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"phone\", \"phone\", \"phone\", \"phone\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plane\", \"plant\", \"plant\", \"plant\", \"plant\", \"plant\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"police\", \"police\", \"police\", \"political\", \"political\", \"political\", \"political\", \"port\", \"port\", \"port\", \"pound\", \"pound\", \"pound\", \"pound\", \"pound\", \"prague\", \"prague\", \"prague\", \"prague\", \"pretax\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"privatisation\", \"privatisation\", \"privatisation\", \"probe\", \"probe\", \"probe\", \"producer\", \"producer\", \"producer\", \"producer\", \"producer\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"production\", \"production\", \"production\", \"production\", \"production\", \"production\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"provisional\", \"prudential\", \"prudential\", \"prudential\", \"public\", \"public\", \"public\", \"public\", \"public\", \"public\", \"public\", \"public\", \"public\", \"pyongyang\", \"quaker\", \"quaker\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"rain\", \"rain\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"reform\", \"reform\", \"reform\", \"reform\", \"reform\", \"regulation\", \"regulation\", \"regulation\", \"regulation\", \"regulation\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"republic\", \"republic\", \"republic\", \"republic\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"retailer\", \"retailer\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"riot\", \"riot\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"robotics\", \"royal\", \"royal\", \"royal\", \"royal\", \"royal\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"russia\", \"russia\", \"russia\", \"russian\", \"russian\", \"russian\", \"sa\", \"sa\", \"sa\", \"sa\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"satellite\", \"satellite\", \"satellite\", \"scandal\", \"scandal\", \"scandal\", \"scandal\", \"sears\", \"sears\", \"season\", \"season\", \"season\", \"securities\", \"securities\", \"securities\", \"securities\", \"securities\", \"securities\", \"securities\", \"security\", \"security\", \"security\", \"security\", \"security\", \"security\", \"security\", \"security\", \"seoul\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"services\", \"services\", \"services\", \"services\", \"shanghai\", \"shanghai\", \"shanghai\", \"share\", \"share\", \"share\", \"share\", \"share\", \"share\", \"share\", \"share\", \"share\", \"shareholder\", \"shareholder\", \"shareholder\", \"shareholder\", \"shareholder\", \"shareholder\", \"shenzhen\", \"shenzhen\", \"shipment\", \"shipment\", \"shipment\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"skoda\", \"smoke\", \"smoke\", \"snapple\", \"snapple\", \"society\", \"society\", \"society\", \"software\", \"software\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"south\", \"south\", \"south\", \"south\", \"south\", \"south\", \"south\", \"south\", \"south\", \"south\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"soviet\", \"soviet\", \"soviet\", \"soviet\", \"sprint\", \"st\", \"st\", \"st\", \"star\", \"star\", \"star\", \"star\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"steel\", \"steel\", \"steel\", \"steel\", \"steel\", \"stg\", \"stg\", \"stg\", \"stg\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"strathcona\", \"strengthen\", \"strengthen\", \"strengthen\", \"strengthen\", \"strengthen\", \"strengthen\", \"strengthen\", \"strengthen\", \"strike\", \"strike\", \"strike\", \"strike\", \"strike\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"study\", \"study\", \"study\", \"study\", \"study\", \"subscriber\", \"subscriber\", \"suez\", \"sugar\", \"sugar\", \"sun\", \"sun\", \"sun\", \"sun\", \"sun\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"surcharge\", \"surcharge\", \"suspend\", \"suspend\", \"suspend\", \"suspend\", \"suspend\", \"taiwan\", \"taiwan\", \"taiwan\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"telecom\", \"telecom\", \"telecom\", \"telecom\", \"telecom\", \"telecom\", \"telecommunications\", \"telecommunications\", \"telecommunications\", \"television\", \"television\", \"television\", \"television\", \"television\", \"television\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tender\", \"tender\", \"tender\", \"tender\", \"territory\", \"territory\", \"territory\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"textile\", \"textile\", \"textile\", \"thomson\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tobacco\", \"tobacco\", \"tobacco\", \"tonne\", \"toronto\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trader\", \"trader\", \"trader\", \"trader\", \"trader\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"traditional\", \"traditional\", \"traditional\", \"traditional\", \"traditional\", \"traditional\", \"truck\", \"truck\", \"truck\", \"tung\", \"tv\", \"tv\", \"tv\", \"uaw\", \"union\", \"union\", \"union\", \"united\", \"united\", \"united\", \"united\", \"united\", \"united\", \"user\", \"user\", \"user\", \"vehicle\", \"vehicle\", \"vehicle\", \"vehicle\", \"vehicle\", \"vote\", \"vote\", \"vote\", \"vote\", \"vote\", \"wage\", \"wage\", \"wage\", \"wage\", \"walsh\", \"wang\", \"wang\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"washington\", \"washington\", \"washington\", \"washington\", \"waste\", \"waste\", \"waste\", \"weather\", \"weather\", \"weather\", \"weather\", \"web\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"windows\", \"wireless\", \"wmx\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"worker\", \"worker\", \"worker\", \"worker\", \"worker\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"xiaoping\", \"xiaoping\", \"xinhua\", \"xinhua\", \"xinjiang\", \"yen\", \"yen\", \"yen\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"yuan\", \"yuan\", \"yuan\", \"yuan\", \"zealand\", \"zealand\", \"zealand\", \"zealand\", \"zemin\", \"zinc\", \"zinc\"], \"Topic\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 8, 7, 1, 1, 2, 3, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 10, 2, 7, 7, 1, 7, 10, 3, 7, 7, 3, 7, 6, 1, 2, 3, 5, 6, 8, 9, 10, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 4, 6, 7, 9, 1, 2, 3, 6, 8, 10, 2, 1, 2, 3, 4, 5, 6, 8, 10, 1, 3, 5, 8, 1, 2, 3, 5, 1, 5, 10, 6, 7, 1, 2, 3, 4, 6, 8, 9, 10, 2, 3, 6, 8, 6, 8, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 4, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 7, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 9, 2, 1, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 6, 7, 8, 9, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 8, 9, 1, 6, 2, 4, 2, 1, 2, 3, 4, 5, 7, 10, 1, 3, 5, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 7, 1, 3, 4, 9, 7, 9, 1, 2, 6, 8, 7, 1, 2, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 9, 4, 5, 4, 1, 5, 4, 7, 8, 9, 4, 7, 8, 9, 5, 2, 3, 4, 5, 6, 7, 1, 2, 3, 6, 9, 1, 3, 4, 7, 6, 5, 7, 9, 9, 9, 5, 7, 9, 4, 6, 4, 2, 8, 4, 5, 7, 8, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 10, 1, 2, 3, 10, 2, 5, 9, 10, 1, 5, 1, 2, 5, 6, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 8, 9, 10, 9, 1, 2, 3, 5, 6, 8, 9, 10, 4, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 10, 9, 1, 3, 6, 8, 6, 2, 5, 9, 10, 1, 8, 1, 2, 3, 6, 7, 9, 3, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 8, 4, 7, 8, 10, 1, 2, 3, 6, 8, 2, 4, 5, 6, 7, 1, 4, 5, 8, 9, 4, 7, 4, 5, 4, 8, 3, 6, 8, 10, 3, 5, 2, 7, 2, 2, 3, 5, 1, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 5, 1, 2, 6, 1, 3, 4, 5, 6, 7, 8, 10, 1, 4, 8, 1, 3, 4, 5, 7, 8, 9, 10, 3, 4, 5, 10, 7, 2, 5, 1, 2, 5, 10, 1, 3, 6, 2, 3, 4, 8, 1, 9, 1, 2, 3, 4, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 9, 5, 7, 8, 9, 8, 9, 1, 2, 3, 4, 5, 7, 8, 9, 3, 4, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 9, 2, 9, 3, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 8, 10, 1, 4, 5, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 9, 1, 2, 3, 5, 6, 8, 1, 2, 4, 7, 9, 3, 7, 1, 2, 3, 4, 6, 5, 1, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 7, 1, 6, 1, 2, 3, 3, 4, 7, 10, 1, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 10, 5, 10, 2, 6, 1, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 8, 3, 6, 7, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 8, 9, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 9, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 5, 6, 10, 4, 7, 1, 2, 3, 4, 5, 7, 8, 9, 4, 7, 8, 9, 1, 3, 4, 7, 2, 10, 4, 7, 4, 7, 5, 7, 8, 9, 1, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 8, 1, 5, 7, 8, 9, 10, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 8, 1, 3, 3, 1, 2, 2, 10, 2, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 7, 1, 2, 6, 9, 7, 1, 2, 3, 5, 6, 7, 8, 9, 5, 7, 10, 4, 1, 2, 3, 4, 5, 6, 1, 2, 8, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 4, 5, 7, 10, 3, 5, 6, 8, 1, 4, 7, 8, 9, 5, 7, 7, 2, 3, 4, 5, 7, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 5, 7, 4, 1, 3, 6, 9, 4, 7, 1, 3, 4, 5, 7, 8, 3, 8, 9, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 2, 1, 3, 6, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 6, 10, 5, 8, 9, 10, 3, 6, 2, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 10, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 1, 3, 6, 5, 9, 1, 3, 5, 3, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 6, 9, 10, 2, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 10, 1, 2, 6, 2, 5, 9, 10, 5, 9, 1, 2, 5, 7, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 1, 5, 9, 10, 2, 1, 2, 10, 1, 2, 6, 8, 5, 8, 9, 1, 2, 3, 5, 9, 1, 2, 4, 5, 6, 7, 8, 9, 5, 6, 7, 9, 2, 3, 4, 5, 7, 4, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 1, 1, 2, 10, 1, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 3, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 7, 4, 5, 7, 8, 4, 7, 9, 1, 2, 3, 6, 10, 3, 5, 7, 8, 1, 1, 2, 3, 5, 6, 8, 9, 10, 2, 5, 6, 2, 6, 10, 1, 5, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 1, 3, 5, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 10, 7, 1, 6, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 5, 7, 8, 9, 9, 10, 1, 2, 3, 5, 6, 8, 9, 10, 3, 4, 5, 6, 8, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 10, 1, 2, 3, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 5, 8, 10, 5, 9, 10, 1, 2, 5, 6, 1, 2, 3, 5, 6, 7, 8, 9, 2, 3, 10, 3, 6, 7, 10, 1, 2, 3, 8, 9, 1, 2, 3, 5, 6, 8, 10, 2, 3, 4, 5, 6, 7, 8, 10, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 10, 4, 8, 9, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 5, 6, 10, 4, 8, 1, 7, 9, 2, 3, 5, 7, 9, 10, 5, 7, 10, 1, 6, 3, 4, 7, 1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 8, 9, 10, 3, 5, 7, 10, 2, 1, 2, 3, 1, 3, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 8, 1, 3, 5, 7, 9, 1, 3, 4, 6, 1, 2, 3, 4, 6, 8, 9, 10, 10, 1, 2, 3, 4, 5, 6, 8, 9, 1, 3, 5, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 7, 10, 1, 2, 6, 8, 9, 2, 3, 7, 9, 10, 1, 2, 3, 5, 8, 9, 8, 10, 1, 2, 3, 5, 6, 4, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 5, 7, 8, 10, 1, 2, 3, 5, 6, 8, 1, 2, 6, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 6, 9, 4, 5, 7, 1, 2, 3, 4, 5, 7, 8, 10, 1, 7, 8, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 8, 9, 5, 7, 10, 4, 2, 3, 7, 5, 3, 5, 9, 1, 2, 3, 4, 5, 7, 2, 7, 9, 1, 2, 3, 5, 7, 1, 2, 3, 4, 5, 1, 3, 5, 8, 10, 4, 7, 1, 2, 3, 4, 5, 7, 8, 9, 10, 2, 4, 5, 7, 1, 7, 9, 1, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 4, 7, 7, 1, 6, 8, 2, 3, 4, 5, 6, 7, 9, 4, 7, 8, 9, 1, 2, 3, 7, 4, 9, 10], \"Freq\": [0.058889136843039744, 0.09716707579101558, 0.24203435242489335, 0.03592237347425424, 0.08597813979083803, 0.015311175579190333, 0.05241133179030537, 0.0011777827368607949, 0.3939683254799359, 0.017077849684481526, 0.3294138827367522, 0.08975855115442839, 0.1175837020123012, 0.03500583495022708, 0.03680100597331564, 0.021542052277062816, 0.08886096564288411, 0.10232474831604838, 0.11937887303538977, 0.05924064376192274, 0.5765975817308809, 0.01974649252503017, 0.04541693280756939, 0.023695791030036203, 0.009873246262515085, 0.06121412682759353, 0.042454958928814866, 0.08787189173638425, 0.13131417529145062, 0.0009873246262515085, 0.6120262355985433, 0.00971470215235783, 0.02050881565497764, 0.002158822700523962, 0.01942940430471566, 0.0286044007819425, 0.016191170253929715, 0.1424822982345815, 0.14032347553405755, 0.00917499647722684, 0.5736456354426681, 0.046711144600331544, 0.013111900238689557, 0.06637899495836588, 0.009833925179017168, 0.008194937649180974, 0.0032779750596723892, 0.16881571557312805, 0.10735368320427074, 0.002458481294754292, 0.051940751625107216, 0.9479187171582067, 0.39885292190891725, 0.04503178150584549, 0.006433111643692213, 0.012866223287384427, 0.11579600958645984, 0.41815225683999385, 0.9865416010483581, 0.9863035619348015, 0.773905239152648, 0.15583159792892684, 0.014007334645296797, 0.05427842175052509, 0.9920667878278006, 0.28441535987788163, 0.061881166165898935, 0.24752466466359574, 0.013685257902073802, 0.07675644649424003, 0.05652606524769614, 0.050575953116359706, 0.09996188380645213, 0.08449159226497739, 0.023800448525345744, 0.9887817474061101, 0.021861825127108405, 0.24445495369403034, 0.0019874386479189457, 0.03974877295837892, 0.40146260687962704, 0.0039748772958378914, 0.23451776045443562, 0.04968596619797364, 0.07913140457915087, 0.91792429311815, 0.9909243999600246, 0.02593225845964446, 0.8989849599343414, 0.07347473230232597, 0.022427740822851622, 0.9756067257940456, 0.9900503708615727, 0.010955709126267523, 0.9860138213640771, 0.9954825897140431, 0.27818841116420534, 0.5467175024963202, 0.07341083072388753, 0.0009659319832090464, 0.04539880321082518, 0.0009659319832090464, 0.022216435613808067, 0.03187575544589853, 0.3082273534244, 0.67810017753368, 0.5568066442581209, 0.136750227718796, 0.012720951415701953, 0.0378978344259454, 0.013250991058022868, 0.045583409239598666, 0.0047703567808882325, 0.10839310685462705, 0.033657517287378085, 0.050618785841647355, 0.9952058990774112, 0.9928601229855162, 0.9886124201152625, 0.0012115348285726257, 0.009692278628581005, 0.1656098905556807, 0.07025874144786454, 0.17815609438565652, 0.06273101914987905, 0.015055444595970973, 0.0727679822138597, 0.010036963063980648, 0.05269405608589841, 0.31616433651539044, 0.05520329685189357, 0.115002628040485, 0.8782018868546126, 0.1581209796493975, 0.008784498869410972, 0.06149149208587681, 0.7642514016387546, 0.1909167454218234, 0.028637511813273512, 0.11455004725309405, 0.5765685711739067, 0.04772918635545585, 0.040092516538582916, 0.9932991043642078, 0.13360231022803448, 0.0021548759714199108, 0.7434322101398693, 0.0043097519428398215, 0.03878776748555839, 0.03016826359987875, 0.02154875971419911, 0.02801338762845884, 0.21695719051705314, 0.7626373969690353, 0.00657446031869858, 0.00986169047804787, 0.1454826304151051, 0.0111909715703927, 0.00559548578519635, 0.8337273819942561, 0.017034593428440593, 0.8432123747078093, 0.1320180990704146, 0.11295989814874899, 0.8848525354985337, 0.0044840253325743465, 0.0029893502217162313, 0.021299120329728145, 0.0018683438885726445, 0.7585476187604936, 0.20813350918699258, 0.0022420126662871733, 0.0007473375554290578, 0.007258566199905839, 0.05080996339934087, 0.9327257566879004, 0.00544392464992938, 0.23078184673675498, 0.057695461684188744, 0.7088299578343188, 0.9894676803214751, 0.22923835426774267, 0.3052700278777421, 0.16692668191067547, 0.03315666969458622, 0.036015003288947106, 0.03029833610022534, 0.019436668441653992, 0.03201333625684187, 0.023438335473759227, 0.12462334471413443, 0.9935672076154091, 0.5104727038257073, 0.4356139921882401, 0.05335674127351387, 0.08822865741125559, 0.46818110142424335, 0.03842215725974034, 0.021345642922077964, 0.0597678001818183, 0.24476337217316066, 0.028460857229437288, 0.0028460857229437287, 0.04838345729004339, 0.009715269208762995, 0.2428817302190749, 0.04857634604381498, 0.009715269208762995, 0.34003442230670483, 0.34974969151546786, 0.20131663810514713, 0.1600913879812262, 0.2336097507022185, 0.04053816262185556, 0.08519885025610321, 0.13260788789861228, 0.00961922502891488, 0.05084447515283579, 0.01923845005782976, 0.06733457520240416, 0.47503857475461775, 0.10283699688932536, 0.06306921561490347, 0.0034175437032706313, 0.048777669219408105, 0.09910876739484831, 0.001242743164825684, 0.14042997762530232, 0.000310685791206421, 0.06617607352696768, 0.10549921483200063, 0.11195835043395985, 0.18516188725616436, 0.015071316404571518, 0.0818157176248168, 0.1571722996476744, 0.004306090401306148, 0.08612180802612296, 0.25190628847640967, 0.9955999819385793, 0.9984975912627044, 0.13760162894203348, 0.0645885197074851, 0.4029761990445266, 0.3159221072648728, 0.00982868778157382, 0.0673967162165062, 0.0014040982545105456, 0.2257795197076944, 0.08899338733004013, 0.24225977662066478, 0.3180689584203286, 0.03872860374548043, 0.08157727171920345, 0.004120064228242598, 0.0008240128456485197, 0.9919689312606795, 0.1136255187234877, 0.0334192702127905, 0.06349661340430195, 0.3742958263832536, 0.41774087765988127, 0.11106689774228716, 0.14930304286668108, 0.40421067702930735, 0.04369845157073593, 0.16204842457481242, 0.027311532231709956, 0.021849225785367965, 0.025490763416262625, 0.052802295647972584, 0.08289393101133855, 0.08289393101133855, 0.20723482752834635, 0.005920995072238467, 0.08289393101133855, 0.4499956254901235, 0.011841990144476934, 0.08289393101133855, 0.9956404571051332, 0.4461643737199657, 0.1670453051558993, 0.17727256873687272, 0.04218746227151538, 0.006818175720648951, 0.14403396209870908, 0.0034090878603244756, 0.006818175720648951, 0.0017045439301622378, 0.004687495807946153, 0.0634403037456657, 0.07401368770327664, 0.0634403037456657, 0.42293535830443796, 0.3700684385163832, 0.11188468341809353, 0.8747347976323676, 0.9924438971040769, 0.00339878046953451, 0.9950284998290261, 0.05661558399772089, 0.008087940571102984, 0.05054962856939365, 0.004043970285551492, 0.04448367314106641, 0.020219851427757462, 0.8148600125386256, 0.04847525541826133, 0.016158418472753778, 0.045243571723710574, 0.885481332306907, 0.07727101828753243, 0.016742053962298692, 0.2781756658351167, 0.042499060058142836, 0.020605604876675312, 0.33741677985555824, 0.09401307224983112, 0.08499812011628567, 0.019317754571883106, 0.03090840731501297, 0.010561537516491698, 0.019010767529685058, 0.004224615006596679, 0.9061799189149877, 0.05703230258905517, 0.10595448560509112, 0.781414331337547, 0.09271017490445473, 0.01324431070063639, 0.8906576109022354, 0.10347889255295681, 0.10865423764020896, 0.014487231685361195, 0.05794892674144478, 0.8112849743802268, 0.9871296496989118, 0.728960975218658, 0.08280562288331125, 0.010859753820762132, 0.17782846881497988, 0.03021966738168793, 0.04088543233993073, 0.1226562970197922, 0.031997294874728396, 0.03910780484689026, 0.07288272721465913, 0.6221696225641633, 0.03555254986080933, 0.005332882479121399, 0.8716408486121027, 0.11778930386650037, 0.8225538621012513, 0.17316923412657922, 0.9887516319539089, 0.9205070676224741, 0.07670892230187284, 0.5610713755597238, 0.24962427103904553, 0.18109424335893373, 0.00787366275473625, 0.45417683808615167, 0.3331924097070771, 0.09575238181873283, 0.11645559950926966, 0.9972075836158295, 0.989388329857849, 0.005819931352104994, 0.7534545594609928, 0.015946128242560694, 0.03189225648512139, 0.19135353891072834, 0.09223960525553535, 0.11320315190452065, 0.06289063994695591, 0.7169532953952974, 0.012578127989391183, 0.01631335998559989, 0.8863592258842606, 0.021751146647466517, 0.07069122660426619, 0.9880184603968882, 0.008487237668007734, 0.016974475336015468, 0.9717887129868855, 0.9965552109158492, 0.9945463567534402, 0.15724883887885024, 0.8302738692803293, 0.00628995355515401, 0.9483373767501087, 0.04385375152601658, 0.9919765819340277, 0.9928340554670301, 0.0031719937874346007, 0.7083159474212073, 0.08585647847529784, 0.14786393515190185, 0.05485275013699585, 0.3021144161979596, 0.3483507343499522, 0.134809505937135, 0.07650346617920058, 0.06870456914151508, 0.00018568802470679752, 0.015226418025957398, 0.0020425682717747726, 0.05217833494261011, 0.024943210397770754, 0.12471605198885377, 0.6734666807398103, 0.10808724505700659, 0.05820082426146509, 0.007201901461477173, 0.9774009126290449, 0.007201901461477173, 0.008230744527402483, 0.008992217044891723, 0.008992217044891723, 0.023979245453044595, 0.9561724124401532, 0.846416075357934, 0.14593380609619552, 0.03183332065043997, 0.28649988585395975, 0.14855549636871987, 0.0954999619513199, 0.08488885506783993, 0.3395554202713597, 0.08427084331529876, 0.007022570276274897, 0.06788484600399067, 0.6062819005183994, 0.02106771082882469, 0.011704283793791494, 0.03979456489889108, 0.11236112442039835, 0.049157991933924276, 0.12241310137689769, 0.5141350257829703, 0.3631588674181298, 0.9938005866170003, 0.19305992382212708, 0.595967590929175, 0.05036345838838098, 0.08154083739071206, 0.011991299616281184, 0.0011991299616281184, 0.00839390973139683, 0.05635910819652157, 0.9936051006089612, 0.4901411437795592, 0.1400403267941598, 0.1872632276898648, 0.11968562813221795, 0.011398631250687423, 0.004070939732388365, 0.008956067411254404, 0.031753329912629254, 0.007327691518299058, 0.017352667576766304, 0.03933271317400362, 0.18856565433419384, 0.13303711808854166, 0.24409419057984602, 0.002313689010235507, 0.1920361878495471, 0.11684129501689312, 0.06015591426612319, 0.004627378020471014, 0.10741095836272238, 0.10241509983422366, 0.0474606560207378, 0.012489646321246788, 0.004995858528498715, 0.6169885282695913, 0.09242338277722623, 0.00999171705699743, 0.9960491163108132, 0.00180686239775985, 0.00542058719327955, 0.01264803678431895, 0.9775125571880788, 0.9969428857162582, 0.00487114160048278, 0.01461342480144834, 0.02922684960289668, 0.9450014704936593, 0.20538064643859424, 0.7872924780146112, 0.12004646053171329, 0.6426016416697594, 0.2012543603031664, 0.015300039087375225, 0.002353852167288496, 0.01765389125466372, 0.012196173149034607, 0.6143822223826183, 0.3719832810455555, 0.09882488894256267, 0.27301264283412996, 0.03270463950617182, 0.014930378904991482, 0.1848523102522755, 0.2175569497584473, 0.09313712555018497, 0.011375526784755416, 0.005687763392377708, 0.06896413113257971, 0.24157392987603316, 0.7549185308626036, 0.7021919845835801, 0.1789901137173832, 0.03442117571488138, 0.08261082171571531, 0.24089896009103826, 0.013765654862345043, 0.0034414137155862608, 0.5179327641957322, 0.22369189151310695, 0.02860268431022567, 0.09880927307168869, 0.17941683794596103, 0.6266588107967624, 0.06240585667685601, 0.003976873556237158, 0.007953747112474315, 0.0835143446809803, 0.8908196765971232, 0.01590749422494863, 0.9380888263811965, 0.058893468920792606, 0.9464653921983752, 0.048125358925341116, 0.9962840602085676, 0.0014565556435797771, 0.034265743841054165, 0.25889673124352036, 0.2893551702133463, 0.41880353583510643, 0.17924534415735466, 0.810188955591243, 0.9815486427716861, 0.01677860927814848, 0.993867031083404, 0.967244207314698, 0.015353082655788856, 0.011514811991841642, 0.9951540868507474, 0.20954983826910456, 0.17689272061677658, 0.013607132355136659, 0.010885705884109326, 0.04082139706540998, 0.008164279413081996, 0.4544782206615644, 0.010885705884109326, 0.07347851471773796, 0.088961936498767, 0.02404376662128838, 0.03366127326980373, 0.028852519945546055, 0.11781445644431306, 0.004808753324257676, 0.03366127326980373, 0.4808753324257676, 0.18754137964604936, 0.8196661627084438, 0.05464441084722959, 0.022768504519678996, 0.10018141988658757, 0.9826386834275648, 0.0020951784294830807, 0.013618659791640024, 0.059519104938098706, 0.04855505929160684, 0.31639103151305104, 0.13000225552268926, 0.0031325844704262478, 0.08301348846629557, 0.35084946068773976, 0.007831461176065618, 0.041521281096139416, 0.005190160137017427, 0.9446091449371717, 0.06643047068856428, 0.06387545258515796, 0.10475574223965906, 0.14308101379075383, 0.010220072413625275, 0.6029842724038912, 0.005110036206812637, 0.0025550181034063187, 0.0834438998974891, 0.8799538534644304, 0.007585809081589918, 0.022757427244769753, 0.9900618791495438, 0.07430318512654474, 0.9164059498940519, 0.21141669319082473, 0.029500003701045312, 0.6785000851240421, 0.07866667653612083, 0.07438423444253354, 0.7717364323412855, 0.1394704395797504, 0.09937649468272185, 0.092278173633956, 0.06388488943889262, 0.7311270680228822, 0.9831872939930526, 0.010804255977945633, 0.23579242337857914, 0.07137500383351585, 0.029314733717336866, 0.036962055556642136, 0.08029687931270532, 0.00892187547918948, 0.3326585000097792, 0.20392858238147385, 0.25160152095978894, 0.23573475837673918, 0.2780461252648719, 0.06648929082420849, 0.013600082214042646, 0.07706713254624166, 0.0007555601230023692, 0.0007555601230023692, 0.011333401845035538, 0.06497817057820375, 0.4751413707641145, 0.14048359985209163, 0.06014958729299325, 0.025836064340614553, 0.04602048960671967, 0.03915778501624393, 0.02381762181400404, 0.11343646999551077, 0.04803893213333018, 0.028258195372547167, 0.9170491118527061, 0.041495434925461816, 0.020747717462730908, 0.020747717462730908, 0.2568430109957031, 0.045401542347725296, 0.16993148707291467, 0.5253607043093927, 0.10210130490562397, 0.891684729509116, 0.28541400031121433, 0.005189345460203897, 0.17643774564693251, 0.005189345460203897, 0.015568036380611692, 0.04151476368163118, 0.456662400497943, 0.010378690920407795, 0.02842512697041415, 0.06253527933491113, 0.3069913712804728, 0.5969276663786971, 0.570823635480558, 0.05155826384985685, 0.020255032226729478, 0.023017082075828952, 0.03314459818919369, 0.05800304683108896, 0.011048199396397897, 0.1141647270961116, 0.0681305629444537, 0.049716897283790536, 0.03195512412076538, 0.085213664322041, 0.8734400593009203, 0.9735487603357574, 0.017918075343602898, 0.9963736589406286, 0.12102781681810026, 0.014359232503842405, 0.004102637858240687, 0.01230791357472206, 0.08205275716481374, 0.5887285326575386, 0.1702594711169885, 0.00615395678736103, 0.14105584185657055, 0.09952273286546921, 0.26408788169813485, 0.0086200792245682, 0.022725663410225253, 0.37771619874926116, 0.04858590108392986, 0.036047604030012476, 0.07494113048520042, 0.06557348917455037, 0.11241169572780063, 0.09367641310650053, 0.42154385897925234, 0.22482339145560126, 0.07689762393523585, 0.12378641901769673, 0.17255076590345605, 0.00843998311484296, 0.08064872754183272, 0.3572926185283519, 0.0009377759016492177, 0.07783539983688506, 0.05063989868905775, 0.05063989868905775, 0.9232636773979258, 0.003895627330792936, 0.03895627330792936, 0.023373763984757615, 0.011686881992378807, 0.013191401062626985, 0.1714882138141508, 0.09233980743838889, 0.0989355079697024, 0.03297850265656746, 0.5870173472869008, 0.060416278704155474, 0.017261793915472994, 0.8285661079427037, 0.06904717566189197, 0.017261793915472994, 0.029574823321299848, 0.9661108951624617, 0.1573915721214457, 0.07869578606072285, 0.6174592444764408, 0.006053522004670988, 0.12712396209809076, 0.9974247274738492, 0.7202127331802174, 0.14947811443363002, 0.12812409808596859, 0.0010538194942778965, 0.0010538194942778965, 0.012645833931334758, 0.20338716239563404, 0.06428298915095168, 0.13067361729045918, 0.1759878555444087, 0.40572050529699016, 0.0010538194942778965, 0.004215277977111586, 0.9652659641924317, 0.011010638374818612, 0.001835106395803102, 0.00917553197901551, 0.012845744770621715, 0.010881890678257811, 0.9875315790518964, 0.22174811726165036, 0.1060534473860067, 0.6652443517849511, 0.044568287985752505, 0.9192209397061454, 0.033426215989314376, 0.9949617506911007, 0.05810043220568281, 0.004469264015821755, 0.004469264015821755, 0.16312813657749406, 0.7575402506817874, 0.011173160039554387, 0.023236287457859148, 0.010934723509580776, 0.3649463971322584, 0.030070489651347135, 0.004100521316092791, 0.3936500463449079, 0.15992033132761885, 0.009567883070883179, 0.004100521316092791, 0.13965219806813095, 0.07899518274560943, 0.19607732860070912, 0.3653527201984436, 0.09592272190538288, 0.03526570658286135, 0.0042318847899433625, 0.0521932457426348, 0.022570052213031266, 0.008463769579886725, 0.012188391118570169, 0.9506945072484733, 0.03656517335571051, 0.9217605395859417, 0.06913204046894562, 0.011166726895541714, 0.9826719668076708, 0.0017699131828637189, 0.9964611219522738, 0.997231604208847, 0.0005042688331430232, 0.017649409160005812, 0.0499226144811593, 0.16136602660576743, 0.3489540325349721, 0.14825503694404882, 0.08522143280117092, 0.13867392911433138, 0.01260672082857558, 0.0368116248194407, 0.8199949462674053, 0.17999889064406457, 0.2500212152444785, 0.043481950477300614, 0.010870487619325154, 0.5978768190628834, 0.08696390095460123, 0.9906026123621033, 0.3380715190711479, 0.12327993682818858, 0.20056697414739913, 0.0815544197478786, 0.06021750760453827, 0.15078084581293835, 0.03698398104845658, 0.0009483072063706814, 0.0071123040477801104, 0.664805895025216, 0.021296010564874606, 0.03240697259872223, 0.27592222384054926, 0.004629567514103175, 0.045084198823043126, 0.9467681752839057, 0.7799033130348044, 0.010295753307390158, 0.04761785904667948, 0.059200581517493404, 0.012869691634237698, 0.016730599124509006, 0.007721814980542618, 0.03603513657586555, 0.028313321595322936, 0.996508031430861, 0.9920478396829802, 0.6586760549744117, 0.2367117072564292, 0.09262632023077665, 0.3312528783983429, 0.11935315657127189, 0.09637607830086661, 0.05106017393423397, 0.014041547831914341, 0.011488539135202643, 0.04914541741170019, 0.19913467834351248, 0.0804197739464185, 0.04850716523752227, 0.04960666386260791, 0.9425266133895502, 0.10275532637494122, 0.20551065274988245, 0.05137766318747061, 0.18495958747489422, 0.08220426109995299, 0.3596436423122943, 0.8444124970717445, 0.1439339483645019, 0.17538133477653298, 0.2543029354259728, 0.4033770699860258, 0.09295210743156247, 0.010522880086591978, 0.0473529603896639, 0.0035076266955306593, 0.012276693434357307, 0.9778409865966011, 0.01774226944752596, 0.002822633775742766, 0.0012097001896040427, 0.22722535217339346, 0.7092185234502887, 0.03442808366263537, 0.027542466930108297, 0.050697947509188, 0.9442492723586264, 0.18417022468086955, 0.8156109950152794, 0.9887531917665661, 0.9922390548416713, 0.017668436815377712, 0.16406405614279304, 0.4290906083734587, 0.38618154753611283, 0.7977642247268754, 0.045217405450106295, 0.03875777610009111, 0.045217405450106295, 0.06782610817515945, 0.6322752599122834, 0.005365843789920368, 0.1761785377690521, 0.006260151088240429, 0.022357682458001533, 0.012520302176480858, 0.01967476056304135, 0.09121934442864625, 0.030406448142882084, 0.0035772291932802455, 0.074514923255734, 0.9225657164995639, 0.012492591982898264, 0.07495555189738959, 0.043724071940143924, 0.049970367931593054, 0.037477775948694794, 0.7807869989311416, 0.9959946022528365, 0.11359657740861442, 0.3015787308868292, 0.23468937566145215, 0.12109279825283771, 0.1037938270738609, 0.015569074061079135, 0.034021309985321074, 0.0743855760696003, 0.001153264745265121, 0.014260701932687782, 0.9839884333554569, 0.027468019070393132, 0.9713690380348117, 0.9884717717400744, 0.013393566725125296, 0.9830877976241967, 0.991695983978886, 0.00615005261382255, 0.03681576526280518, 0.013805911973551944, 0.7915389531503114, 0.15646700236692204, 0.15300531552258248, 0.10635735347301466, 0.2518989950676663, 0.0018659184819827133, 0.041050206603619695, 0.2518989950676663, 0.00559775544594814, 0.15767011172753928, 0.029854695711723413, 0.1543539790673502, 0.13564440584706536, 0.0993946077327634, 0.0023386966525356097, 0.0058467416313390235, 0.04794328137697999, 0.0011693483262678048, 0.41277995917253507, 0.14032179915213658, 0.06371347128987698, 0.0824527275516055, 0.08432665317777835, 0.11805731444888969, 0.12086820288814897, 0.10212894662642044, 0.14522923602839605, 0.2304928520192608, 0.004684814065432131, 0.04872206628049416, 0.9384560822531083, 0.018222448199089483, 0.036444896398178966, 0.03681778108611242, 0.005259683012301774, 0.9520026252266212, 0.9923905572147369, 0.9904960990623, 0.01708060524782755, 0.049343970715946255, 0.04744612568840986, 0.013284915192754761, 0.5712513532884548, 0.24861769860726768, 0.0018978450275363944, 0.049343970715946255, 0.008972479212296495, 0.9510827965034285, 0.03588991684918598, 0.9949511745697159, 0.11372631182960334, 0.03610359105701693, 0.5235020703267456, 0.0812330798782881, 0.23828370097631174, 0.00541553865855254, 0.8678154283441051, 0.022837248114318556, 0.04567449622863711, 0.057093120285796384, 0.1754050068948439, 0.025057858127834846, 0.020501883922773963, 0.6173345047857494, 0.07517357438350453, 0.011389935512652202, 0.0045559742050608806, 0.06833961307591321, 0.0022779871025304403, 0.019330044180671024, 0.006443348060223674, 0.9407288167926564, 0.025773392240894696, 0.006785425803175895, 0.2917733095365635, 0.03392712901587947, 0.6717571545144135, 0.989226618441795, 0.9796716606724171, 0.01565868637960011, 0.003212038231712843, 0.0016060191158564214, 0.03304571145402187, 0.9624563460983869, 0.9946257272475377, 0.003381324505647079, 0.2603619869348251, 0.06086384110164742, 0.5680625169487092, 0.10482105967505945, 0.995684801392228, 0.1927273624359995, 0.10497490379492738, 0.2583366773078291, 0.025423609512833976, 0.12547781469237415, 0.08119152715388915, 0.04182593823079138, 0.05822826694874878, 0.06396908200003387, 0.04920698615387221, 0.08041575815534716, 0.07445903532902515, 0.5763129334466546, 0.11913445652644024, 0.08488330027508867, 0.01638098777238553, 0.0491429633171566, 0.04023647960564083, 0.04023647960564083, 0.012487183325888532, 0.6146469125965133, 0.09712253697913302, 0.19424507395826604, 0.9951398280467707, 0.05378754371358247, 0.10757508742716494, 0.8068131557037371, 0.03227252622814948, 0.9654626571632234, 0.027324414825374246, 0.04750404837910489, 0.7900673309366919, 0.12251044055663894, 0.010000852290337873, 0.020001704580675746, 0.007500639217753405, 0.9947376632934909, 0.009283630740605401, 0.9840648585041725, 0.01869333091642089, 0.8812570289169849, 0.09079617873690148, 0.008011427535608953, 0.20619703533933276, 0.025774629417416595, 0.4197582505122131, 0.09021120296095808, 0.05154925883483319, 0.10862165254482707, 0.03497985420935109, 0.06259552858515459, 0.08584932187585906, 0.311860801916386, 0.1690706032861306, 0.08847736234144658, 0.05518884977733797, 0.06219695768557136, 0.09022938931850494, 0.09110540280703411, 0.03679256651822531, 0.009636148373820916, 0.9893564660182812, 0.9560626304035923, 0.002583953055144844, 0.01291976527572422, 0.015503718330869064, 0.01291976527572422, 0.2600158087641591, 0.11466387907074535, 0.19982916684528237, 0.002141026787628025, 0.00927778274638811, 0.053525669690700634, 0.001427351191752017, 0.31568250524248775, 0.02616810518212031, 0.017366106166316207, 0.0490240544704085, 0.9437130485553636, 0.9967366657562253, 0.21194657581248316, 0.7803487564005062, 0.00963393526420378, 0.008576747747464539, 0.13008067416987884, 0.35450557356186757, 0.24014893692900707, 0.21584815164452423, 0.04860157056896572, 0.002858915915821513, 0.12624475437250557, 0.32658969065930793, 0.07135573073228577, 0.37187313516248927, 0.10428914491641765, 0.03930334198415791, 0.010719093268406704, 0.8253701816673161, 0.12148305704194264, 0.17161072075951608, 0.8176746106776942, 0.998793821517784, 0.8181175555237432, 0.15650944540454217, 0.021342197100619388, 0.6731047568254663, 0.06632857234570687, 0.06392391111262988, 0.017634182375897898, 0.016031074887179905, 0.04368467906756525, 0.01042019867666694, 0.0018034959248077395, 0.06833245670660436, 0.039075745037501025, 0.07776235621720304, 0.91677725224492, 0.0017893425567303304, 0.1359900343115051, 0.5779576458238967, 0.09125647039324686, 0.11451792363074115, 0.055469619258640246, 0.008946712783651651, 0.014314740453842643, 0.9833999473010028, 0.017033978379407965, 0.970936767626254, 0.17674868639114127, 0.6775366311660416, 0.14729057199261775, 0.9250775547810632, 0.06687307624923348, 0.13542926158965143, 0.19957996444790738, 0.6628905962019781, 0.948376431568275, 0.03135128699399256, 0.00783782174849814, 0.021707235094521368, 0.030390129132329916, 0.40954316878330316, 0.10130043044109972, 0.1244548145419225, 0.07380459932137265, 0.08538179137178405, 0.09261753640329116, 0.05788596025205698, 0.004341447018904274, 0.04887813916469369, 0.9433480858785882, 0.8491871152699233, 0.051736189795856674, 0.026760098170270693, 0.010704039268108277, 0.0035680130893694256, 0.026760098170270693, 0.026760098170270693, 0.0035680130893694256, 0.9930106708918804, 0.004257951209741651, 0.9566197051219577, 0.03832156088767486, 0.17168919717084946, 0.2616898781585736, 0.25882361443284985, 0.05474563716132262, 0.11035115344036235, 0.0074522856868816136, 0.05159274706302655, 0.030955648237815932, 0.012898186765756638, 0.03955443941498702, 0.8688210228551191, 0.1228635789896128, 0.008829424667032713, 0.002207356166758178, 0.9888955627076639, 0.016251651546080038, 0.11917877800458694, 0.2600264247372806, 0.6013111072049614, 0.9305294430912449, 0.062498246177770174, 0.22341666410621092, 0.027497435582302884, 0.17014038266549908, 0.515576917168179, 0.024060256134515023, 0.03780897392566646, 0.9867477765055864, 0.04907904333552464, 0.42535170890788015, 0.20086052920649897, 0.01090645407456103, 0.12269760833881159, 0.13269519124049253, 0.009997582901680945, 0.0009088711728800858, 0.009997582901680945, 0.03726371808808352, 0.0006190282810958678, 0.07923561998027108, 0.0006190282810958678, 0.140519419808762, 0.1231866279380777, 0.06809311092054546, 0.3943210150580678, 0.11018703403506448, 0.05137934733095703, 0.03218947061698513, 0.8155811311655896, 0.009267967399608973, 0.1668234131929615, 0.01726774517602762, 0.41634452257755483, 0.5180323552808286, 0.04604732046940699, 0.9965191646707691, 0.01274730643454624, 0.9687952890255142, 0.9945374642465835, 0.809813552249383, 0.034097412726289814, 0.008524353181572453, 0.15343835726830415, 0.1736865925182024, 0.13815978950311555, 0.682904102401114, 0.09781228400741877, 0.07113620655085003, 0.6846859880519315, 0.12448836146398753, 0.017784051637712506, 0.18354360339973128, 0.09177180169986564, 0.011471475212483205, 0.02294295042496641, 0.007647650141655469, 0.6385787868282317, 0.026766775495794145, 0.019119125354138674, 0.009877621087788743, 0.8593530346376207, 0.0888985897900987, 0.02963286326336623, 0.015581267498620738, 0.08629625076159178, 0.768276343585838, 0.06472218807119383, 0.0635236290328384, 0.9939198047702156, 0.12653720606339683, 0.23273807543803346, 0.3411985377780879, 0.022595929654178008, 0.16720987944091725, 0.045191859308356015, 0.024855522619595807, 0.03163430151584921, 0.009038371861671203, 0.03594985674134212, 0.9598611749938347, 0.9847807655393513, 0.9752618116026819, 0.011177785806334462, 0.011177785806334462, 0.10897728180643242, 0.8796023460090616, 0.007784091557602315, 0.04270911124654735, 0.055797387273715096, 0.17841386794928654, 0.37818229152184674, 0.00826627959610594, 0.007577422963097112, 0.2479883878831782, 0.0509753908426533, 0.01997684235725602, 0.010332849495132424, 0.599090027891226, 0.0185026305465656, 0.06592684864650074, 0.0014370975181798526, 0.03844235861131105, 0.029819773502231937, 0.0026945578465872233, 0.1986787318883646, 0.021556462772697786, 0.02389174623974005, 0.010468468748840795, 0.9351832082297776, 0.02791591666357545, 0.02093693749768159, 0.13744592512899884, 0.23241724145077233, 0.22812205629049112, 0.11358378534965875, 0.17085292082007494, 0.07110917654243343, 0.01002209870732283, 0.006681399138215221, 0.013840041072017244, 0.01670349784553805, 0.99569491233948, 0.020574783762736495, 0.016002609593239497, 0.8687130922044298, 0.013716522508490998, 0.08001304796619749, 0.20115316078430348, 0.06081374628362663, 0.11694951208389737, 0.04989845848912954, 0.021830575588994174, 0.0015593268277852982, 0.015593268277852982, 0.49430660440793955, 0.017152595105638282, 0.020271248761208876, 0.15678182373514188, 0.011832590470576745, 0.8312394805580163, 0.7956927959216027, 0.11007607848717825, 0.07076319331318602, 0.02201521569743565, 0.0391176767916121, 0.027941197708294358, 0.9332360034570316, 0.6760126916911762, 0.012370522883476853, 0.2692407921697903, 0.0407499577338061, 0.001455355633350218, 0.02479658168716939, 0.5136434778056517, 0.00354236881245277, 0.4534232079939546, 0.9906826038621671, 0.38895853981213185, 0.13341326964552316, 0.03286282744944872, 0.002452449809660352, 0.016676658705690395, 0.16039021755178703, 0.257997719976269, 0.006866859467048986, 0.020376996103430755, 0.3138057399928336, 0.6642900729718425, 0.07959250922403222, 0.8038843431627253, 0.1114295129136451, 0.03433341589711241, 0.07248165578279286, 0.022888943931408275, 0.7972982136107215, 0.06866683179422482, 0.39716972873221024, 0.3133488984080554, 0.1872259667988131, 0.003133488984080554, 0.01645081716642291, 0.0023501167380604155, 0.05013582374528886, 0.029768145348765263, 0.09918506251509086, 0.016530843752515144, 0.5400075625821613, 0.0018367604169461271, 0.2846978646266497, 0.055102812508383815, 0.926240297760574, 0.0011281855027534397, 0.0050768347623904786, 0.0005640927513767198, 0.0011281855027534397, 0.04794788386702119, 0.014102318784417997, 0.003384556508260319, 0.0005640927513767198, 0.04892868237712635, 0.15345814018280537, 0.1356658920456685, 0.00667209305142632, 0.23352325679992122, 0.044480620342842135, 0.0022240310171421068, 0.020016279154278963, 0.040032558308557925, 0.3158124044341792, 0.9894674542840353, 0.15158823947421066, 0.026750865789566588, 0.8203598842133754, 0.001667394532834833, 0.13839374622529113, 0.22843305099837213, 0.3351463010998014, 0.1367263516924563, 0.11004803916709897, 0.035015285189531495, 0.01167176172984383, 0.001667394532834833, 0.989838124971101, 0.9845745550057777, 0.009288439198167716, 0.030973467099283863, 0.09980339398658133, 0.299410181959744, 0.024090474410554116, 0.024090474410554116, 0.017207481721824368, 0.058505437854202855, 0.4439530284230687, 0.9796960582508499, 0.0005880528560929471, 0.0011761057121858942, 0.0023522114243717885, 0.00411636999265063, 0.007644687129208312, 0.00411636999265063, 0.9265448295678685, 0.06118692270731207, 0.2320815503362744, 0.056605256179579115, 0.04151052119835802, 0.022642102471831647, 0.03490657464407379, 0.6000157155035386, 0.012264472172242143, 0.0009434209363263186, 0.13520307012402316, 0.5218838506787293, 0.1162746403066599, 0.1541314999413864, 0.06760153506201158, 0.7611796441749195, 0.050372182335104966, 0.050372182335104966, 0.02798454574172498, 0.10074436467020993, 0.45839155036127827, 0.04082839719817053, 0.07237761321493867, 0.03959117304065021, 0.009897793260162553, 0.1039268292317068, 0.11568045872814985, 0.01299085365396335, 0.033405052253048614, 0.11320601041310921, 0.016706713711225497, 0.055689045704084984, 0.8743180175541343, 0.04455123656326799, 0.03346311884471157, 0.1171209159564905, 0.2509733913353368, 0.17568137393473576, 0.07529201740060104, 0.008365779711177893, 0.33463118844711576, 0.7465036199762302, 0.013979468538880714, 0.009319645692587144, 0.004659822846293572, 0.02795893707776143, 0.05778180329404029, 0.013979468538880714, 0.06430555527885129, 0.005591787415552286, 0.05498590958626415, 0.8657265716769973, 0.12773014991955697, 0.7900517925301542, 0.16590076053129998, 0.010115900032396341, 0.033382470106907924, 0.0481757172099785, 0.21334960478704765, 0.013764490631422429, 0.17205613289278035, 0.15829164226135792, 0.041293471894267283, 0.0068822453157112145, 0.3441122657855607, 0.053903117042699546, 0.05282505470184556, 0.06791792747380143, 0.3514483231184011, 0.06360567811038546, 0.030185745543911745, 0.2964671437348475, 0.0237173714987878, 0.003234187022561973, 0.05713730406526152, 0.00883305228652593, 0.9893018560909043, 0.6342942959808725, 0.009135926073798576, 0.013051322962569395, 0.016314153703211744, 0.0032628307406423487, 0.0006525661481284697, 0.009135926073798576, 0.23557637947437757, 0.0646040486647185, 0.015009021406954802, 0.9892261837328078, 0.06857670190103958, 0.017144175475259895, 0.6857670190103957, 0.04286043868814973, 0.17144175475259893, 0.0029542722373571677, 0.056131172509786185, 0.22452469003914474, 0.4726835579771468, 0.0635168531031791, 0.04874549191639326, 0.0812424865273221, 0.05022262803507185, 0.9828331491140242, 0.003485223933028455, 0.010455671799085364, 0.9708787405784459, 0.021575083123965465, 0.005393770780991366, 0.04029674274217116, 0.08865283403277655, 0.04835609129060539, 0.8220535519402916, 0.7812310504372679, 0.035655951980167876, 0.016913720811105273, 0.017370848400594604, 0.1211388112146729, 0.007314041431829308, 0.0009142551789786635, 0.019199358758551933, 0.9838236619510323, 0.004775843019179769, 0.004775843019179769, 0.06232947213152092, 0.872612609841293, 0.03116473606576046, 0.025970613388133716, 0.9726676708860403, 0.024016485700889885, 0.20575437211368605, 0.030482129202027565, 0.762053230050689, 0.18215256739316585, 0.04187415342371629, 0.033499322738973034, 0.004187415342371629, 0.48783388738629474, 0.16959032136605096, 0.08165459917624676, 0.04486491636191069, 0.12023797584992064, 0.08614063941486852, 0.17407587548421347, 0.40019505394824334, 0.10229200930515636, 0.03768652974400498, 0.034097336435052125, 0.9939468671255693, 0.05584701400250197, 0.6786472587645809, 0.20359417762937426, 0.01979387838063361, 0.014845408785475207, 0.014138484557595435, 0.01201771187395612, 0.13306738652666292, 0.3326684663166573, 0.14970080984249579, 0.37425202460623946, 0.089000955993184, 0.8785255656101388, 0.0287099858042529, 0.7309714044429593, 0.04393976197089445, 0.013676614653192311, 0.0011639672045270053, 0.0014549590056587565, 0.04364877016976269, 0.06838307326596156, 0.0011639672045270053, 0.09573630257234618, 0.27118891830774233, 0.1925669209832288, 0.03760182480737604, 0.02620733244150451, 0.21307700724179754, 0.25865497670528365, 0.3940194910326701, 0.599594877658411, 0.28295065702944755, 0.09986493777509914, 0.6158337829464446, 0.2336872917750435, 0.4493986380289298, 0.010785567312694316, 0.010785567312694316, 0.007190378208462877, 0.28401993923428365, 0.9861208988538155, 0.9470511740619434, 0.0442547277599039, 0.9801767905753754, 0.009424776832455533, 0.7844458622478083, 0.20359663600324795, 0.01197627270607341, 0.0028846984737357344, 0.9966633226756962, 0.062234733418926866, 0.02682531612884779, 0.08906004954777466, 0.04506653109646428, 0.31224667973978826, 0.09657113806385204, 0.005365063225769557, 0.33156090735255866, 0.03219037935461735, 0.06769732498226791, 0.017919880142365036, 0.01592878234876892, 0.029866466903941725, 0.08760830291822906, 0.06570622718867179, 0.6152492182211996, 0.00398219558719223, 0.08362610733103683, 0.009955488967980576, 0.02987836603468479, 0.09295491655235268, 0.7104411479358383, 0.02987836603468479, 0.1327927379319324, 0.00331981844829831, 0.011845535443777241, 0.7936508747330752, 0.16583749621288138, 0.023691070887554483, 0.9888708261279426, 0.11428589315599094, 0.008791222550460841, 0.8791222550460842, 0.042611357814307214, 0.8096157984718371, 0.11718123398934484, 0.021305678907153607, 0.0007216466461886725, 0.13783450942203643, 0.013711286277584776, 0.1414427426529798, 0.1472159158224892, 0.17175190179290403, 0.10969029022067821, 0.26267937921267676, 0.015154579569962122, 0.20426110041667056, 0.09404827645084111, 0.08963976349220794, 0.06612769437949767, 0.12490786716127336, 0.4173392267506075, 0.002939008639088785, 0.04342410847572874, 0.054280135594660925, 0.8359140881577782, 0.054280135594660925, 0.02171205423786437, 0.9142190042303094, 0.037315061397155486, 0.012438353799051829, 0.031095884497629574, 0.3499934671480051, 0.12702465603055013, 0.011711493109199656, 0.001801768170646101, 0.05765658146067523, 0.1626095774008106, 0.031080500943645242, 0.258103290445054, 0.9857732912033791, 0.20344034300907568, 0.00884523230474242, 0.1238332522663939, 0.01769046460948484, 0.08845232304742422, 0.10614278765690906, 0.4334163829323786, 0.026535696914227264, 0.026182064228512946, 0.026182064228512946, 0.8705536355980554, 0.05018228977131648, 0.026182064228512946, 0.5805663875800553, 0.021582393590336625, 0.06366806109149305, 0.0550351036553584, 0.06906365948907721, 0.03237359038550494, 0.017265914872269302, 0.13381084026008708, 0.017265914872269302, 0.008632957436134651, 0.6287808333722301, 0.042993561256220864, 0.11285809829757976, 0.08598712251244173, 0.1289806837686626, 0.007411387936321491, 0.9857145955307582, 0.9875684354301321, 0.10958339042505548, 0.8850966149716019, 0.050266150507667334, 0.9086573361001402, 0.015466507848513025, 0.007733253924256512, 0.015466507848513025, 0.0867712535931936, 0.058629225400806485, 0.03048719720841937, 0.44792728206216154, 0.15712632407416138, 0.2181007184910001, 0.216122833402844, 0.7760774472193035, 0.009342471942426827, 0.004671235971213414, 0.01401370791364024, 0.1307946071939756, 0.8314800028759877, 0.24058181709208473, 0.7469226181812398, 0.011189851957771384, 0.05699753176414956, 0.16735445496707743, 0.013339847859694578, 0.027892409161179573, 0.2764986647282149, 0.10065521566860454, 0.3225817755162507, 0.008488994092532913, 0.026679695719389156, 0.055122184357689834, 0.08056319252277745, 0.5724226837144714, 0.025441008165087615, 0.02968117619260222, 0.10600420068786506, 0.08904352857780666, 0.038161512247631424, 0.01737912967798538, 0.778198806692012, 0.07917159075526672, 0.10234376365924724, 0.0038620288173300842, 0.01737912967798538, 0.0019310144086650421, 0.018080620411485976, 0.8769100899570698, 0.0030134367352476626, 0.006026873470495325, 0.0482149877639626, 0.0482149877639626, 0.032725886111919435, 0.9545050115976503, 0.005454314351986573, 0.02399600174247777, 0.6457105923430382, 0.20069383275526861, 0.08943964285832623, 0.017451637630892924, 0.019633092334754538, 0.17575255969445974, 0.07611818606485404, 0.15037983100617508, 0.14419136059439833, 0.12253171415317969, 0.10149091475313873, 0.12253171415317969, 0.042081598800081915, 0.026610422770640033, 0.03836851655301586, 0.029593196376690376, 0.7595587070017197, 0.13810158309122175, 0.06905079154561088, 0.9573744210389236, 0.0027353554886826387, 0.038294976841556944, 0.016968298430543975, 0.14705858639804778, 0.29411717279609556, 0.03393659686108795, 0.028280497384239957, 0.11877808901380782, 0.03959269633793594, 0.3167415707034875, 0.008669984010747565, 0.9450282571714844, 0.04334992005373782, 0.9984212478367102, 0.16023684551126063, 0.1840274688622775, 0.22671064605086658, 0.06647380053960594, 0.08606607859338454, 0.06297517945857405, 0.1070578050795759, 0.048980695134446486, 0.036385659242731676, 0.021691450702397728, 0.032643004262085114, 0.962968625731511, 0.0032643004262085116, 0.999281467703697, 0.996815670734574, 0.4150905983757284, 0.016758316083492143, 0.10828450392410308, 0.0012891012371917033, 0.04382944206451791, 0.08379158041746071, 0.0025782024743834066, 0.06703326433396857, 0.26168755114991576, 0.07450989155151727, 0.052067153132385566, 0.04488547683826342, 0.0017954190735305366, 0.1364518495883208, 0.017954190735305367, 0.2872670517648859, 0.33664107628697565, 0.012567933514713758, 0.035908381470610734, 0.005612332763760363, 0.0710895483409646, 0.37602629517194436, 0.5294300573813943, 0.016836998291281092, 0.24407352528450982, 0.056123858755261616, 0.011746854158078014, 0.011746854158078014, 0.009136442122949565, 0.13443621980911505, 0.02088329628102758, 0.3432691826193908, 0.040461386544490933, 0.12791018972129392, 0.00729461436185169, 0.1385976728751821, 0.7659345079944274, 0.051062300532961825, 0.01458922872370338, 0.01458922872370338, 0.9416729904425014, 0.05371960012591451, 0.003159976477994971, 0.9986925695722882, 0.8775928742686682, 0.10398019837306495, 0.012477623804767794, 0.9919362063527357, 0.2659785772177432, 0.7299412081539569, 0.003999677852898394, 0.22214198366244897, 0.05362047881507389, 0.1630500274172655, 0.062374842703249216, 0.16523861838930934, 0.3337601232366844, 0.8862614979677993, 0.004160852103135208, 0.1081821546815154, 0.009230641724957259, 0.009230641724957259, 0.0584607309247293, 0.8984491278958397, 0.021538164024900268, 0.030523767954607116, 0.07461365500015073, 0.1695764886367062, 0.1254866015911626, 0.5969092400012058, 0.048078298739073995, 0.042736265545843546, 0.470098921004279, 0.4433887550381268, 0.990142523340509, 0.009903351467931299, 0.9804317953251985, 0.11081945911177576, 0.026385585502803753, 0.04485549535476638, 0.08179531505869164, 0.19789189127102816, 0.4723019805001872, 0.005277117100560751, 0.007915675650841126, 0.05540972955588788, 0.2201955836446735, 0.09234008346389534, 0.02841233337350626, 0.6570352092623323, 0.8024712717788584, 0.1565797603470943, 0.03914494008677358, 0.27150908344557423, 0.015971122555622014, 0.6867582698917466, 0.015971122555622014, 0.9924699618663049, 0.10849723397861784, 0.12371330947561912, 0.04564822649100385, 0.06417214448735324, 0.10518939147926974, 0.08931174748239884, 0.1164360559770533, 0.17068467296636222, 0.08335763098357224, 0.09261958998174694, 0.9961962238721886, 0.9923141594618299, 0.9948925093365361, 0.029587560650743187, 0.17183544839470083, 0.2935996403035286, 0.1081083946854078, 0.19800905973958902, 0.059175121301486375, 0.07510688472880964, 0.005689915509758305, 0.004551932407806644, 0.05462318889367973, 0.11071308311300704, 0.05535654155650352, 0.782212000254941, 0.04332251078335058, 0.009627224618522352, 0.09003955278850641, 0.162595697268565, 0.17570825349990085, 0.08479453029597206, 0.15210565228349626, 0.02972179412436134, 0.09703291611188555, 0.006119192907956746, 0.14161560729842756, 0.05944358824872268, 0.970044640248902, 0.02536064418951378, 0.7986050520644128, 0.19206956948384613, 0.9896662878030562, 0.004937195696349238, 0.9430043780027044, 0.04937195696349238, 0.0047878564522435725, 0.7229663242887795, 0.10054498549711502, 0.009575712904487145, 0.047878564522435725, 0.09575712904487145, 0.014363569356730718, 0.012464279214450936, 0.009348209410838203, 0.937937010887433, 0.040508907446965545, 0.09563718710801017, 0.009563718710801016, 0.8511709652612904, 0.02869115613240305, 0.9852272224490645, 0.979145379338873, 0.01472399066674997]}, \"R\": 30};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el45995470601851163684455707973\", ldavis_el45995470601851163684455707973_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el45995470601851163684455707973\", ldavis_el45995470601851163684455707973_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el45995470601851163684455707973\", ldavis_el45995470601851163684455707973_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics         x         y\n",
       "topic                                                \n",
       "1      21.924676        1       1 -0.160148 -0.007949\n",
       "5      14.437502        1       2 -0.015733 -0.123081\n",
       "3      12.920510        1       3  0.012238 -0.053868\n",
       "0       9.698103        1       4  0.226205  0.033388\n",
       "9       8.993744        1       5  0.085544 -0.023278\n",
       "4       8.146445        1       6 -0.034196 -0.122927\n",
       "8       7.756998        1       7  0.198601  0.063036\n",
       "6       7.555307        1       8 -0.078050  0.105785\n",
       "7       4.549334        1       9 -0.121892  0.221830\n",
       "2       4.017381        1      10 -0.112568 -0.092937, topic_info=     Category         Freq         Term        Total  loglift  logprob\n",
       "term                                                                  \n",
       "775   Default  2490.000000         kong  2490.000000  30.0000  30.0000\n",
       "683   Default  2479.000000         hong  2479.000000  29.0000  29.0000\n",
       "195   Default  2676.000000         bank  2676.000000  28.0000  28.0000\n",
       "298   Default  3429.000000        china  3429.000000  27.0000  27.0000\n",
       "997   Default  5566.000000      percent  5566.000000  26.0000  26.0000\n",
       "1241  Default  3436.000000        share  3436.000000  25.0000  25.0000\n",
       "1387  Default   975.000000        tonne   975.000000  24.0000  24.0000\n",
       "1315  Default  2220.000000        stock  2220.000000  23.0000  23.0000\n",
       "852   Default  4203.000000       market  4203.000000  22.0000  22.0000\n",
       "1055  Default  2038.000000        price  2038.000000  21.0000  21.0000\n",
       "882   Default  4990.000000      million  4990.000000  20.0000  20.0000\n",
       "0     Default  1698.000000          000  1698.000000  19.0000  19.0000\n",
       "1103  Default  1700.000000      quarter  1700.000000  18.0000  18.0000\n",
       "342   Default  5385.000000      company  5385.000000  17.0000  17.0000\n",
       "299   Default  1545.000000      chinese  1545.000000  16.0000  16.0000\n",
       "207   Default  1255.000000      beijing  1255.000000  15.0000  15.0000\n",
       "1071  Default  1772.000000       profit  1772.000000  14.0000  14.0000\n",
       "639   Default   566.000000         gold   566.000000  13.0000  13.0000\n",
       "636   Default   564.000000           gm   564.000000  12.0000  12.0000\n",
       "230   Default   552.000000          bre   552.000000  11.0000  11.0000\n",
       "351   Default   971.000000     computer   971.000000  10.0000  10.0000\n",
       "437   Default   686.000000         deng   686.000000   9.0000   9.0000\n",
       "215   Default  3218.000000      billion  3218.000000   8.0000   8.0000\n",
       "22    Default  1852.000000         1996  1852.000000   7.0000   7.0000\n",
       "1113  Default  1059.000000         rate  1059.000000   6.0000   6.0000\n",
       "151   Default   825.000000        apple   825.000000   5.0000   5.0000\n",
       "1204  Default  2187.000000         sale  2187.000000   4.0000   4.0000\n",
       "550   Default   770.000000       export   770.000000   3.0000   3.0000\n",
       "141   Default  3773.000000      analyst  3773.000000   2.0000   2.0000\n",
       "259   Default   494.000000       canada   494.000000   1.0000   1.0000\n",
       "...       ...          ...          ...          ...      ...      ...\n",
       "717   Topic10   124.910553    indonesia   160.094879   2.9664  -5.0811\n",
       "1347  Topic10    78.522644    surcharge   101.793964   2.9550  -5.5453\n",
       "198   Topic10    85.783665   bankruptcy   121.326701   2.8679  -5.4569\n",
       "930   Topic10   110.906275      norfolk   184.596623   2.7051  -5.2000\n",
       "441   Topic10   109.773620      deposit   262.652988   2.3421  -5.2103\n",
       "1234  Topic10    45.285984     services   120.239830   2.2380  -6.0957\n",
       "377   Topic10    88.764629       copper   245.071807   2.1990  -5.4227\n",
       "675   Topic10    34.991859         hire    97.318556   2.1917  -6.3536\n",
       "213   Topic10    35.683009      bidding   102.930756   2.1552  -6.3340\n",
       "1180  Topic10    50.230209         rich   145.301418   2.1523  -5.9921\n",
       "1162  Topic10    40.473791     resource   119.534584   2.1316  -6.2080\n",
       "368   Topic10    31.723026   consultant    94.240875   2.1257  -6.4516\n",
       "1076  Topic10   142.467909      project   449.634017   2.0652  -4.9496\n",
       "1315  Topic10   572.894232        stock  2220.041438   1.8600  -3.5580\n",
       "1242  Topic10   226.655483  shareholder   877.616982   1.8608  -4.4852\n",
       "1372  Topic10    56.006605         test   176.800285   2.0650  -5.8832\n",
       "221   Topic10   117.397910        board   464.458433   1.8392  -5.1431\n",
       "1259  Topic10    79.050322         site   278.149486   1.9565  -5.5386\n",
       "539   Topic10   160.421161     exchange   784.588399   1.6272  -4.8309\n",
       "1241  Topic10   329.091893        share  3436.522940   0.8687  -4.1123\n",
       "200   Topic10   217.504176         base  1749.270977   1.1298  -4.5265\n",
       "342   Topic10   280.649748      company  5385.376906   0.2602  -4.2716\n",
       "1149  Topic10   182.716833       report  1616.521944   1.0344  -4.7007\n",
       "215   Topic10   212.595705      billion  3218.685979   0.4972  -4.5493\n",
       "141   Topic10   190.591353      analyst  3773.302675   0.2290  -4.6585\n",
       "882   Topic10   195.489047      million  4990.307922  -0.0252  -4.6332\n",
       "277   Topic10   130.529886         cent   736.664950   1.4840  -5.0371\n",
       "1467  Topic10   140.252521         week  1511.559272   0.8371  -4.9652\n",
       "922   Topic10   137.516684          new  3488.862490  -0.0190  -4.9849\n",
       "997   Topic10   132.752353      percent  5566.776018  -0.5215  -5.0202\n",
       "\n",
       "[577 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "0         1  0.058889       000\n",
       "0         2  0.097167       000\n",
       "0         3  0.242034       000\n",
       "0         4  0.035922       000\n",
       "0         5  0.085978       000\n",
       "0         6  0.015311       000\n",
       "0         7  0.052411       000\n",
       "0         8  0.001178       000\n",
       "0         9  0.393968       000\n",
       "0        10  0.017078       000\n",
       "1         1  0.329414        10\n",
       "1         2  0.089759        10\n",
       "1         3  0.117584        10\n",
       "1         4  0.035006        10\n",
       "1         5  0.036801        10\n",
       "1         6  0.021542        10\n",
       "1         7  0.088861        10\n",
       "1         8  0.102325        10\n",
       "1         9  0.119379        10\n",
       "1        10  0.059241        10\n",
       "21        1  0.576598      1995\n",
       "21        2  0.019746      1995\n",
       "21        3  0.045417      1995\n",
       "21        4  0.023696      1995\n",
       "21        5  0.009873      1995\n",
       "21        6  0.061214      1995\n",
       "21        7  0.042455      1995\n",
       "21        8  0.087872      1995\n",
       "21        9  0.131314      1995\n",
       "21       10  0.000987      1995\n",
       "...     ...       ...       ...\n",
       "1485      7  0.097033     world\n",
       "1485      8  0.006119     world\n",
       "1485      9  0.141616     world\n",
       "1485     10  0.059444     world\n",
       "1490      4  0.970045  xiaoping\n",
       "1490      7  0.025361  xiaoping\n",
       "1491      4  0.798605    xinhua\n",
       "1491      7  0.192070    xinhua\n",
       "1492      7  0.989666  xinjiang\n",
       "1493      1  0.004937       yen\n",
       "1493      6  0.943004       yen\n",
       "1493      8  0.049372       yen\n",
       "1495      2  0.004788     young\n",
       "1495      3  0.722966     young\n",
       "1495      4  0.100545     young\n",
       "1495      5  0.009576     young\n",
       "1495      6  0.047879     young\n",
       "1495      7  0.095757     young\n",
       "1495      9  0.014364     young\n",
       "1496      4  0.012464      yuan\n",
       "1496      7  0.009348      yuan\n",
       "1496      8  0.937937      yuan\n",
       "1496      9  0.040509      yuan\n",
       "1497      1  0.095637   zealand\n",
       "1497      2  0.009564   zealand\n",
       "1497      3  0.851171   zealand\n",
       "1497      7  0.028691   zealand\n",
       "1498      4  0.985227     zemin\n",
       "1499      9  0.979145      zinc\n",
       "1499     10  0.014724      zinc\n",
       "\n",
       "[2016 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'ylab': 'PC2', 'xlab': 'PC1'}, topic_order=[2, 6, 4, 1, 10, 5, 9, 7, 8, 3])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(lda, tf_large, vectorizer, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Neural Networks</span><a id='nn_ml'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Interested? Check out the Stanford course CS224n ([Syllabus](http://web.stanford.edu/class/cs224n/syllabus.html))!   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
