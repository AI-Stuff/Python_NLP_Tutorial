{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text mining (nlp) with python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Ties de Kok ([Personal Website](http://www.tiesdekok.com))  \n",
    "**Last updated:** 24 Oct 2017  \n",
    "**Python version:** Python 3.5  \n",
    "**License:** MIT License  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Some features (like the ToC) will only work if you run it locally, use Binder, or use nbviewer by clicking this link: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Introduction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code examples to get you started with Natural Language Processing (NLP) / Text Mining for Research and Data Science purposes.  \n",
    "\n",
    "In the large scheme of things there are roughly 4 steps:  \n",
    "\n",
    "1. Identify a data source  \n",
    "2. Gather the data  \n",
    "3. Process the data  \n",
    "4. Analyze the data  \n",
    "\n",
    "This notebook only discusses step 3 and 4. If you want to learn more about step 2 see my [Python tutorial](https://github.com/TiesdeKok/LearnPythonforResearch). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Elements / topics that are discussed in this notebook: *\n",
    "\n",
    "\n",
    "<img style=\"float: left\" src=\"https://i.imgur.com/c3aCZLA.png\" width=\"40%\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Table of Contents*  <a id='toc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Primer on NLP tools](#tool_primer)     \n",
    "* [Process + Clean text](#proc_clean)   \n",
    "    * [Normalization](#normalization)\n",
    "        * [Deal with unwanted characters](#unwanted_char)\n",
    "        * [Sentence segmentation](#sentence_seg)   \n",
    "        * [Word tokenization](#word_token)\n",
    "        * [Lemmatization & Stemming](#lem_and_stem) \n",
    "    * [Language modeling](#lang_model) \n",
    "        * [Part-of-Speech tagging](#pos_tagging) \n",
    "        * [Uni-Gram & N-Grams](#n_grams) \n",
    "        * [Stop words](#stop_words) \n",
    "* [Direct feature extraction](#feature_extract) \n",
    "    * [Feature search](#feature_search) \n",
    "        * [Entity recognition](#entity_recognition) \n",
    "        * [Pattern search](#pattern_search) \n",
    "    * [Text evaluation](#text_eval) \n",
    "        * [Language](#language) \n",
    "        * [Dictionary counting](#dict_counting) \n",
    "        * [Readability](#readability) \n",
    "* [Represent text numerically](#text_numerical) \n",
    "    * [Bag of Words](#bows) \n",
    "        * [TF-IDF](#tfidf) \n",
    "    * [Word Embeddings](#word_embed) \n",
    "        * [Word2Vec](#Word2Vec) \n",
    "* [Statistical models](#stat_models) \n",
    "    * [\"Traditional\" machine learning](#trad_ml) \n",
    "        * [Supervised](#trad_ml_supervised) \n",
    "            * [Na√Øve Bayes](#trad_ml_supervised_nb) \n",
    "            * [Support Vector Machines (SVM)](#trad_ml_supervised_svm) \n",
    "        * [Unsupervised](#trad_ml_unsupervised) \n",
    "            * [Latent Dirichilet Allocation (LDA)](#trad_ml_unsupervised_lda) \n",
    "            * [pyLDAvis](#trad_ml_unsupervised_pyLDAvis) \n",
    "* [Model Selection and Evaluation](#trad_ml_eval) \n",
    "* [Neural Networks](#nn_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Primer on NLP tools</span><a id='tool_primer'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many tools available for NLP purposes.  \n",
    "The code examples below are based on what I personally like to use, it is not intended to be a comprehsnive overview.  \n",
    "\n",
    "Besides build-in Python functionality I will use / demonstrate the following packages:\n",
    "\n",
    "**Standard NLP libraries**:\n",
    "1. `Spacy` and the higher-level wrapper `Textacy` \n",
    "2. `NLTK` and the higher-level wrapper `TextBlob`\n",
    "\n",
    "*Note: besides installing the above packages you also often have to download (model) data . Make sure to check the documentation!*\n",
    "\n",
    "**Standard machine learning library**:\n",
    "\n",
    "1. `scikit learn`\n",
    "\n",
    "**Topic modelling libraries**:\n",
    "\n",
    "1. `Gensim` \n",
    "2. `FastText`\n",
    "\n",
    "**Specific task libraries**:\n",
    "\n",
    "There are many, just a couple of examples:\n",
    "\n",
    "1. `pyLDAvis` for visualizing LDA)\n",
    "2. `langdetect` for detecting languages\n",
    "3. `fuzzywuzzy` for fuzzy text matching\n",
    "4. `textstat` to calculate readability statistics\n",
    "\n",
    "** Neural Network (+ Deep Learning) libraries**:\n",
    "\n",
    "1. `Tensorflow`  \n",
    "2. `PyTorch`  \n",
    "3. `Keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Get some example data</span><a id='example_data'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many example datasets available to play around with, see for example this great repository:  \n",
    "https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that I will use for most of the examples is the \"Reuter_50_50 Data Set\" that is used for author identification experiments. \n",
    "\n",
    "See the details here: https://archive.ics.uci.edu/ml/datasets/Reuter_50_50  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't follow what I am doing here? Please see my [Python tutorial](https://github.com/TiesdeKok/LearnPythonforResearch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, zipfile, io, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Download and extract the zip file with the data *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('C50test'):\n",
    "    r = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\")\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load the data into memory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_dict = {'test' : 'C50test', 'train' : 'C50train'}\n",
    "text_dict = {'test' : {}, 'train' : {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for label, folder in folder_dict.items():\n",
    "    authors = os.listdir(folder)\n",
    "    for author in authors:\n",
    "        text_files = os.listdir(os.path.join(folder, author))\n",
    "        for file in text_files:\n",
    "            with open(os.path.join(folder, author, file), 'r') as text_file:\n",
    "                text_dict[label].setdefault(author, []).append(' '.join(text_file.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the text comes pre-split per sentence, for the sake of example I undo this doing `' '.join(text_file.readlines()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain\\'s Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997. The shares fell 6p to 781p on the news.\\n \"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers.  \\n Dermott Carr, an analyst at Nikko said, \"the market is going to hang onto them for the moment but until we get a decision they will be held back.\"\\n Whatever the MMC decides many analysts expect Lang to defer a decision until after the next general election which will be called by May 22.\\n \"They will probably try to defer the decision until after the election. I don\\'t think they want the negative PR of having a large number of people fired,\" said Wakley.  \\n If the deal does not go through, analysts calculate the maximum loss to Bass of 60 million, with most sums centred on the 30-40 million range.\\n \"It\\'s a maxiumum loss of 60 million for Bass if they fail and, unlike Allied, you would have to compare it to the perceived upside of doing the deal,\" said Wakley.\\n Bass said at the time of the deal it would take a one-off charge of 75 million stg for restructuring the combined business, resulting in expected annual cost savings of 90 million stg within three years.  \\n Under the terms of the complex deal, if Bass cannot combine C-T with its own brewing business within 16 months, it has the option to put its whole shareholding to Carlsberg for 110 million stg and Carlsberg has an option to put 15 percent of C-T to Allied Domecq, which would reimburse Bass 30 million stg.\\n Bass is also entitled to receive 50 percent of all profits earnied by C-T until the merger is complete, which should give it some 30-35 million stg in a full year. Carlsberg has agreed to contribute its interests and 20 million stg in exchange for a 20 percent share in the combined Bass Breweries and Carlsberg-Tetley business.\\n C-T was a joint venture between Allied Domecq and Carlsberg formed in 1992 by the merger of their UK brewing and wholesaleing businesses.\\n -- London Newsroom +44 171 542 6437\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Process + Clean text</span><a id='proc_clean'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the text into a NLP representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the text directly, but if want to use packages like `spacy` and `nltk` / `textblob` we first have to convert the text into a corresponding object.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all text in the \"test\" sample to a `spacy` `doc` object using `parser()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_text = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    spacy_text[author] = [parser(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spacy_text['TimFarrand'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply basic `nltk` operations directly to the text so we don't need to convert first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all text in the \"test\" sample to a `TextBlob` object using `TextBlob()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textblob_text = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    textblob_text[author] = [TextBlob(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(textblob_text['TimFarrand'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Normalization</span><a id='normalization'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of **text normalization** describes the task of transforming the text into a different form.  \n",
    "\n",
    "This can imply many things, I will show a couple of things below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Deal with unwanted characters</span><a id='unwanted_char'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will often notice that there are characters that you don't want in your text.  \n",
    "\n",
    "Let's look at this sentence for example:\n",
    "\n",
    "> \"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain\\'s Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\"\n",
    "\n",
    "You notice that there are some `\\` and `\\n` in there. These are used to define how a string should be displayed, if we print this text we get:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
      " Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\n"
     ]
    }
   ],
   "source": [
    "print(text_dict['test']['TimFarrand'][0][:298])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to analyze text we often don't care about the visual representation. They might actually cause problems!  \n",
    "\n",
    "** So how do we remove them? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases it is sufficient to simply use the `.replace()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts. Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:298].replace('\\n', '').replace('\\\\', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, however, the problem arrises because of encoding / decoding problems.  \n",
    "\n",
    "In those cases you can usually do something like:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"This is some  text that has to be cleaned! it's annoying!\"\n"
     ]
    }
   ],
   "source": [
    "problem_sentence = 'This is some \\\\u03c0 text that has to be cleaned\\\\u2026! it\\\\u0027s annoying!'\n",
    "print(problem_sentence.encode().decode('unicode_escape').encode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Sentence segmentation</span><a id='sentence_seg'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation means the task of splitting up the piece of text by sentence.  \n",
    "\n",
    "You could do this by splitting on the `.` symbol, but dots are used in many other cases as well so it is not very robust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts\",\n",
       " '\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997',\n",
       " ' The shares fell 6p to 781p on the news',\n",
       " '\\n \"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers',\n",
       " '  \\n Dermott Carr, an analyst at Nikko said, \"the mark']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:550].split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to use a more sophisticated implementation such as the one by `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_paragraph = spacy_text['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
       "  ,\n",
       " Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997.,\n",
       " The shares fell 6p to 781p on the news.\n",
       "  ,\n",
       " \"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers.  \n",
       "  ,\n",
       " Dermott Carr, an analyst at Nikko said, \"the market is going to hang onto them for the moment but until we get a decision they will be held back.\"\n",
       "  ]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = [s for s in example_paragraph.sents]\n",
    "sentence_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the returned object is still a `spacy` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to all texts (for use later on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_sentences = {}\n",
    "for author, text_list in spacy_text.items():\n",
    "    spacy_sentences[author] = [[s for s in text.sents] for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
       "  ,\n",
       " Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997.,\n",
       " The shares fell 6p to 781p on the news.\n",
       "  ]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentences['TimFarrand'][0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Word tokenization</span><a id='word_token'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenization means to split the sentence (or text) up into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
       " "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][0][0]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word is called a `token` in this context (hence `tokenization`), using `spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares,\n",
       " in,\n",
       " brewing,\n",
       " -,\n",
       " to,\n",
       " -,\n",
       " leisure,\n",
       " group,\n",
       " Bass,\n",
       " Plc,\n",
       " are,\n",
       " likely,\n",
       " to,\n",
       " be,\n",
       " held]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [token for token in example_sentence]\n",
    "token_list[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Lemmatization & Stemming</span><a id='lem_and_stem'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you want to convert a word (i.e. token) into a more general representation.  \n",
    "\n",
    "For example: convert \"car\", \"cars\", \"car's\", \"cars'\" all into the word `car`.\n",
    "\n",
    "This is generally done through lemmatization / stemming (different approaches trying to achieve a similar goal).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spacy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Space offers build-in functionality for lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['share',\n",
       " 'in',\n",
       " 'brewing',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'leisure',\n",
       " 'group',\n",
       " 'bass',\n",
       " 'plc',\n",
       " 'be',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'be',\n",
       " 'hold']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = [token.lemma_ for token in example_sentence]\n",
    "lemmatized[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the NLTK libary we can also use the more aggressive Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['share',\n",
       " 'in',\n",
       " 'brew',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'leisur',\n",
       " 'group',\n",
       " 'bass',\n",
       " 'plc',\n",
       " 'are',\n",
       " 'like',\n",
       " 'to',\n",
       " 'be',\n",
       " 'held']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed = [stemmer.stem(token.text) for token in example_sentence]\n",
    "stemmed[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares  |  share  |  share\n",
      "in  |  in  |  in\n",
      "brewing  |  brewing  |  brew\n",
      "-  |  -  |  -\n",
      "to  |  to  |  to\n",
      "-  |  -  |  -\n",
      "leisure  |  leisure  |  leisur\n",
      "group  |  group  |  group\n",
      "Bass  |  bass  |  bass\n",
      "Plc  |  plc  |  plc\n",
      "are  |  be  |  are\n",
      "likely  |  likely  |  like\n",
      "to  |  to  |  to\n",
      "be  |  be  |  be\n",
      "held  |  hold  |  held\n"
     ]
    }
   ],
   "source": [
    "for original, lemma, stem in zip(token_list[:15], lemmatized[:15], stemmed[:15]):\n",
    "    print(original, ' | ', lemma, ' | ', stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Porter Stemmer is the most aggressive. In my experience it is usually best to use lemmatization instead of a stemmer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Language modeling</span><a id='lang_model'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is inherently structured in complex ways, we can often use some of this underlying structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Part-of-Speech tagging</span><a id='pos_tagging'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech tagging refers to the identification of words as nouns, verbs, adjectives, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Shares, 'NOUN'),\n",
       " (in, 'ADP'),\n",
       " (brewing, 'NOUN'),\n",
       " (-, 'PUNCT'),\n",
       " (to, 'ADP'),\n",
       " (-, 'PUNCT'),\n",
       " (leisure, 'NOUN'),\n",
       " (group, 'NOUN'),\n",
       " (Bass, 'PROPN'),\n",
       " (Plc, 'PROPN')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = [(token, token.pos_) for token in example_sentence]\n",
    "pos_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Uni-Gram & N-Grams</span><a id='n_grams'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously a sentence is not a random collection of words, the sequence of words has information value.  \n",
    "\n",
    "A simple way to incorporate some of this sequence is by using what is called `n-grams`.  \n",
    "An `n-gram` is nothing more than a a combination of `N` words into one \"word\" (a uni-gram is just one word).  \n",
    "\n",
    "So we can convert `\"Sentence about flying cars\"` into a list of bigrams:\n",
    "\n",
    "> Sentence-about, about-flying, flying-cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `NLTK`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are-likely', 'likely-to', 'to-be', 'be-held', 'held-back']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_list = ['-'.join(x) for x in nltk.bigrams([token.text for token in example_sentence])]\n",
    "bigram_list[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Stop words</span><a id='stop_words'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what you are trying to do it is possible that there are many words that don't add any information value to the sentence.  \n",
    "\n",
    "The primary example are stop words.  \n",
    "\n",
    "Sometimes you can improve the accuracy of your model by removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_stop_words = [token for token in example_sentence if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares, brewing, -, -, leisure, group, Bass, Plc, likely, held]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares, in, brewing, -, to, -, leisure, group, Bass, Plc]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the awesome `textacy` package to save time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Textacy` is a high-level wrapper built on `spaCy`, it has many cool features!\n",
    "\n",
    "See their GitHub page for details: https://github.com/chartbeat-labs/textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can almost completely wrap all of our pre-processing into one `textacy` command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://textacy.readthedocs.io/en/latest/api_reference.html#module-textacy.preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_text = text_dict['test']['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_text = textacy.preprocess_text(example_text, lowercase=True, fix_unicode=True, no_punct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's use the above + textacy to do some basic preprocessing **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split into sentences\n",
    "2. Apply lemmatizer\n",
    "3. Clean up the sentence using `textacy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: using textacy can be quite slow so use with caution! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_text_clean = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    lst = []\n",
    "    for text in text_list:\n",
    "        sentences = [' '.join([token.lemma_ for token in s]) for s in parser(text).sents]\n",
    "        sentences_cleaned = [textacy.preprocess_text(sen, lowercase=True, fix_unicode=True, no_punct=True) for sen in sentences]\n",
    "        lst.append([parser(sen) for sen in sentences_cleaned])\n",
    "    spacy_text_clean[author] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[share in brewing to leisure group bass plc be likely to be hold back until britain s trade and industry secretary ian lang decide whether to allow pron propose merge with brewer carlsberg tetley say analyst,\n",
       " earlier lang announce the bass deal would be refer to the monoplies and mergers commission which be due to report before march 24 1997,\n",
       " the share fall 6p to 781p on the news]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_text_clean['TimFarrand'][0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Direct feature extraction</span><a id='feature_extract'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have pre-processed our text into something that we can use.  \n",
    "\n",
    "The easiest thing we can do now is what I label \"direct feature extraction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Feature search</span><a id='feature_search'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many features that you can extract from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Entity recognition</span><a id='entity_recognition'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful / relevant to extract entities that are mentioned in a piece of text. \n",
    "\n",
    "Example using `textacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "if the deal do not go through analyst calculate the maximum loss to bass of 60 million with most sum centre on the 30 40 million range"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_text_clean['TimFarrand'][0][8]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract numeric entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'PERCENT'), ('48', 'PERCENT'), ('percent', 'PERCENT'), ('1487', 'CARDINAL'), ('million', 'CARDINAL'), ('2464', 'CARDINAL'), ('million', 'CARDINAL'), ('november', 'DATE'), ('1995', 'DATE')]\n",
      "[('the', 'DATE'), ('year', 'DATE')]\n",
      "[('the', 'DATE'), ('current', 'DATE'), ('year', 'DATE')]\n",
      "[('the', 'DATE'), ('year', 'DATE'), ('end', 'DATE'), ('27', 'CARDINAL'), ('1996', 'DATE'), ('312', 'CARDINAL'), ('million', 'CARDINAL'), ('239', 'CARDINAL'), ('million', 'CARDINAL')]\n",
      "[('107', 'PERCENT'), ('percent', 'PERCENT'), ('1113', 'CARDINAL'), ('million', 'CARDINAL')]\n",
      "[('471', 'MONEY'), ('million', 'MONEY'), ('the', 'DATE'), ('11', 'DATE'), ('month', 'DATE'), ('18', 'MONEY'), ('million', 'MONEY')]\n",
      "[('145', 'CARDINAL'), ('150', 'CARDINAL'), ('million', 'CARDINAL')]\n",
      "[('597p', 'CARDINAL'), ('pron', 'ORG')]\n",
      "[('one', 'CARDINAL')]\n",
      "[('1997', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('britain', 'GPE'), ('annual', 'DATE'), ('december', 'DATE')]\n",
      "[('170', 'CARDINAL'), ('million', 'CARDINAL'), ('199798', 'DATE'), ('about', 'CARDINAL'), ('40', 'CARDINAL'), ('million', 'CARDINAL')]\n",
      "[('502', 'PERCENT'), ('percent', 'PERCENT'), ('789', 'CARDINAL'), ('million', 'CARDINAL')]\n",
      "[('920', 'CARDINAL'), ('514', 'CARDINAL')]\n",
      "[('88', 'CARDINAL'), ('41', 'CARDINAL'), ('418', 'PERCENT'), ('percent', 'PERCENT'), ('248', 'CARDINAL'), ('million', 'CARDINAL')]\n",
      "[('145', 'PERCENT'), ('percent', 'PERCENT'), ('252', 'CARDINAL'), ('million', 'CARDINAL')]\n",
      "[('16034', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "for sen in spacy_text_clean['TimFarrand'][4]:\n",
    "    print([(token.text, token.ent_type_) for token in sen if token.ent_type_ != ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Pattern search</span><a id='pattern_search'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the build-in `re` (regular expression) library you can pattern match anything you want.  \n",
    "\n",
    "I will not go into details about regular expressions but see here for a tutorial:  \n",
    "https://regexone.com/references/python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: Use [Pythex.org](https://pythex.org/) to try out your regular expression\n",
    "\n",
    "Example on Pythex: <a href=\"https://pythex.org/?regex=IDNUMBER: (\\d\\d\\d-\\w\\w)&test_string=Ties de Kok (IDNUMBER: 123-AZ). Rest of Text.\" target='_blank'>click here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string_1 = 'Ties de Kok (#IDNUMBER: 123-AZ). Rest of text...'\n",
    "string_2 = 'Philip Joos (#IDNUMBER: 663-BY). Rest of text...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = r'#IDNUMBER: (\\d\\d\\d-\\w\\w)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123-AZ\n",
      "663-BY\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(pattern, string_1)[0])\n",
    "print(re.findall(pattern, string_2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a sentence contains the word 'million' return True, otherwise return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyst forecast for pretax profit range from 218 to 232 million stg after restructure cost up from 206 million last time\n",
      "a restructure cost of some 35 million be anticipate with the bulk of pron or about 25 million stem from the closure of pron small production plant in france\n",
      "cadbury s us drink business should turn in about 112 million stg in trading profit against 59 million in the first half of 1995 due entirely to the contribution of dr pepper\n",
      "campbell estimate uk beverage will contribute 47 million stg in operating profit down from 50 million last time\n",
      "broadly analyst expect a pretty flat performance from the group s confectionery business with a consensus forecast of 110 million stg for operate profit\n",
      "on average analyst calculate beverage will chip in trading profit of 150 million\n",
      "after the sale of pron 51 percent stake in the coca cola amp schweppes beverages ccsb operation to coca cola enterprises in june for 620 million stg many analyst want to see a clear statement of strategy from the company\n",
      "but so far say analyst the company have not say when shareholder can expect a return on investment in emerge market one of the large so far a 75 million russian plant\n",
      "cadbury announce an investment of 20 million stg in build a new plant at wrocoaw in poland in 1993 and a joint venture in china at a cost of 20 million\n",
      "net debt at 134 billion at the end of 1995 should fall to about 510 million by the end of 1996 as a result of the ccsb sale and provide there be no further acquisition\n"
     ]
    }
   ],
   "source": [
    "for sen in spacy_text_clean['TimFarrand'][2]:\n",
    "    TERM = 'million'\n",
    "    contains = True if re.search('million', sen.text) else False\n",
    "    if contains:\n",
    "        print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Text evaluation</span><a id='text_eval'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides search for features there are also many ways to analyze the text as a whole.  \n",
    "\n",
    "Let's, for example, evaluate the following paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soft drink and confectionery group cadbury schweppes plc expect to report a solid eight percent rise in first half profit on wednesday face question about the performance of pron 7up soft drink in the us one of the main question will be the success or otherwise of the relaunch of the 7up brand say mark duffy food manufacturing analyst at sbc warburg competitor sprite own by coca cola have see an agressive marketing push and be rank the fast grow brand in the us after cadbury s dr pepper analyst '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_paragraph = ' '.join([x.text for x in spacy_text_clean['TimFarrand'][2]])\n",
    "example_paragraph[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Language</span><a id='language'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `langdetect` package it is easy to detect the language of a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(example_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Readability</span><a id='readability'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `textstat` package we can compute various readability metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/shivam5992/textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-610.94\n",
      "0\n",
      "269.6\n",
      "10.42\n",
      "344.4\n",
      "40.86\n",
      "133\n",
      "54.0\n",
      "285.3213352685051\n",
      "269th and 270th grade\n"
     ]
    }
   ],
   "source": [
    "print(textstat.flesch_reading_ease(example_paragraph))\n",
    "print(textstat.smog_index(example_paragraph))\n",
    "print(textstat.flesch_kincaid_grade(example_paragraph))\n",
    "print(textstat.coleman_liau_index(example_paragraph))\n",
    "print(textstat.automated_readability_index(example_paragraph))\n",
    "print(textstat.dale_chall_readability_score(example_paragraph))\n",
    "print(textstat.difficult_words(example_paragraph))\n",
    "print(textstat.linsear_write_formula(example_paragraph))\n",
    "print(textstat.gunning_fog(example_paragraph))\n",
    "print(textstat.text_standard(example_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Term (dictionary) counting</span><a id='dict_counting'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common techniques that researchers currently use (at least in Accounting research) are simple metrics based on counting words in a dictionary.  \n",
    "This technique is, for example, very prevalent in sentiment analysis (counting positive and negative words).  \n",
    "\n",
    "In essence this technique is very simple to program:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dictionary = ['soft', 'first', 'most', 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft 3\n",
      "first 3\n",
      "most 1\n",
      "be 25\n"
     ]
    }
   ],
   "source": [
    "for word in word_dictionary:\n",
    "    print(word, example_paragraph.count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = ['great', 'increase']\n",
    "neg = ['bad', 'decrease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = '''According to Trump everything is great, great, \n",
    "and great even though his popularity is seeing a decrease.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "pos_count = 0\n",
    "for word in pos:\n",
    "    pos_count += sentence.lower().count(word)\n",
    "print(pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "neg_count = 0\n",
    "for word in neg:\n",
    "    neg_count += sentence.lower().count(word)\n",
    "print(neg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count / (neg_count + pos_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the total number of words is also easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parser(example_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the count per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_count_dict = {}\n",
    "for word in pos:\n",
    "    pos_count_dict[word] = sentence.lower().count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'great': 3, 'increase': 0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Represent text numerically</span><a id='text_numerical'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Bag of Words</span><a id='bows'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn includes the `CountVectorizer` and `TfidfVectorizer` function.  \n",
    "\n",
    "For details, see the documentation:  \n",
    "[TF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)  \n",
    "[TFIDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "Note: these functions also already includes a lot of preprocessing options (e.g. ngrames, remove stop words, accent stripper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_1 = \"The sky is blue.\"\n",
    "doc_2 = \"The sun is bright today.\"\n",
    "doc_3 = \"The sun in the sky is bright.\"\n",
    "doc_4 = \"We can see the shining sun, the bright sun.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate term frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tf = vectorizer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blue', 'bright', 'shining', 'sky', 'sun', 'today']\n",
      "[1 0 0 1 0 0]\n",
      "[0 1 0 0 1 1]\n",
      "[0 1 0 1 1 0]\n",
      "[0 1 1 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "for doc_tf_vector in tf.toarray():\n",
    "    print(doc_tf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">TF-IDF</span><a id='tfidf'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = transformer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.78528828  0.          0.          0.6191303   0.          0.        ]\n",
      "[ 0.          0.47380449  0.          0.          0.47380449  0.74230628]\n",
      "[ 0.          0.53256952  0.          0.65782931  0.53256952  0.        ]\n",
      "[ 0.          0.36626037  0.57381765  0.          0.73252075  0.        ]\n"
     ]
    }
   ],
   "source": [
    "for doc_vector in tfidf.toarray():\n",
    "    print(doc_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More elaborate example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_paragraphs = []\n",
    "for author, value in spacy_text_clean.items():\n",
    "    for article in value:\n",
    "        clean_paragraphs.append(' '.join([x.text for x in article]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_large = transformer.fit_transform(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors: 2500\n",
      "Number of words in dictionary: 26902\n"
     ]
    }
   ],
   "source": [
    "print('Number of vectors:', len(tfidf_large.toarray()))\n",
    "print('Number of words in dictionary:', len(tfidf_large.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2500x26902 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 450663 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Word Embeddings</span><a id='word_embed'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Word2Vec</span><a id='Word2Vec'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple example below is from:  https://medium.com/@mishra.thedeepak/word2vec-in-minutes-gensim-nlp-python-6940f4e00980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = brown.sents()\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('brown_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('brown_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find words most similar to 'mother':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('father', 0.9754652976989746), ('husband', 0.9616575837135315), ('wife', 0.9492062330245972), ('son', 0.9271498918533325), ('voice', 0.9259058237075806), ('friend', 0.9201636910438538), ('boy', 0.9127996563911438), ('doctor', 0.8933248519897461), ('maid', 0.8927353620529175), ('uncle', 0.88885498046875)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"mother\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the odd one out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garden\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"pizza pasta garden fries\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve vector representation of the word \"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.82445643e-02,  -2.40943223e-01,  -6.55970454e-01,\n",
       "        -4.80932444e-01,  -4.06032711e-01,   1.36476532e-01,\n",
       "        -1.59991145e-01,  -2.74191685e-02,   4.31680381e-01,\n",
       "         6.55596256e-01,   1.25325048e+00,  -7.73438334e-01,\n",
       "        -5.25921881e-01,  -1.30807951e-01,   3.34413707e-01,\n",
       "        -1.76354930e-01,   1.27241477e-01,   2.61553586e-01,\n",
       "         3.84649962e-01,  -8.61963928e-01,  -3.18135411e-01,\n",
       "         1.57137334e-01,  -8.10451388e-01,   7.75934875e-01,\n",
       "         2.33956248e-01,  -6.89105690e-01,   1.72768995e-01,\n",
       "        -5.06303966e-01,  -6.60854578e-02,   5.26066184e-01,\n",
       "        -1.04654598e+00,  -8.30319002e-02,   9.10344347e-02,\n",
       "         8.68004411e-02,  -9.44068611e-01,  -8.71582210e-01,\n",
       "         6.06841087e-01,  -6.07468367e-01,   1.88034549e-01,\n",
       "        -1.36444676e+00,  -3.32189739e-01,   4.59514678e-01,\n",
       "         3.41839641e-01,   1.08876240e+00,   5.81619680e-01,\n",
       "        -2.33794630e-01,   5.90388179e-01,  -1.32641211e-01,\n",
       "         1.37848902e+00,   4.24762160e-01,  -7.28657722e-01,\n",
       "        -5.26151001e-01,   1.53519675e-01,  -1.02361433e-01,\n",
       "        -2.93140411e-01,  -3.54104877e-01,   3.77866358e-01,\n",
       "         4.78656352e-01,  -7.75554359e-01,  -4.43456948e-01,\n",
       "        -6.10873699e-01,   2.46224226e-04,   5.74796021e-01,\n",
       "         3.36129338e-01,   5.32471597e-01,   3.63837779e-01,\n",
       "         1.69670105e-01,  -1.79598078e-01,   1.90546870e-01,\n",
       "         7.75349736e-01,  -6.54659629e-01,   9.30552542e-01,\n",
       "        -3.17187190e-01,   1.89170584e-01,   8.08737993e-01,\n",
       "         1.23904027e-01,  -2.46807665e-01,  -3.94924670e-01,\n",
       "         5.57708703e-02,   1.11806118e+00,   1.36649162e-01,\n",
       "         1.14919459e-02,   3.44368875e-01,  -1.12451065e+00,\n",
       "         1.15357876e-01,   2.86711782e-01,   2.30237886e-01,\n",
       "         6.43311918e-01,  -1.29238522e+00,  -2.33212188e-01,\n",
       "        -2.20283374e-01,  -2.38195539e-01,  -9.68601927e-03,\n",
       "         2.34228253e-01,  -2.03108504e-01,   2.83699125e-01,\n",
       "        -1.88099533e-01,   1.05807245e+00,  -5.28496623e-01,\n",
       "         4.89185572e-01], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['human']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Statistical models</span><a id='stat_models'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">\"Traditional\" machine learning</span><a id='trad_ml'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library to use for machine learning is scikit-learn ([\"sklearn\"](http://scikit-learn.org/stable/index.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span>Supervised</span><a id='trad_ml_supervised'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the data into a pandas dataframe (so that we can input it easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for author, value in spacy_text_clean.items():\n",
    "    for article in value:\n",
    "        article_list.append((author, ' '.join([x.text for x in article])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_df = pd.DataFrame(article_list, columns=['author', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>the chairman of an influential national resear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>NickLouth</td>\n",
       "      <td>long distance telephone company and regional b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>EricAuchard</td>\n",
       "      <td>att corp say thursday pron withdraw pron appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>RogerFillion</td>\n",
       "      <td>the clinton administration thursday throw pron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>EdnaFernandes</td>\n",
       "      <td>us car giant ford motor co say on thursday pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                               text\n",
       "624   AaronPressman  the chairman of an influential national resear...\n",
       "1674      NickLouth  long distance telephone company and regional b...\n",
       "1247    EricAuchard  att corp say thursday pron withdraw pron appli...\n",
       "1994   RogerFillion  the clinton administration thursday throw pron...\n",
       "680   EdnaFernandes  us car giant ford motor co say on thursday pro..."
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the sample into a training and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(article_df.text, article_df.author, test_size=0.20, random_state=3561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 500\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple function to train (i.e. fit) and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Accuracy on training set:\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(\"Accuracy on testing set:\")\n",
    "    print(clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span>Na√Øve Bayes estimator</span><a id='trad_ml_supervised_nb'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english'\n",
    "                            )),\n",
    "        \n",
    "    ('clf', MultinomialNB(alpha = 1,\n",
    "                          fit_prior = True\n",
    "                          )\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and show evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.8355\n",
      "Accuracy on testing set:\n",
      "0.678\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.88      0.78      0.82         9\n",
      "       AlanCrosby       0.52      1.00      0.69        11\n",
      "   AlexanderSmith       1.00      0.88      0.93         8\n",
      "  BenjaminKangLim       0.60      0.33      0.43         9\n",
      "    BernardHickey       0.43      0.75      0.55         8\n",
      "      BradDorfman       1.00      0.43      0.60        14\n",
      " DarrenSchuettler       0.71      1.00      0.83        10\n",
      "      DavidLawder       0.57      0.44      0.50         9\n",
      "    EdnaFernandes       1.00      0.30      0.46        10\n",
      "      EricAuchard       0.33      0.22      0.27         9\n",
      "   FumikoFujisaki       1.00      1.00      1.00        10\n",
      "   GrahamEarnshaw       0.47      0.70      0.56        10\n",
      " HeatherScoffield       1.00      0.64      0.78        14\n",
      "       JanLopatka       0.78      0.58      0.67        12\n",
      "    JaneMacartney       0.67      0.40      0.50        10\n",
      "     JimGilchrist       0.75      1.00      0.86         6\n",
      "   JoWinterbottom       0.71      1.00      0.83        10\n",
      "         JoeOrtiz       0.20      0.33      0.25         3\n",
      "     JohnMastrini       0.80      0.40      0.53        10\n",
      "     JonathanBirt       0.69      1.00      0.81        11\n",
      "      KarlPenhaul       0.93      1.00      0.96        13\n",
      "        KeithWeir       1.00      0.47      0.64        17\n",
      "   KevinDrawbaugh       0.55      0.86      0.67         7\n",
      "    KevinMorrison       0.50      0.55      0.52        11\n",
      "    KirstinRidley       0.50      0.88      0.64         8\n",
      "KouroshKarimkhany       0.54      0.88      0.67         8\n",
      "        LydiaZajc       0.86      0.67      0.75         9\n",
      "   LynneO'Donnell       0.86      0.92      0.89        13\n",
      "  LynnleyBrowning       1.00      1.00      1.00         9\n",
      "  MarcelMichelson       0.73      0.80      0.76        10\n",
      "     MarkBendeich       1.00      0.33      0.50        12\n",
      "       MartinWolk       0.80      0.40      0.53        10\n",
      "     MatthewBunce       1.00      0.79      0.88        14\n",
      "    MichaelConnor       0.78      0.78      0.78         9\n",
      "       MureDickie       0.21      0.33      0.26         9\n",
      "        NickLouth       0.82      0.69      0.75        13\n",
      "  PatriciaCommins       0.27      0.80      0.40         5\n",
      "    PeterHumphrey       0.40      0.89      0.55         9\n",
      "       PierreTran       0.86      0.75      0.80         8\n",
      "       RobinSidel       0.89      0.80      0.84        10\n",
      "     RogerFillion       1.00      0.92      0.96        12\n",
      "      SamuelPerry       0.43      0.60      0.50        10\n",
      "     SarahDavison       1.00      0.45      0.62        11\n",
      "      ScottHillis       0.45      0.56      0.50         9\n",
      "      SimonCowell       1.00      0.70      0.82        10\n",
      "         TanEeLyn       0.33      0.11      0.17         9\n",
      "   TheresePoletti       0.77      0.77      0.77        13\n",
      "       TimFarrand       0.64      0.90      0.75        10\n",
      "       ToddNissen       0.69      0.90      0.78        10\n",
      "     WilliamKazer       0.67      0.22      0.33         9\n",
      "\n",
      "      avg / total       0.74      0.68      0.67       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_bayes_results.pkl']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'naive_bayes_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict out of sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_y, example_X = y_train[33], X_train[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: TanEeLyn\n",
      "Predicted author: TanEeLyn\n"
     ]
    }
   ],
   "source": [
    "print('Actual author:', example_y)\n",
    "print('Predicted author:', clf.predict([example_X])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span>Support Vector Machines (SVM)</span><a id='trad_ml_supervised_svm'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_svm = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english'\n",
    "                            )),\n",
    "        \n",
    "    ('clf', SVC(kernel='rbf' ,\n",
    "                C=10, gamma=0.3)\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* The SVC estimator is very sensitive to the hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and show evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.997\n",
      "Accuracy on testing set:\n",
      "0.814\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       1.00      1.00      1.00         9\n",
      "       AlanCrosby       0.77      0.91      0.83        11\n",
      "   AlexanderSmith       0.78      0.88      0.82         8\n",
      "  BenjaminKangLim       0.80      0.89      0.84         9\n",
      "    BernardHickey       0.73      1.00      0.84         8\n",
      "      BradDorfman       0.90      0.64      0.75        14\n",
      " DarrenSchuettler       0.91      1.00      0.95        10\n",
      "      DavidLawder       0.67      0.67      0.67         9\n",
      "    EdnaFernandes       0.86      0.60      0.71        10\n",
      "      EricAuchard       0.75      0.33      0.46         9\n",
      "   FumikoFujisaki       1.00      1.00      1.00        10\n",
      "   GrahamEarnshaw       0.73      0.80      0.76        10\n",
      " HeatherScoffield       0.93      0.93      0.93        14\n",
      "       JanLopatka       0.77      0.83      0.80        12\n",
      "    JaneMacartney       0.71      0.50      0.59        10\n",
      "     JimGilchrist       1.00      1.00      1.00         6\n",
      "   JoWinterbottom       0.71      1.00      0.83        10\n",
      "         JoeOrtiz       0.33      0.33      0.33         3\n",
      "     JohnMastrini       0.86      0.60      0.71        10\n",
      "     JonathanBirt       0.73      1.00      0.85        11\n",
      "      KarlPenhaul       0.93      1.00      0.96        13\n",
      "        KeithWeir       1.00      0.65      0.79        17\n",
      "   KevinDrawbaugh       0.50      0.86      0.63         7\n",
      "    KevinMorrison       1.00      0.73      0.84        11\n",
      "    KirstinRidley       0.88      0.88      0.88         8\n",
      "KouroshKarimkhany       0.70      0.88      0.78         8\n",
      "        LydiaZajc       1.00      0.89      0.94         9\n",
      "   LynneO'Donnell       1.00      0.92      0.96        13\n",
      "  LynnleyBrowning       1.00      1.00      1.00         9\n",
      "  MarcelMichelson       0.91      1.00      0.95        10\n",
      "     MarkBendeich       0.92      0.92      0.92        12\n",
      "       MartinWolk       0.70      0.70      0.70        10\n",
      "     MatthewBunce       1.00      0.79      0.88        14\n",
      "    MichaelConnor       0.89      0.89      0.89         9\n",
      "       MureDickie       0.25      0.22      0.24         9\n",
      "        NickLouth       0.85      0.85      0.85        13\n",
      "  PatriciaCommins       0.83      1.00      0.91         5\n",
      "    PeterHumphrey       0.70      0.78      0.74         9\n",
      "       PierreTran       1.00      1.00      1.00         8\n",
      "       RobinSidel       0.90      0.90      0.90        10\n",
      "     RogerFillion       1.00      1.00      1.00        12\n",
      "      SamuelPerry       0.64      0.70      0.67        10\n",
      "     SarahDavison       0.83      0.91      0.87        11\n",
      "      ScottHillis       0.86      0.67      0.75         9\n",
      "      SimonCowell       1.00      0.90      0.95        10\n",
      "         TanEeLyn       0.67      0.44      0.53         9\n",
      "   TheresePoletti       0.79      0.85      0.81        13\n",
      "       TimFarrand       0.82      0.90      0.86        10\n",
      "       ToddNissen       0.82      0.90      0.86        10\n",
      "     WilliamKazer       0.29      0.44      0.35         9\n",
      "\n",
      "      avg / total       0.83      0.81      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf_svm, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_results.pkl']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf_svm, 'svm_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict out of sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_y, example_X = y_train[33], X_train[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: TanEeLyn\n",
      "Predicted author: TanEeLyn\n"
     ]
    }
   ],
   "source": [
    "print('Actual author:', example_y)\n",
    "print('Predicted author:', clf_svm.predict([example_X])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span>Model Selection and Evaluation</span><a id='trad_ml_eval'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `TfidfVectorizer` and `SVC()` estimator take a lot of hyperparameters.  \n",
    "\n",
    "It can be difficult to figure out what the best parameters are.\n",
    "\n",
    "We can use `GridSearchCV` to help figure this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the options that should be tried out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lala\n"
     ]
    }
   ],
   "source": [
    "print('lala')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_search = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "parameters = { 'vect__stop_words': ['english'],\n",
    "                'vect__strip_accents': ['unicode'],\n",
    "              'vect__max_features' : [1500],\n",
    "              'vect__ngram_range': [(1,1), (2,2) ],\n",
    "             'clf__gamma' : [0.2, 0.3, 0.4], \n",
    "             'clf__C' : [8, 10, 12],\n",
    "              'clf__kernel' : ['rbf']\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=6,\n",
       "       param_grid={'clf__gamma': [0.2, 0.3, 0.4], 'vect__max_features': [1500], 'clf__kernel': ['rbf'], 'clf__C': [8, 10, 12], 'vect__strip_accents': ['unicode'], 'vect__ngram_range': [(1, 1), (2, 2)], 'vect__stop_words': ['english']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(f1_score, average=micro), verbose=0)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(clf_search, param_grid=parameters, scoring=make_scorer(f1_score, average='micro'), n_jobs=6)\n",
    "grid.fit(X_train, y_train)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* if you are on a powerful unix system you can set n_jobs to the number of available threads to speed up the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'clf__gamma': 0.2, 'vect__max_features': 1500, 'clf__kernel': 'rbf', 'clf__C': 8, 'vect__strip_accents': 'unicode', 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english'} with a score of 0.78\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       1.00      1.00      1.00         9\n",
      "       AlanCrosby       0.77      0.91      0.83        11\n",
      "   AlexanderSmith       0.78      0.88      0.82         8\n",
      "  BenjaminKangLim       0.89      0.89      0.89         9\n",
      "    BernardHickey       0.73      1.00      0.84         8\n",
      "      BradDorfman       0.90      0.64      0.75        14\n",
      " DarrenSchuettler       0.91      1.00      0.95        10\n",
      "      DavidLawder       0.62      0.56      0.59         9\n",
      "    EdnaFernandes       0.86      0.60      0.71        10\n",
      "      EricAuchard       0.60      0.33      0.43         9\n",
      "   FumikoFujisaki       1.00      1.00      1.00        10\n",
      "   GrahamEarnshaw       0.73      0.80      0.76        10\n",
      " HeatherScoffield       0.93      0.93      0.93        14\n",
      "       JanLopatka       0.77      0.83      0.80        12\n",
      "    JaneMacartney       0.71      0.50      0.59        10\n",
      "     JimGilchrist       1.00      1.00      1.00         6\n",
      "   JoWinterbottom       0.71      1.00      0.83        10\n",
      "         JoeOrtiz       0.33      0.33      0.33         3\n",
      "     JohnMastrini       0.86      0.60      0.71        10\n",
      "     JonathanBirt       0.73      1.00      0.85        11\n",
      "      KarlPenhaul       0.93      1.00      0.96        13\n",
      "        KeithWeir       1.00      0.65      0.79        17\n",
      "   KevinDrawbaugh       0.50      0.86      0.63         7\n",
      "    KevinMorrison       1.00      0.73      0.84        11\n",
      "    KirstinRidley       0.88      0.88      0.88         8\n",
      "KouroshKarimkhany       0.70      0.88      0.78         8\n",
      "        LydiaZajc       1.00      0.89      0.94         9\n",
      "   LynneO'Donnell       1.00      0.92      0.96        13\n",
      "  LynnleyBrowning       1.00      1.00      1.00         9\n",
      "  MarcelMichelson       0.91      1.00      0.95        10\n",
      "     MarkBendeich       0.92      0.92      0.92        12\n",
      "       MartinWolk       0.70      0.70      0.70        10\n",
      "     MatthewBunce       1.00      0.79      0.88        14\n",
      "    MichaelConnor       0.89      0.89      0.89         9\n",
      "       MureDickie       0.25      0.22      0.24         9\n",
      "        NickLouth       0.86      0.92      0.89        13\n",
      "  PatriciaCommins       0.71      1.00      0.83         5\n",
      "    PeterHumphrey       0.70      0.78      0.74         9\n",
      "       PierreTran       1.00      1.00      1.00         8\n",
      "       RobinSidel       0.90      0.90      0.90        10\n",
      "     RogerFillion       1.00      1.00      1.00        12\n",
      "      SamuelPerry       0.60      0.60      0.60        10\n",
      "     SarahDavison       0.83      0.91      0.87        11\n",
      "      ScottHillis       0.86      0.67      0.75         9\n",
      "      SimonCowell       1.00      0.90      0.95        10\n",
      "         TanEeLyn       0.67      0.44      0.53         9\n",
      "   TheresePoletti       0.85      0.85      0.85        13\n",
      "       TimFarrand       0.82      0.90      0.86        10\n",
      "       ToddNissen       0.82      0.90      0.86        10\n",
      "     WilliamKazer       0.33      0.56      0.42         9\n",
      "\n",
      "      avg / total       0.83      0.81      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "y_true, y_pred = y_test, grid.predict(X_test)\n",
    "print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span>Unsupervised</span><a id='trad_ml_unsupervised'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span>Latent Dirichilet Allocation (LDA)</span><a id='trad_ml_unsupervised_lda'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizer (using countvectorizer for the sake of example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english', max_df=0.8)\n",
    "tf_large = vectorizer.fit_transform(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "n_top_words = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=10,\n",
    "                                learning_method='online',\n",
    "                                n_jobs=1)\n",
    "lda_fitted = lda.fit_transform(tf_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_top_words(model, feature_names, n_top_words):\n",
    "    out_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        out_list.append((topic_idx+1, \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])))\n",
    "    out_df = pd.DataFrame(out_list, columns=['topic_id', 'top_words'])\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df = save_top_words(lda, vectorizer.get_feature_names(), n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>top_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>company share million percent group pound mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>bank financial market banking fund debt firm n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>china kong hong chinese beijing people year de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>thomson ford car television gm new csf cable t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>government trade percent year czech oil crown ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>gold bre canada toronto busang north canadian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>company service new corp computer internet net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>tonne market price year trader percent stock w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>percent year million profit sale analyst share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>tobacco air conrail boeing state airline compa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id                                          top_words\n",
       "0         1  company share million percent group pound mark...\n",
       "1         2  bank financial market banking fund debt firm n...\n",
       "2         3  china kong hong chinese beijing people year de...\n",
       "3         4  thomson ford car television gm new csf cable t...\n",
       "4         5  government trade percent year czech oil crown ...\n",
       "5         6  gold bre canada toronto busang north canadian ...\n",
       "6         7  company service new corp computer internet net...\n",
       "7         8  tonne market price year trader percent stock w...\n",
       "8         9  percent year million profit sale analyst share...\n",
       "9        10  tobacco air conrail boeing state airline compa..."
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span>pyLDAvis</span><a id='trad_ml_unsupervised_pyLDAvis'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u1238329/anaconda3/lib/python3.5/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el13432476860674764248518930798\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el13432476860674764248518930798_data = {\"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"mdsDat\": {\"x\": [-0.19945724696240769, -0.072734368074207, 0.0907544764872037, -0.07485991061902489, -0.024171846243769265, -0.050096221773481504, -0.14474441183567574, 0.1628936655618562, 0.09146496469554798, 0.22095089876395813], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"y\": [-0.028757754124876883, 0.04887747335517329, -0.02755694890111977, 0.08866228667077078, -0.02044839324791202, 0.016569371166412803, -0.13416644124770688, 0.1738817216894316, 0.11264687890374113, -0.2297081942639139], \"Freq\": [18.742531197718233, 18.62892821978641, 14.374058582669521, 11.442473299976456, 9.036709297650738, 8.600028121374132, 6.823840338569139, 4.581431847942361, 4.4778007996117974, 3.292198294701207]}, \"token.table\": {\"Term\": [\"1995\", \"1995\", \"1995\", \"1995\", \"1995\", \"1995\", \"1995\", \"1995\", \"1996\", \"1996\", \"1996\", \"1996\", \"1996\", \"1996\", \"1996\", \"1997\", \"1997\", \"1997\", \"1997\", \"1997\", \"1997\", \"1997\", \"3com\", \"542\", \"542\", \"542\", \"542\", \"747\", \"747\", \"747\", \"abn\", \"abn\", \"abn\", \"abn\", \"abnormal\", \"abnormal\", \"access\", \"access\", \"access\", \"access\", \"access\", \"access\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"activist\", \"add\", \"add\", \"add\", \"add\", \"add\", \"add\", \"add\", \"add\", \"add\", \"add\", \"aerospatiale\", \"aerospatiale\", \"agreement\", \"agreement\", \"agreement\", \"agreement\", \"agreement\", \"agreement\", \"agreement\", \"agreement\", \"air\", \"air\", \"air\", \"air\", \"air\", \"air\", \"air\", \"airbus\", \"airbus\", \"aircraft\", \"aircraft\", \"aircraft\", \"aircraft\", \"airline\", \"airline\", \"airlines\", \"airlines\", \"airport\", \"airport\", \"airport\", \"airport\", \"alcatel\", \"alcatel\", \"amro\", \"amro\", \"amro\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"aol\", \"apec\", \"apple\", \"army\", \"army\", \"army\", \"army\", \"arrest\", \"arrest\", \"arrest\", \"arrest\", \"arrival\", \"arrival\", \"arrival\", \"arrival\", \"arrival\", \"asset\", \"asset\", \"asset\", \"asset\", \"att\", \"attorney\", \"attractive\", \"attractive\", \"attractive\", \"attractive\", \"attractive\", \"auction\", \"auction\", \"auction\", \"australia\", \"australia\", \"australia\", \"australia\", \"australia\", \"australian\", \"australian\", \"auto\", \"auto\", \"automaker\", \"automotive\", \"automotive\", \"automotive\", \"automotive\", \"aviation\", \"aviation\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"banker\", \"banker\", \"banker\", \"banker\", \"banker\", \"banking\", \"bankruptcy\", \"bankruptcy\", \"bankruptcy\", \"bankruptcy\", \"barrick\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"bean\", \"beijing\", \"beijing\", \"bell\", \"bell\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"boeing\", \"boeing\", \"bond\", \"bond\", \"bond\", \"bond\", \"bre\", \"britain\", \"britain\", \"britain\", \"britain\", \"britain\", \"britain\", \"british\", \"british\", \"british\", \"british\", \"british\", \"british\", \"british\", \"british\", \"broadcast\", \"broadcast\", \"broadcast\", \"broadcast\", \"broadcasting\", \"broadcasting\", \"brokerage\", \"brokerage\", \"brokerage\", \"brokerage\", \"brokerage\", \"bskyb\", \"budget\", \"budget\", \"budget\", \"busang\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buying\", \"buying\", \"buying\", \"bzw\", \"bzw\", \"bzw\", \"bzw\", \"cabinet\", \"cabinet\", \"cable\", \"cable\", \"cable\", \"calif\", \"canada\", \"canada\", \"canada\", \"canada\", \"canada\", \"canada\", \"canada\", \"canadian\", \"canadian\", \"canadian\", \"canadian\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"car\", \"car\", \"car\", \"car\", \"car\", \"cargo\", \"cargo\", \"cargo\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"casino\", \"casino\", \"cathay\", \"cathay\", \"ce\", \"cellular\", \"cereal\", \"cereal\", \"channel\", \"channel\", \"channel\", \"channel\", \"channel\", \"chee\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"china\", \"china\", \"china\", \"china\", \"chinese\", \"chinese\", \"chinese\", \"chrysler\", \"cigarette\", \"cigarette\", \"civil\", \"civil\", \"civil\", \"civil\", \"civil\", \"client\", \"client\", \"client\", \"client\", \"club\", \"club\", \"club\", \"cnb\", \"cnb\", \"coast\", \"coast\", \"coast\", \"cocoa\", \"cocoa\", \"coffee\", \"coffee\", \"colony\", \"communication\", \"communication\", \"communication\", \"communication\", \"communication\", \"communications\", \"communications\", \"communist\", \"communist\", \"communist\", \"communist\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"computer\", \"computer\", \"conrail\", \"continue\", \"continue\", \"continue\", \"continue\", \"continue\", \"continue\", \"continue\", \"continue\", \"continue\", \"continue\", \"copper\", \"copper\", \"copper\", \"corn\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"corruption\", \"corruption\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"court\", \"court\", \"court\", \"court\", \"court\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"crop\", \"crown\", \"crown\", \"crown\", \"csf\", \"csf\", \"csf\", \"csx\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"czech\", \"czech\", \"czech\", \"czech\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"death\", \"death\", \"death\", \"death\", \"debate\", \"debate\", \"debate\", \"debt\", \"debt\", \"debt\", \"debt\", \"debt\", \"defence\", \"defence\", \"defence\", \"defence\", \"defence\", \"defence\", \"deficit\", \"democracy\", \"democratic\", \"democratic\", \"deng\", \"deposit\", \"deposit\", \"deposit\", \"deposit\", \"detroit\", \"device\", \"digital\", \"digital\", \"diplomat\", \"diplomat\", \"diplomat\", \"dissident\", \"distance\", \"distance\", \"drug\", \"drug\", \"drug\", \"earn\", \"earn\", \"earn\", \"earn\", \"earn\", \"earn\", \"earn\", \"earn\", \"earn\", \"earning\", \"earning\", \"earning\", \"earning\", \"earning\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economist\", \"economist\", \"economist\", \"economist\", \"economist\", \"economy\", \"economy\", \"economy\", \"economy\", \"economy\", \"economy\", \"economy\", \"electric\", \"electricity\", \"electricity\", \"electronics\", \"electronics\", \"electronics\", \"embassy\", \"embassy\", \"ethnic\", \"ethnic\", \"exceptional\", \"exceptional\", \"exclude\", \"exclude\", \"exclude\", \"exclude\", \"exclude\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"export\", \"export\", \"export\", \"export\", \"exporter\", \"exporter\", \"factory\", \"factory\", \"factory\", \"factory\", \"factory\", \"factory\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"farmer\", \"farmer\", \"farmer\", \"farmer\", \"farmer\", \"fcc\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"film\", \"film\", \"finance\", \"finance\", \"finance\", \"finance\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"flight\", \"flight\", \"float\", \"float\", \"float\", \"float\", \"float\", \"florida\", \"florida\", \"florida\", \"food\", \"food\", \"food\", \"food\", \"food\", \"food\", \"food\", \"ford\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"fourth\", \"fourth\", \"fourth\", \"fourth\", \"fourth\", \"fourth\", \"fourth\", \"franc\", \"franc\", \"franchise\", \"franchise\", \"franchise\", \"franchise\", \"freedom\", \"freedom\", \"freeport\", \"fund\", \"fund\", \"fund\", \"fund\", \"fund\", \"fund\", \"fund\", \"gas\", \"gas\", \"gas\", \"gas\", \"gazprom\", \"gec\", \"gec\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"gm\", \"gold\", \"gold\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"grade\", \"grade\", \"grade\", \"grade\", \"grade\", \"grain\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"growth\", \"growth\", \"growth\", \"growth\", \"growth\", \"growth\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"handover\", \"harvest\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"hilton\", \"hong\", \"hong\", \"hong\", \"hotel\", \"hotel\", \"hotel\", \"hotel\", \"hp\", \"hughes\", \"human\", \"hwa\", \"hwang\", \"ibm\", \"ibm\", \"import\", \"import\", \"import\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"income\", \"income\", \"income\", \"income\", \"income\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"index\", \"index\", \"index\", \"indonesia\", \"indonesia\", \"indonesia\", \"indonesian\", \"indonesian\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"inflation\", \"inflation\", \"inflation\", \"institution\", \"institution\", \"institution\", \"institution\", \"insurance\", \"insurance\", \"insurance\", \"insurance\", \"insurance\", \"intel\", \"internet\", \"investigation\", \"investigation\", \"investigation\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investor\", \"investor\", \"investor\", \"investor\", \"investor\", \"investor\", \"investor\", \"investor\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"item\", \"item\", \"item\", \"itt\", \"itt\", \"itt\", \"ivory\", \"ivory\", \"japan\", \"japan\", \"japan\", \"japan\", \"japan\", \"japan\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"jet\", \"jet\", \"jiang\", \"jones\", \"jones\", \"jones\", \"judge\", \"judge\", \"judge\", \"judge\", \"judge\", \"july\", \"july\", \"july\", \"july\", \"july\", \"july\", \"kellogg\", \"kill\", \"kill\", \"kill\", \"klaus\", \"klaus\", \"kmart\", \"kmart\", \"kong\", \"kong\", \"kong\", \"korea\", \"korea\", \"korea\", \"korea\", \"korea\", \"korea\", \"korean\", \"korean\", \"korean\", \"lagardere\", \"lagardere\", \"law\", \"law\", \"law\", \"law\", \"law\", \"law\", \"law\", \"lawsuit\", \"lawsuit\", \"lawsuit\", \"lawyer\", \"lawyer\", \"lawyer\", \"lawyer\", \"leader\", \"leader\", \"leader\", \"leader\", \"leader\", \"leader\", \"leader\", \"lee\", \"lee\", \"lee\", \"legislature\", \"lending\", \"lending\", \"li\", \"liberty\", \"lloyd\", \"lloyd\", \"lme\", \"loan\", \"loan\", \"loan\", \"london\", \"london\", \"london\", \"london\", \"london\", \"london\", \"london\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"machine\", \"machine\", \"machine\", \"mainframe\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"management\", \"management\", \"management\", \"management\", \"management\", \"management\", \"mao\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"markets\", \"markets\", \"mci\", \"melbourne\", \"melbourne\", \"melbourne\", \"mercury\", \"mercury\", \"mercury\", \"mercury\", \"merger\", \"merger\", \"merger\", \"merger\", \"merger\", \"metal\", \"metal\", \"metal\", \"mgam\", \"mgam\", \"microsoft\", \"mid\", \"mid\", \"mid\", \"mid\", \"mid\", \"mid\", \"mid\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"mining\", \"mining\", \"mining\", \"minister\", \"minister\", \"minister\", \"minister\", \"minister\", \"minister\", \"modem\", \"morris\", \"morris\", \"moscow\", \"natwest\", \"natwest\", \"natwest\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"netscape\", \"network\", \"network\", \"network\", \"network\", \"network\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nickel\", \"nickel\", \"nomura\", \"nomura\", \"norfolk\", \"norilsk\", \"norilsk\", \"north\", \"north\", \"north\", \"north\", \"north\", \"north\", \"north\", \"north\", \"north\", \"northern\", \"northern\", \"northern\", \"nynex\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"official\", \"official\", \"official\", \"official\", \"official\", \"official\", \"official\", \"official\", \"official\", \"official\", \"oil\", \"oil\", \"oil\", \"oil\", \"online\", \"optus\", \"optus\", \"optus\", \"ounce\", \"outlook\", \"outlook\", \"outlook\", \"outlook\", \"output\", \"output\", \"output\", \"output\", \"partnership\", \"partnership\", \"partnership\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"passenger\", \"passenger\", \"passenger\", \"passenger\", \"passenger\", \"patten\", \"pc\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penny\", \"penny\", \"pension\", \"pension\", \"pension\", \"pension\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"philip\", \"philip\", \"philip\", \"phone\", \"phone\", \"phone\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plane\", \"plane\", \"plant\", \"plant\", \"plant\", \"plant\", \"plant\", \"plant\", \"plc\", \"plc\", \"plc\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"police\", \"police\", \"police\", \"police\", \"political\", \"political\", \"political\", \"political\", \"port\", \"port\", \"port\", \"pound\", \"pound\", \"pound\", \"pounds\", \"pounds\", \"prague\", \"prague\", \"prague\", \"prague\", \"prague\", \"pre\", \"pre\", \"pre\", \"pre\", \"pre\", \"pre\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"pretax\", \"pretax\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"prime\", \"prime\", \"prime\", \"prime\", \"prime\", \"prime\", \"prime\", \"probe\", \"probe\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"production\", \"production\", \"production\", \"production\", \"production\", \"production\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"provisional\", \"provisional\", \"provisional\", \"prudential\", \"prudential\", \"prudential\", \"pyongyang\", \"quaker\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quota\", \"quota\", \"radio\", \"radio\", \"radio\", \"radio\", \"rain\", \"rain\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"retailer\", \"retailer\", \"retailer\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"riot\", \"riot\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"robotics\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"russia\", \"russia\", \"russian\", \"russian\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"satellite\", \"satellite\", \"scandal\", \"season\", \"season\", \"securities\", \"securities\", \"securities\", \"securities\", \"securities\", \"securities\", \"security\", \"security\", \"security\", \"security\", \"security\", \"security\", \"security\", \"security\", \"seoul\", \"seoul\", \"server\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"shanghai\", \"shanghai\", \"shanghai\", \"share\", \"share\", \"share\", \"share\", \"share\", \"shareholder\", \"shareholder\", \"shareholder\", \"shareholder\", \"shareholder\", \"sino\", \"sino\", \"skoda\", \"skoda\", \"skoda\", \"smoke\", \"smoke\", \"snapple\", \"software\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"south\", \"south\", \"south\", \"south\", \"south\", \"south\", \"south\", \"south\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"soviet\", \"soviet\", \"soviet\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sprint\", \"st\", \"st\", \"st\", \"stake\", \"stake\", \"stake\", \"stake\", \"stake\", \"stake\", \"stake\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"station\", \"station\", \"station\", \"station\", \"station\", \"station\", \"steel\", \"steel\", \"steel\", \"stg\", \"stg\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"store\", \"store\", \"store\", \"store\", \"store\", \"store\", \"strathcona\", \"strike\", \"strike\", \"strike\", \"strike\", \"strike\", \"strike\", \"strike\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"subscriber\", \"subscriber\", \"suez\", \"suez\", \"suez\", \"sugar\", \"sugar\", \"surcharge\", \"suspend\", \"suspend\", \"suspend\", \"taiwan\", \"taiwan\", \"takeover\", \"takeover\", \"takeover\", \"takeover\", \"takeover\", \"takeover\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"telecom\", \"telecom\", \"telecom\", \"telecom\", \"telecom\", \"telecom\", \"television\", \"television\", \"television\", \"television\", \"television\", \"television\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"territory\", \"territory\", \"territory\", \"territory\", \"textile\", \"thomson\", \"thomson\", \"thomson\", \"tobacco\", \"tobacco\", \"tobacco\", \"tokyo\", \"tokyo\", \"tokyo\", \"tonne\", \"tonne\", \"toronto\", \"toronto\", \"town\", \"town\", \"town\", \"town\", \"town\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trader\", \"trader\", \"trader\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"truck\", \"truck\", \"truck\", \"trust\", \"trust\", \"trust\", \"trust\", \"tung\", \"tv\", \"tv\", \"uaw\", \"union\", \"union\", \"union\", \"union\", \"union\", \"union\", \"union\", \"united\", \"united\", \"united\", \"united\", \"united\", \"united\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"user\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"vehicle\", \"vehicle\", \"vehicle\", \"vehicle\", \"venture\", \"venture\", \"venture\", \"venture\", \"venture\", \"venture\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"wage\", \"wage\", \"walsh\", \"walsh\", \"wang\", \"watchdog\", \"watchdog\", \"weather\", \"weather\", \"weather\", \"web\", \"wednesday\", \"wednesday\", \"wednesday\", \"wednesday\", \"wednesday\", \"wednesday\", \"wednesday\", \"wednesday\", \"wednesday\", \"wednesday\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"windows\", \"wireless\", \"wireless\", \"wmx\", \"worker\", \"worker\", \"worker\", \"worker\", \"worker\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"xiaoping\", \"xinhua\", \"xinjiang\", \"xinjiang\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yen\", \"yen\", \"yen\", \"yuan\", \"yuan\", \"yuan\", \"yuan\", \"zealand\", \"zemin\", \"zinc\", \"zinc\"], \"Topic\": [1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 9, 4, 1, 2, 7, 9, 4, 7, 9, 1, 2, 4, 6, 1, 7, 1, 2, 3, 4, 5, 9, 1, 2, 4, 6, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 9, 1, 2, 3, 4, 5, 6, 9, 10, 2, 3, 4, 5, 7, 8, 9, 4, 9, 4, 5, 8, 9, 4, 9, 4, 9, 3, 5, 9, 10, 1, 9, 1, 2, 6, 1, 2, 3, 4, 5, 6, 7, 9, 10, 4, 5, 4, 3, 5, 6, 8, 2, 3, 8, 10, 2, 3, 5, 7, 10, 1, 2, 6, 9, 4, 9, 1, 2, 4, 6, 7, 2, 6, 7, 1, 2, 3, 5, 7, 1, 2, 1, 8, 8, 1, 2, 4, 8, 1, 9, 1, 2, 5, 6, 7, 2, 4, 6, 7, 9, 6, 1, 2, 5, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 3, 5, 2, 4, 1, 2, 3, 4, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 9, 1, 5, 6, 7, 10, 1, 2, 3, 6, 8, 9, 1, 2, 3, 5, 6, 7, 8, 9, 3, 4, 8, 10, 2, 8, 1, 2, 4, 6, 7, 8, 1, 3, 5, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 9, 2, 4, 7, 1, 2, 6, 7, 3, 5, 2, 4, 8, 4, 1, 2, 3, 4, 5, 8, 10, 1, 2, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 8, 10, 3, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 7, 3, 7, 2, 4, 1, 7, 1, 3, 4, 8, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 7, 3, 5, 7, 8, 2, 9, 2, 3, 5, 6, 8, 2, 4, 6, 9, 2, 3, 5, 5, 6, 2, 5, 7, 7, 10, 5, 7, 3, 2, 3, 4, 8, 9, 4, 8, 3, 5, 8, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 4, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 7, 10, 7, 1, 2, 4, 6, 7, 8, 9, 10, 3, 5, 1, 2, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 6, 8, 9, 1, 2, 3, 4, 5, 6, 9, 10, 7, 1, 5, 6, 1, 8, 9, 9, 1, 2, 4, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 9, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 7, 10, 3, 5, 6, 1, 2, 5, 6, 7, 1, 2, 3, 5, 8, 9, 5, 3, 3, 5, 3, 5, 6, 7, 10, 8, 4, 4, 8, 3, 5, 10, 3, 4, 5, 1, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 6, 7, 1, 2, 3, 5, 6, 7, 10, 1, 3, 5, 6, 7, 1, 2, 3, 5, 6, 7, 9, 2, 2, 7, 1, 4, 8, 3, 10, 3, 10, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 7, 1, 5, 7, 10, 5, 7, 1, 3, 5, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 7, 9, 10, 5, 2, 3, 4, 6, 8, 9, 6, 8, 1, 2, 5, 6, 1, 2, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 9, 1, 2, 4, 6, 8, 2, 8, 9, 1, 2, 3, 5, 7, 9, 10, 8, 1, 2, 5, 7, 1, 2, 3, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 8, 10, 1, 2, 1, 2, 4, 8, 2, 3, 10, 1, 2, 3, 5, 6, 7, 9, 1, 2, 5, 7, 5, 1, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 2, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 6, 7, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 7, 2, 3, 7, 10, 4, 8, 3, 3, 10, 4, 8, 5, 7, 10, 1, 2, 3, 4, 6, 1, 2, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 7, 2, 7, 10, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 5, 7, 2, 3, 6, 7, 1, 2, 5, 6, 7, 4, 4, 6, 8, 9, 1, 2, 4, 5, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 6, 1, 2, 9, 5, 7, 1, 2, 4, 5, 6, 7, 1, 4, 6, 7, 4, 9, 3, 1, 4, 7, 3, 4, 6, 8, 9, 1, 2, 3, 5, 7, 8, 1, 3, 9, 10, 5, 6, 1, 4, 3, 5, 7, 3, 5, 7, 8, 9, 10, 7, 8, 10, 8, 9, 2, 3, 5, 6, 8, 9, 10, 4, 8, 9, 3, 6, 8, 9, 1, 2, 3, 4, 5, 6, 8, 2, 3, 7, 3, 6, 7, 3, 3, 2, 9, 7, 5, 6, 7, 1, 2, 3, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 4, 9, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 9, 3, 1, 2, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 9, 2, 6, 4, 1, 2, 7, 2, 4, 6, 8, 1, 2, 4, 6, 9, 5, 7, 10, 6, 9, 4, 1, 2, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 9, 10, 2, 3, 5, 6, 8, 10, 4, 2, 9, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 4, 1, 2, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 10, 1, 6, 9, 5, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 2, 7, 9, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 6, 7, 4, 2, 4, 8, 10, 1, 4, 6, 7, 1, 5, 7, 10, 1, 2, 4, 1, 2, 3, 4, 5, 6, 10, 1, 4, 5, 8, 9, 3, 4, 3, 5, 6, 9, 1, 2, 1, 2, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 9, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 9, 1, 2, 5, 7, 8, 10, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 9, 3, 6, 8, 10, 2, 3, 5, 8, 3, 7, 10, 1, 2, 6, 1, 2, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 1, 2, 4, 5, 6, 7, 8, 9, 2, 3, 4, 5, 6, 7, 10, 6, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 5, 7, 8, 10, 1, 2, 4, 5, 6, 7, 1, 2, 4, 5, 7, 8, 9, 10, 2, 3, 5, 1, 2, 6, 10, 1, 1, 4, 5, 7, 10, 5, 7, 2, 3, 5, 8, 7, 10, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 4, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 10, 3, 10, 1, 2, 3, 4, 5, 7, 9, 10, 4, 1, 2, 3, 4, 5, 6, 8, 9, 1, 5, 1, 5, 1, 2, 4, 5, 6, 7, 8, 4, 8, 6, 7, 8, 1, 2, 4, 6, 7, 9, 2, 3, 4, 5, 6, 8, 9, 10, 7, 10, 4, 1, 2, 3, 4, 5, 6, 8, 9, 3, 6, 7, 1, 2, 4, 6, 7, 1, 2, 4, 6, 9, 3, 5, 2, 5, 8, 9, 10, 1, 4, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 5, 7, 8, 9, 10, 2, 3, 5, 7, 9, 10, 3, 5, 10, 1, 2, 4, 8, 10, 4, 1, 2, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 8, 10, 2, 3, 8, 1, 2, 1, 2, 4, 6, 7, 9, 10, 1, 3, 4, 7, 8, 10, 10, 1, 3, 4, 5, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 4, 8, 1, 2, 6, 1, 7, 5, 6, 9, 10, 3, 7, 1, 2, 3, 4, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 1, 2, 4, 5, 7, 8, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 6, 5, 1, 8, 9, 1, 2, 9, 3, 6, 7, 7, 10, 7, 10, 2, 3, 5, 8, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 9, 1, 8, 10, 2, 3, 6, 9, 3, 4, 8, 8, 1, 2, 3, 5, 6, 8, 10, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 8, 10, 1, 2, 3, 4, 7, 8, 2, 3, 4, 5, 6, 8, 10, 1, 5, 9, 10, 3, 2, 6, 1, 5, 7, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 2, 4, 2, 1, 3, 5, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 3, 3, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 7, 3, 5, 6, 7, 2, 3, 7, 10], \"Freq\": [0.5958417802226795, 0.0911421461143678, 0.06379950228005746, 0.04329251940432471, 0.08658503880864941, 0.05126745718933189, 0.06493877910648706, 0.003417830479288793, 0.6764645275736569, 0.041893978028241054, 0.016634373628860416, 0.034500923082080866, 0.09795797803662246, 0.025259604399380634, 0.10658320880714267, 0.6256616115700605, 0.04721974426943853, 0.061748896352342694, 0.061748896352342694, 0.04994396028498306, 0.14619959283422315, 0.008172648046633592, 0.9930112509479662, 0.30593832423672745, 0.07021535310351122, 0.020061529458146064, 0.6018458837443819, 0.011672830440800787, 0.023345660881601574, 0.9454992657048638, 0.11271432717278872, 0.20493514031416132, 0.010246757015708065, 0.6660392060210243, 0.9280520098869258, 0.06256530403732084, 0.004109705758948603, 0.09041352669686925, 0.012329117276845807, 0.5609748360964842, 0.2856245502469279, 0.045206763348434625, 0.3718490299823532, 0.5620973709035572, 0.06399262376440497, 0.001729530372010945, 0.9919402570172864, 0.2607265467744302, 0.22458036642615695, 0.031998258013225526, 0.09540221370609833, 0.11080878237913284, 0.0610337143585598, 0.1279930320529021, 0.02310985300955177, 0.03970154234974278, 0.02429497367670827, 0.9280366646180047, 0.06424869216586186, 0.00953806492060329, 0.11445677904723947, 0.05532077653949908, 0.09728826219015356, 0.4711804070778025, 0.005722838952361974, 0.22128310615799632, 0.024798968793568552, 0.002606944723810316, 0.052138894476206316, 0.028676391961913475, 0.005213889447620632, 0.002606944723810316, 0.03910417085715474, 0.8681125930288351, 0.007226774012601786, 0.9900680397264447, 0.0045616222300905685, 0.0045616222300905685, 0.059301088991177385, 0.930570934938476, 0.015304000547358467, 0.9794560350309419, 0.021891118674996272, 0.9632092216998359, 0.016104372919030316, 0.005368124306343439, 0.9125811320783846, 0.05904936736977783, 0.030286947159828735, 0.9631249196825538, 0.13067897932194966, 0.1960184689829245, 0.6642848115532441, 0.4498229452174776, 0.19956746596079245, 0.04617651473028974, 0.13826416192230434, 0.007696085788381623, 0.06714171394691554, 0.08333003232937344, 0.0015922936113893014, 0.006369174445557206, 0.9979811363379886, 0.9886431336509082, 0.9976176038351042, 0.8298090068707183, 0.08298090068707184, 0.01310224747690608, 0.07424606903580112, 0.021233359759752178, 0.42466719519504353, 0.010616679879876089, 0.5414506738736805, 0.033946301974553936, 0.18670466086004664, 0.008486575493638484, 0.755305218933825, 0.016973150987276968, 0.06099817339130651, 0.47464203670110383, 0.4612986862717555, 0.0019061929184783285, 0.9977240850274551, 0.9932937947226228, 0.011572715635959846, 0.7753719476093097, 0.046290862543839385, 0.06943629381575908, 0.09258172508767877, 0.9382866450930895, 0.046914332254654474, 0.009382866450930895, 0.11475127067762964, 0.8303819223581199, 0.01043193369796633, 0.039641348052272055, 0.004172773479186532, 0.15654990950117295, 0.8422385131163105, 0.039044098206565826, 0.9500730563597685, 0.9931481087465015, 0.06986693228574554, 0.019961980653070154, 0.019961980653070154, 0.8883081390616219, 0.009575605838150633, 0.9862874013295152, 0.00035554450053390475, 0.002844356004271238, 0.026665837540042855, 0.9688587639548903, 0.0007110890010678095, 0.09222319918868237, 0.006587371370620169, 0.8431835354393816, 0.019762114111860508, 0.039524228223721015, 0.9986914456670721, 0.11596891591927219, 0.04141746997116864, 0.016566987988467455, 0.820065905429139, 0.990930507044344, 0.14744765641803914, 0.2972735008428209, 0.04815830713653698, 0.2693297917635957, 0.021998239062368745, 0.0725347342056483, 0.03626736710282415, 0.02378188006742567, 0.021998239062368745, 0.06064379417193546, 0.9930495440425551, 0.9985054231526294, 0.0007142385001091769, 0.0361586774395513, 0.960787714822363, 0.0038811428597185734, 0.8473828577052218, 0.03881142859718573, 0.0012937142865728577, 0.0012937142865728577, 0.006468571432864289, 0.09573485720639148, 0.0025874285731457155, 0.42378762433475, 0.27946562530812863, 0.0018989736714029126, 0.048107333008873786, 0.14463849463852185, 0.06899604339430583, 0.0025319648952038835, 0.004747434178507282, 0.024370162116337378, 0.0012659824476019418, 0.0223655900905603, 0.9766307672877997, 0.0260285878884388, 0.057262893354565365, 0.9057948585176703, 0.01041143515537552, 0.9986734380124535, 0.1099715335889437, 0.5539816004543039, 0.26805561312305026, 0.05086183428488646, 0.012371797528756166, 0.0027492883397235925, 0.12486776653864491, 0.5416342081026935, 0.28054705988552686, 0.0008108296528483435, 0.033244015766782084, 0.0008108296528483435, 0.007297466875635092, 0.01135161513987681, 0.04257303384312356, 0.0681168541489977, 0.8259168565565971, 0.05960224738037298, 0.10940672802227329, 0.8851998903620294, 0.12213116171522148, 0.06281031173925676, 0.04187354115950451, 0.746744817344497, 0.020936770579752254, 0.9838348807277059, 0.11714249031366399, 0.023428498062732797, 0.8551401792897471, 0.9976183847454769, 0.3840288000181732, 0.24001800001135823, 0.04014094138120991, 0.18208262069827177, 0.008690306896962971, 0.14235550345501247, 0.002482944827703706, 0.09570625690125448, 0.6038609066388675, 0.004557440804821642, 0.09798497730366529, 0.009114881609643284, 0.05696801006027052, 0.12305090173018432, 0.007975521408437873, 0.23605828241165355, 0.02145984385560487, 0.7296346910905656, 0.01816969109439338, 0.9266542458140624, 0.04542422773598345, 0.00908484554719669, 0.3476634324082479, 0.6373829594151211, 0.11306585421764336, 0.10175926879587903, 0.7829810404571803, 0.993569028334561, 0.03635766511118545, 0.05453649766677817, 0.018178832555592724, 0.014543066044474178, 0.052718614411218896, 0.007271533022237089, 0.814411698490554, 0.07441556985112975, 0.02083635955831633, 0.04762596470472304, 0.8542907418909694, 0.06550385506802055, 0.29662123049669686, 0.08775044735527282, 0.0012359217937362367, 0.07662715121164669, 0.3176319009902129, 0.012359217937362369, 0.06303201148054807, 0.0803349165928554, 0.016157790166841848, 0.0023082557381202636, 0.020774301643082374, 0.9048362493431434, 0.05539813771488633, 0.02173277855314194, 0.2209499152902764, 0.757025119601111, 0.0022379788242082883, 0.0939951106167481, 0.09847106826516468, 0.040283618835749185, 0.0022379788242082883, 0.19246617888191278, 0.020141809417874593, 0.008951915296833153, 0.5415908754584057, 0.974142229437031, 0.020580469635993612, 0.04350987281469225, 0.9463397337195565, 0.9940410065399569, 0.9881056467980265, 0.979833893728631, 0.014845968086797438, 0.05552034160151779, 0.023133475667299078, 0.046266951334598155, 0.8420585142896865, 0.027760170800758894, 0.9861184963476052, 0.2441508455962141, 0.35814774776797587, 0.1786250671825243, 0.13105155682737962, 0.012566587640981609, 0.025133175281963217, 0.0008976134029272578, 0.003590453611709031, 0.007180907223418062, 0.03949498972879934, 0.0036111694430797986, 0.9127925223046321, 0.008055685680716474, 0.07527899377497119, 0.8511019048755143, 0.005463564296633115, 0.1426597344120869, 0.9951014428690478, 0.006030257984580416, 0.9889623094711882, 0.04279571875467657, 0.8987100938482079, 0.008559143750935314, 0.008559143750935314, 0.03851614687920891, 0.14145476571394355, 0.1285952415581305, 0.6987008124658424, 0.0257190483116261, 0.7858840780950643, 0.1480651161628382, 0.05694812160109162, 0.9786573076048295, 0.012546888559036275, 0.0751546201461953, 0.1186651897045189, 0.8029677836672445, 0.9874949290156148, 0.008624409860398382, 0.25548512058435413, 0.7369763093779446, 0.9917539272214799, 0.07134944398868323, 0.04077111085067613, 0.8409041612951952, 0.030578333138007098, 0.010192777712669033, 0.8409880324531008, 0.15786464773010425, 0.8710202867037751, 0.12180494774485509, 0.002298206561223681, 0.002298206561223681, 0.21521855120395575, 0.3743577215074847, 0.24400101491112844, 0.022840277651498323, 0.059793247185223254, 0.005013719484475242, 0.03212494336348951, 0.04623763524571612, 0.0003713866284796475, 0.9931791893089023, 0.006176487495702129, 0.9947382803222886, 0.5072629621618892, 0.07423360421881305, 0.09588507211596685, 0.0907299607118826, 0.03917884667104022, 0.06289235912982771, 0.0752646264996299, 0.012372267369802174, 0.03196169070532228, 0.01031022280816848, 0.004028181877641801, 0.6364527366674045, 0.3544800052324785, 0.9906536203759502, 0.061141288396867395, 0.2254199016652182, 0.5076579703255051, 0.01914525192225141, 0.009881420346968468, 0.14081023994430067, 0.03211461612764752, 0.002470355086742117, 0.9549064932910298, 0.03744731346239333, 0.4264147087761517, 0.17928800255360924, 0.16798155194212036, 0.08964400127680462, 0.017767279532339652, 0.031496540989147566, 0.029881333758934873, 0.05653225305744435, 0.03701797872731692, 0.12339326242438974, 0.2400559832619946, 0.015704597035831422, 0.44309398779667225, 0.030287437140532027, 0.05720960348767161, 0.014582840104700606, 0.013461083173569791, 0.02467865248487795, 0.04739459493122445, 0.15166270377991825, 0.04107531560706119, 0.01579819831040815, 0.742515320589183, 0.16403404225546578, 0.033319414833141485, 0.010252127640966611, 0.08970611685845785, 0.033319414833141485, 0.6561361690218631, 0.002563031910241653, 0.012815159551208264, 0.9964417547166852, 0.016335840920868604, 0.9576886739859218, 0.024503761381302905, 0.003335210077426573, 0.9872221829182656, 0.003335210077426573, 0.993389233258002, 0.05672641034909375, 0.08682613828942921, 0.761754653259259, 0.06714554694382525, 0.020838273189463012, 0.006946091063154337, 0.3719811306286789, 0.006682295759796626, 0.022274319199322087, 0.14032821095572914, 0.2906798655511532, 0.09689328851705108, 0.07016410547786457, 0.0011137159599661044, 0.9743509840997063, 0.01066807646824496, 0.012446089212952453, 0.0017780127447074932, 0.04075170465250043, 0.10519626084715226, 0.25209194040849103, 0.08434655149005903, 0.15163424986976903, 0.05117655933104705, 0.11941197177244312, 0.09761454835366382, 0.03222227809732592, 0.06444455619465184, 0.006830897939990114, 0.5198313332332477, 0.02868977134795848, 0.06762588960590213, 0.1256885220958181, 0.12227307312582304, 0.006147808145991103, 0.013661795879980228, 0.09290021198386555, 0.017077244849975286, 0.009479276756740047, 0.8278568367552974, 0.03791710702696019, 0.12323059783762061, 0.23214894592146723, 0.6472031219628783, 0.119591881232271, 0.2040178943857769, 0.11142515770300122, 0.0031387368367042597, 0.676397788309768, 0.0031387368367042597, 0.002758048412947398, 0.10756388810494853, 0.1489346142991595, 0.03309658095536878, 0.6729638127591652, 0.03309658095536878, 0.9968486891106668, 0.9939834860267852, 0.9863210885658337, 0.006088401781270578, 0.9991306481428451, 0.07941252878345825, 0.4656461915030052, 0.014438641596992409, 0.44037856870826847, 0.9955721825846919, 0.9886624421660976, 0.26184028603186404, 0.7385238836796165, 0.9185281269591252, 0.008465697022664748, 0.07195842469265036, 0.9953559277304652, 0.988314027183335, 0.006610796168450402, 0.19450036429202508, 0.0070727405197100034, 0.7956833084673753, 0.778555693258815, 0.010665146482997467, 0.0159977197244962, 0.0319954394489924, 0.05865830565648607, 0.03732801269049113, 0.0053325732414987335, 0.05332573241498733, 0.0053325732414987335, 0.8839671120702398, 0.05805484131316321, 0.043814974575972235, 0.012049118008392364, 0.0021907487287986115, 0.12320258337855436, 0.004563058643650162, 0.3939440629017973, 0.4121962974763979, 0.03954650824496807, 0.012168156383067097, 0.012168156383067097, 0.08500958101082519, 0.028336527003608395, 0.8047573669024783, 0.028336527003608395, 0.05100574860649511, 0.16589709387228704, 0.0026332872043220166, 0.157997232259321, 0.5345573024773693, 0.05003245688211831, 0.0789986161296605, 0.010533148817288066, 0.9943360919233417, 0.986567952791586, 0.010223502101467214, 0.035924070494465034, 0.06286712336531382, 0.8891207447380097, 0.14386370327023404, 0.851193577682218, 0.23052922358943162, 0.7684307452981054, 0.9006353744801904, 0.08659955523847984, 0.7559362611455162, 0.11570452976717084, 0.038568176589056945, 0.07713635317811389, 0.007713635317811389, 0.15487710502960314, 0.42311786425613224, 0.06546351862076008, 0.22193729483623542, 0.0015966711858721972, 0.08701857963003475, 0.009580027115233184, 0.007185020336424887, 0.030336752531571747, 0.42666516450903186, 0.16862460568634463, 0.03592969806391847, 0.11513835061392055, 0.05471113114278495, 0.04491212257989809, 0.09799008562886856, 0.0065326723752579035, 0.03592969806391847, 0.013881928797423046, 0.8423158744216431, 0.08933653213562881, 0.017016482311548346, 0.046795326356757955, 0.0013017754975107026, 0.5727812189047091, 0.42047348569595694, 0.00520710199004281, 0.16220582704647374, 0.8369820675598045, 0.05958272013202997, 0.07149926415843597, 0.023833088052811986, 0.6256185613863147, 0.2085395204621049, 0.0059582720132029966, 0.5394444556785444, 0.10978167869949323, 0.03312378236622641, 0.05299805178596225, 0.017035088074059297, 0.06530117095056064, 0.15804776157599457, 0.010410331600814013, 0.0018927875637843661, 0.01230311916459838, 0.0933748202390979, 0.05187490013283217, 0.7573735419393497, 0.041499920106265735, 0.05187490013283217, 0.9956609191740395, 0.01459217165410982, 0.02918434330821964, 0.1556498309771714, 0.1556498309771714, 0.05350462939840267, 0.5836868661643928, 0.03410815610936462, 0.9618500022840824, 0.12153772027148979, 0.12771760435309096, 0.1956963292507039, 0.5541296059835721, 0.14218210422902744, 0.24385422345425375, 0.05560194020129564, 0.023829402943412417, 0.5282184319123086, 0.004765880588682484, 0.07389565323112751, 0.2516193761920671, 0.010289268171422818, 0.08044336934021475, 0.08231414537138254, 0.3713490421868053, 0.06641254910645636, 0.03835090863893959, 0.024320088405181205, 0.0052224529613520525, 0.99226606265689, 0.017854474402545308, 0.8570147713221747, 0.017854474402545308, 0.05356342320763592, 0.05356342320763592, 0.11582631930157526, 0.02316526386031505, 0.8455321309014994, 0.7236718719382904, 0.026031362299938507, 0.033840770989920055, 0.005206272459987701, 0.12495053903970482, 0.007809408689981551, 0.07549095066982167, 0.9975744851168364, 0.787328317859127, 0.03250438009510158, 0.05056236903682467, 0.128211721486234, 0.02798472559276248, 0.04197708838914372, 0.3298199802004149, 0.22487725922755564, 0.29483907320946184, 0.02098854419457186, 0.003997817941823211, 0.054969996700069156, 0.8854892156379842, 0.022238313634743895, 0.010108324379429044, 0.06873660578011749, 0.0020216648758858086, 0.0020216648758858086, 0.008086659503543234, 0.9942691192476059, 0.0031266324504641693, 0.1424493449514265, 0.7876610838490641, 0.008379373232436852, 0.050276239394621114, 0.03663206011062109, 0.9585389062279184, 0.9962803016618411, 0.00794904472067701, 0.18812739172268925, 0.02384713416203103, 0.03179617888270804, 0.7021656169931358, 0.022522293375251527, 0.02384713416203103, 0.008018745580924238, 0.2806560953323483, 0.705649611121333, 0.004009372790462119, 0.985181471442195, 0.021652480049285282, 0.963535362193195, 0.17168482973753882, 0.13571277017348307, 0.09156524252668737, 0.09810561699287933, 0.050687902112987654, 0.05722827657917961, 0.017986029782027877, 0.13244258294038708, 0.2436289488656503, 0.99749311508176, 0.003166741055657307, 0.995940062004223, 0.3664916409301207, 0.16606652479646095, 0.10665479394255466, 0.05082208301960658, 0.1281289135283039, 0.02505313951670747, 0.08947549827395525, 0.026484747489090753, 0.031495375392432245, 0.008589647834299703, 0.0015207513114980924, 0.10695950890869917, 0.22202969147872148, 0.3654872318633749, 0.06691305770591606, 0.03193577754145994, 0.08820357606688936, 0.05424013011009863, 0.06285772087525449, 0.06782785518593165, 0.11304642530988608, 0.04521857012395443, 0.6443646242663507, 0.12435106784087469, 0.9963532685138686, 0.2642856781813891, 0.43803750812763104, 0.10745178957201805, 0.0649283154009641, 0.012802766417091513, 0.023319324545416683, 0.0004572416577532683, 0.0649283154009641, 0.024233807860923218, 0.802528705497391, 0.03424862987705129, 0.04998448684758837, 0.10459716692180529, 0.003702554581302842, 0.0055538318719542635, 0.7710720203260321, 0.06897768871926165, 0.056660244305107785, 0.03202535547680005, 0.012317444414153866, 0.024634888828307732, 0.03325709991821544, 0.9972054344185836, 0.9960675999792326, 0.29440959368709646, 0.15170450679073094, 0.0739238062751443, 0.10413614449194242, 0.055282150779673134, 0.025069812562875028, 0.2192001559984714, 0.008999419894365394, 0.026355443976355798, 0.040497389524644274, 0.9889386220042582, 0.9961593007864098, 0.0004493276052261659, 0.0031452932365831614, 0.8595377342498718, 0.05789632406864421, 0.008907126779791417, 0.07125701423833133, 0.9841194266484043, 0.9840005812850328, 0.9956058834854943, 0.98611906846574, 0.9963237614935367, 0.9779522635547507, 0.019461736588154244, 0.5747461657616906, 0.41934269604930213, 0.002466721741466483, 0.7647853054207466, 0.005098568702804978, 0.1172670801645145, 0.0917742366504896, 0.015295706108414934, 0.8001172842681652, 0.013975847760142624, 0.09433697238096271, 0.08385508656085575, 0.010481885820106968, 0.6466617944567405, 0.08386103962695483, 0.005590735975130321, 0.04845304511779612, 0.056839149080491605, 0.020499365242144515, 0.09131535426046192, 0.04565767713023096, 0.02132285125412669, 0.02132285125412669, 0.9559744978933465, 0.06883897650252918, 0.09178530200337225, 0.8375408807807718, 0.006067209176100969, 0.9889550957044579, 0.1071334194277724, 0.17046643533258812, 0.003551384069428919, 0.2296561698230701, 0.10535772739305793, 0.08108993625196032, 0.07221147607838803, 0.08819270439081815, 0.14264726012206158, 0.09496970332502312, 0.7027758046051712, 0.1975369829160481, 0.20373412706240765, 0.03395568784373461, 0.7062783071496799, 0.054329100549975376, 0.015405592057209103, 0.4801409524496837, 0.005135197352403034, 0.4929789458306913, 0.002567598676201517, 0.9976590483255654, 0.9985882221910644, 0.7156572614282063, 0.013674979517736425, 0.2689412638488164, 0.11909808548627018, 0.3591998258265909, 0.060025435085080174, 0.09813682244068664, 0.34776640961990896, 0.007622277471121292, 0.006669492787231131, 0.04849973211319719, 0.3618826165369329, 0.006217914373486819, 0.09824304710109175, 0.007461497248184183, 0.20021684282627558, 0.2760753981828148, 0.0012435828746973638, 0.07335260191925642, 0.0858762168810807, 0.23705414034881653, 0.04919991592145249, 0.18517059264983027, 0.22095234968361388, 0.04114902058885117, 0.009839983184290496, 0.06440716266081052, 0.033098125256249854, 0.9290276644015679, 0.042228530200071265, 0.016891412080028506, 0.0057802950703693705, 0.9595289816813155, 0.03468177042221622, 0.005200930236878932, 0.988176745006997, 0.004237969400990048, 0.027546801106435308, 0.07628344921782085, 0.04873664811138555, 0.6886700276608827, 0.15468588313613674, 0.04668588184344865, 0.0031123921228965764, 0.7656484622325578, 0.18363113525089803, 0.010788764047704534, 0.9817775283411125, 0.9980592319081857, 0.8675411491942608, 0.05226151501170246, 0.06271381801404295, 0.0629154798942795, 0.006990608877142166, 0.027962435508568664, 0.013981217754284332, 0.8808167185199128, 0.13442916341384611, 0.1242835661750653, 0.6036630357074599, 0.05833718412298983, 0.06848278136177068, 0.0076091979290856294, 0.9912040548370534, 0.21129622459979547, 0.0832379066605255, 0.6979178327690214, 0.7462311657606507, 0.25144745802804536, 0.9325464410311549, 0.06286829939535875, 0.9960009542162327, 0.0004474397817682986, 0.0035795182541463887, 0.007943753765976867, 0.015887507531953733, 0.09135316830873397, 0.09929692207471083, 0.007943753765976867, 0.7745159921827446, 0.022740488093730703, 0.05457717142495369, 0.9187157189867204, 0.9896443974377171, 0.006146859611414392, 0.018733657477420354, 0.6041604536468064, 0.03434503870860398, 0.1514303979424812, 0.02810048621613053, 0.1342578785881792, 0.02810048621613053, 0.004654964174907753, 0.027929785049446522, 0.9682325483808127, 0.053277871827271804, 0.09989600967613464, 0.03329866989204488, 0.8124875453658951, 0.04461043516076032, 0.024332964633141992, 0.8151543152102567, 0.0351476155812051, 0.04055494105523665, 0.0013518313685078884, 0.037851278318220875, 0.011441528005612168, 0.9782506444798404, 0.005720764002806084, 0.9933459491204135, 0.8972323815348402, 0.08873726850344574, 0.9931288606572113, 0.9915533486594736, 0.9892407282041168, 0.007609544063108591, 0.988559698938281, 0.01040128726740674, 0.9673197158688267, 0.018202252717961793, 0.12853869006697913, 0.6176534457763933, 0.08012801458720777, 0.02170133728403544, 0.06009601094040583, 0.08012801458720777, 0.01001600182340097, 0.06204807257389616, 0.15428169396752558, 0.15847413130359966, 0.30772490046783635, 0.14002740702487376, 0.09642605872970349, 0.05869412270503691, 0.00838487467214813, 0.012577312008222194, 0.000838487467214813, 0.5615157659525256, 0.10956405189317574, 0.0261459669290533, 0.0037351381327219, 0.0261459669290533, 0.24900920884812666, 0.0149405525308876, 0.0012450460442406334, 0.008715322309684434, 0.06144736382911485, 0.9049520854833278, 0.027930619922324935, 0.9929919622604421, 0.11391761190104319, 0.23339218048018603, 0.1540070850787622, 0.11907764310213573, 0.10042214568280113, 0.10677295331491504, 0.02222782671239867, 0.10359754949885809, 0.029372485298526815, 0.01746472098831324, 0.10213120012528351, 0.4185704923167357, 0.05357702301654217, 0.036834203323872744, 0.3834105709621299, 0.005022845907800828, 0.9850996717951689, 0.8853949035647339, 0.09135026782810747, 0.004684629119390126, 0.002342314559695063, 0.01405388735817038, 0.23456563900568853, 0.2178109505052822, 0.004011685978970528, 0.11515898574927164, 0.05852341898733476, 0.15008425191913272, 0.21828291356163168, 0.0011799076408736849, 0.15055018127113187, 0.8387795813677348, 0.9966817272391936, 0.1603622119500655, 0.8206772023326881, 0.009433071291180322, 0.015458012236273999, 0.015458012236273999, 0.1803434760898633, 0.7883586240499739, 0.02633744951036372, 0.7568551280346626, 0.0623781698929667, 0.09010180095650745, 0.06515053299932078, 0.0861128784544021, 0.6817269544306833, 0.22963434254507228, 0.954298553829577, 0.03817194215318308, 0.9973144408023982, 0.12689159984699314, 0.17039729122310507, 0.02900379425074129, 0.06888401134551056, 0.007250948562685323, 0.5764504107334831, 0.025378319969398627, 0.5615797929623652, 0.23469593027548616, 0.017494085255040426, 0.0579860129240104, 0.03439848224305701, 0.03105691539658862, 0.05700319914563734, 0.0031450040907937842, 0.002162190312420727, 0.0005896882670238346, 0.010788660245209095, 0.007192440163472731, 0.9817680823140277, 0.00520754769965762, 0.18920756642089354, 0.6440000655243258, 0.052075476996576206, 0.02083019079863048, 0.08505661242774114, 0.9921507064238433, 0.18851939234477466, 0.7989631389849974, 0.9883685234233671, 0.9239218025333505, 0.02179060855031487, 0.04793933881069271, 0.8215546495053263, 0.034842719510270535, 0.009169136713229088, 0.07885457573377015, 0.025673582797041447, 0.011002964055874906, 0.01650444608381236, 0.9883664509958388, 0.031085705086090425, 0.04015236906953347, 0.8535616521555662, 0.006476188559602172, 0.06864759873178303, 0.12008310625291818, 0.1959874894646393, 0.08717144009471098, 0.29175747297005306, 0.05514927842726613, 0.03528367813357349, 0.03795219160586056, 0.09458397751773061, 0.06671283680717677, 0.014825074846039281, 0.06487547170910983, 0.9277192454402706, 0.0052687538816435, 0.990525729748978, 0.9956103318947969, 0.9080127929642657, 0.08804972537835304, 0.1766290072624459, 0.1263950694171631, 0.012963596863298782, 0.0032408992158246956, 0.004861348823737043, 0.029168092942422258, 0.0680588835323186, 0.008102248039561739, 0.5703982619851464, 0.9301534308795789, 0.0666378577346564, 0.002776577405610683, 0.9840360545210592, 0.0120191549580906, 0.5846460376042643, 0.011160643889655557, 0.13736177094960686, 0.0120191549580906, 0.03949150914801197, 0.010302132821220516, 0.1339277266758667, 0.05923726372201796, 0.001755569038713201, 0.005266707116139604, 0.3505286180630692, 0.029844673658124418, 0.23114992343057147, 0.08426731385823366, 0.06027453699581991, 0.03745213949254829, 0.03686694981297722, 0.16209754124118558, 0.05537000831494375, 0.8803831322076057, 0.007382667775325834, 0.05537000831494375, 0.9980176671685339, 0.10594779147524215, 0.010594779147524215, 0.8793666692445099, 0.9927240438951987, 0.7498537025197443, 0.14997074050394887, 0.008331707805774937, 0.0916487858635243, 0.015322549929330343, 0.6243939096202115, 0.3562492858569305, 0.0038306374823325858, 0.011701437811662397, 0.7839963333813806, 0.18722300498659836, 0.0011557789398479333, 0.10864322034570573, 0.8622110891265582, 0.006934673639087599, 0.016180905157871066, 0.0034673368195437997, 0.0011557789398479333, 0.006960338631533848, 0.006960338631533848, 0.02784135452613539, 0.11832575673607541, 0.8352406357840617, 0.9872882212759981, 0.9972762721943205, 0.05734894260660391, 0.6636091930192738, 0.24578118259973106, 0.016385412173315402, 0.27693123852408463, 0.7224293178889164, 0.03893367915941624, 0.15573471663766497, 0.7942470548520913, 0.007786735831883248, 0.0525224011020187, 0.07983404967506842, 0.6253667224547027, 0.0567241931901802, 0.026611349891689475, 0.04481911560705596, 0.03711583011209322, 0.03221373934257147, 0.020308661759447233, 0.02381015516624848, 0.5840146256881371, 0.1807064959410646, 0.0007192298345913019, 0.021217280120443407, 0.1136383138654257, 0.012766329563995609, 0.08127297130881711, 0.00017980745864782548, 0.00035961491729565097, 0.005214416300786939, 0.1603161896199396, 0.014574199056358145, 0.8161551471560561, 0.002951618691315524, 0.020661330839208666, 0.9769857868254384, 0.09695483149268885, 0.25705460452164175, 0.12280945322407255, 0.2302055742621279, 0.11883181911155198, 0.05071483493463725, 0.012430106601626776, 0.07756386519415108, 0.031323868636099476, 0.0024860213203253554, 0.013100691085180735, 0.9825518313885552, 0.06699061957451531, 0.05359249565961225, 0.16970956958877212, 0.10941801197170835, 0.571653287035864, 0.02902926848228997, 0.326905369869803, 0.6681802065470699, 0.0035923667018659673, 0.24548287935376759, 0.06376178684513444, 0.07492009954303297, 0.07810818888528968, 0.06216774217400608, 0.03188089342256722, 0.42401588252014405, 0.019128536053540333, 0.42341356787137713, 0.006092281552106146, 0.021322985432371513, 0.5452591989135, 0.01911803923074002, 0.8682776150627759, 0.10674238570496511, 0.004779509807685005, 0.066402610902324, 0.9185694508154819, 0.005533550908526999, 0.39144491484820126, 0.5935219975133441, 0.013980175530292903, 0.12212248717868682, 0.8635804450492853, 0.8967903870422395, 0.0532341325641512, 0.004094933274165477, 0.008189866548330954, 0.032759466193323816, 0.7882635090152134, 0.0802860981404384, 0.047441785264804505, 0.032844312875633885, 0.003649368097292654, 0.047441785264804505, 0.012562169740070012, 0.0696629412858428, 0.295781996607103, 0.19756866954837382, 0.05367472525302642, 0.1004973579205601, 0.10963348136788374, 0.022840308618309112, 0.13704185170985467, 0.8921670731402372, 0.09746362983884944, 0.22474648691191795, 0.2277497807236585, 0.10511528341091932, 0.01601756699594961, 0.013014273184209058, 0.40844795839671505, 0.004004391748987403, 0.0010010979372468507, 0.06972285946506225, 0.092963812620083, 0.02324095315502075, 0.5996165913995354, 0.092963812620083, 0.0185927625240166, 0.1022601938820913, 0.7892375348314645, 0.20938955005732732, 0.06355028925539116, 0.02204805953758469, 0.11802196576001217, 0.12061585511737508, 0.2101050379463953, 0.3255331143490446, 0.08689529347165731, 0.041502229717806476, 0.011672502108133071, 0.3939535526880845, 0.0549303428584693, 0.006866292857308663, 0.4497421821537174, 0.04634747678683347, 0.0008582866071635829, 0.04548919017966989, 0.0017165732143271657, 0.15284533636187297, 0.01841510076649072, 0.30200765257044776, 0.19520006812480162, 0.2983246324171496, 0.03498869145633236, 0.9059385569070912, 0.052345420050123694, 0.000518271485644789, 0.000518271485644789, 0.03472418953820086, 0.006219257827737469, 0.030472808958713658, 0.18518091597987532, 0.18518091597987532, 0.23909434721452255, 0.03516093341390038, 0.011720311137966791, 0.046881244551867166, 0.2648790317180495, 0.0300185457467557, 0.9605934638961824, 0.010006181915585234, 0.033233110516399884, 0.9554519273464968, 0.008308277629099971, 0.9948523515654102, 0.990768285215838, 0.8992625450709375, 0.08370059073352572, 0.0034587020964266827, 0.0069174041928533655, 0.006225663773568029, 0.662348378361483, 0.3311741891807415, 0.05406033644802339, 0.14743728122188196, 0.019658304162917594, 0.776503014435245, 0.9495713768141136, 0.038916859705496465, 0.28210226416737927, 0.014105113208368962, 0.08463067925021377, 0.26235510567566267, 0.0883920427724455, 0.2689374918395682, 0.43978415986386177, 0.05325021878693483, 0.11902990081785433, 0.0394678092185517, 0.0056382584597931, 0.1259211056020459, 0.04573254084054403, 0.024432453325770098, 0.026311872812367798, 0.11965637398005356, 0.7115990014902557, 0.10139600663083231, 0.004567387686073528, 0.025577371042011757, 0.021009983355938226, 0.08495341096096762, 0.011875207983791172, 0.003653910148858822, 0.033798668876944105, 0.8760159179253314, 0.08435708839280968, 0.032445033997234494, 0.5520872144869418, 0.01084650716084365, 0.39372820993862445, 0.03796277506295277, 0.00433860286433746, 0.021780940468868352, 0.1877309630888177, 0.6482422758591772, 0.029041253958491137, 0.06119407084110632, 0.016595002261994934, 0.006223125848248101, 0.010371876413746834, 0.017632189903369618, 0.14578386819401487, 0.8473687338777115, 0.6498801174559234, 0.06284982192744225, 0.019438089255909972, 0.02073396187297064, 0.033692688043577286, 0.20863549134676707, 0.001295872617060665, 0.0032396815426516622, 0.9931233352159212, 0.00442920022143935, 0.13287600664318047, 0.5669376283442368, 0.005905600295252466, 0.10187160509310504, 0.13139960656936736, 0.03248080162388856, 0.023622401181009865, 0.011700396891393261, 0.9857584380998823, 0.009234437683935493, 0.98346761333913, 0.8012818664324608, 0.09811614691009723, 0.0476564142134758, 0.004672197471909392, 0.0004672197471909392, 0.020557668876401328, 0.027565965084265415, 0.1652647664350045, 0.8306729049759437, 0.9917947440140815, 0.9375003963223811, 0.05379100634636613, 0.15515259006811535, 0.0809491774268428, 0.029231647404137678, 0.6633335372477396, 0.029231647404137678, 0.042723176975278145, 0.10925007999865124, 0.19072471592984877, 0.04073731796559877, 0.06110597694839815, 0.516623259654639, 0.03888562169443519, 0.0055550888134907415, 0.03703392542327161, 0.03059600275833575, 0.9668336871634097, 0.9960025975449, 0.01352484379994016, 0.1829074113896669, 0.01481292416183922, 0.677530270358907, 0.038642410856971884, 0.03993049121887095, 0.021253325971334534, 0.011592723257091564, 0.26834638307716724, 0.016429370392479627, 0.7119393836741171, 0.47243221868084356, 0.4355695299573071, 0.023887022292851643, 0.04511993099760866, 0.022707416253698473, 0.05542887546466571, 0.8706952520907905, 0.0011547682388472022, 0.06697655785313773, 0.003464304716541607, 0.9328465603881578, 0.06396662128375939, 0.34728569329639347, 0.574817699249203, 0.05987684367179198, 0.9460997203355083, 0.04452233978049451, 0.9901333298041717, 0.99782632513502, 0.11669754441093518, 0.12312126245190408, 0.02248301314339118, 0.06637841975667873, 0.21198269535197398, 0.3083384659665076, 0.10385010832899735, 0.047107265633771996, 0.04978281096240003, 0.15892205037996934, 0.0727594929450462, 0.013403064489876932, 0.10722451591901545, 0.07084476944649236, 0.003829446997107695, 0.5208047916066465, 0.2452338182614405, 0.09482374306109033, 0.01961870546091524, 0.10463309579154795, 0.5297050474447115, 0.006539568486971747, 0.043657165966558185, 0.7312575299398496, 0.21828582983279093, 0.07326787260106023, 0.11323216674709308, 0.01332143138201095, 0.7659823044656296, 0.0266428627640219, 0.9891734925432157, 0.05703179712700073, 0.8934981549896781, 0.04752649760583394, 0.0528922217910834, 0.8545399583121912, 0.011570173516799495, 0.006611527723885425, 0.019834583171656275, 0.006611527723885425, 0.04628069406719798, 0.009581427087190396, 0.02326918006889096, 0.2607516943013957, 0.015056528279870621, 0.25527659310871553, 0.1793095640602774, 0.027375505963401128, 0.04790713543595197, 0.17794078876210734, 0.003421938245425141, 0.03636839261300744, 0.06754130056701382, 0.10390969318002126, 0.005195484659001063, 0.7793226988501595, 0.010390969318002126, 0.9546457717669796, 0.02952512696186535, 0.00984170898728845, 0.9293503593262826, 0.0682092006844978, 0.21172821197568364, 0.29827761964241806, 0.13984819543890423, 0.1305575810565994, 0.1975478005500605, 0.0019559188173273317, 0.02004816787760515, 0.7022777552751571, 0.006423272151906925, 0.21625016244753315, 0.05995054008446463, 0.006423272151906925, 0.008564362869209233, 0.9922525178271954, 0.013444196467100766, 0.0403325894013023, 0.015684895878284227, 0.34282700991106957, 0.5108794657498291, 0.033610491167751916, 0.0403325894013023, 0.5639866771773026, 0.13411878298728538, 0.0630472911478692, 0.06075466237885577, 0.06190097676336248, 0.028657859612667813, 0.08024200691546989, 0.003438943153520138, 0.00458525753802685, 0.9875666359720267, 0.011416955329156379, 0.01650891485559614, 0.00825445742779807, 0.9740259764801721, 0.01776198404971526, 0.9769091227343395, 0.9907413874149169, 0.7068308123051629, 0.26446050800533305, 0.01923349149129695, 0.9951007340374116, 0.0026750019732188486, 0.0214593430240177, 0.9120220785207522, 0.024141760902019912, 0.0026824178780022125, 0.005364835756004425, 0.029506596658024336, 0.033870848330918364, 0.2327255062737294, 0.1245573132169256, 0.04042649639496708, 0.28298547476476954, 0.004370432042699144, 0.015296512149447003, 0.07211212870453587, 0.14640947343042132, 0.04807475246969058, 0.01477338404177639, 0.01899435091085536, 0.0021104834345394845, 0.8547457909884911, 0.08864030425065834, 0.0021104834345394845, 0.016883867476315876, 0.0021104834345394845, 0.018753085001016558, 0.945691286479835, 0.016074072858014193, 0.005358024286004731, 0.008037036429007096, 0.005358024286004731, 0.08878722562495267, 0.07869776816757168, 0.0665904192187145, 0.004035782982952394, 0.004035782982952394, 0.7567093093035739, 0.15960499392286576, 0.17679322403763592, 0.21055581890593444, 0.04665376745437615, 0.13811970627940306, 0.09944618852117021, 0.029465537339605986, 0.023326883727188073, 0.06998065118156421, 0.046039902093134355, 0.02844547841627241, 0.9576644400145043, 0.0031606087129191565, 0.006321217425838313, 0.9974026145593295, 0.003902767210457262, 0.9913028714561446, 0.001951383605228631, 0.002974598233209468, 0.002974598233209468, 0.9905412116587529, 0.027717256445697478, 0.748365924033832, 0.21249896608368066, 0.996539921408939, 0.0020504936654504916, 0.0026165993844939252, 0.9943077661076917, 0.0833273497061245, 0.22725640828943044, 0.07575213609647681, 0.05302649526753377, 0.5605658071139284, 0.06908550945717318, 0.13654547751535406, 0.11378789792946171, 0.018693726088411567, 0.529113725371997, 0.04632792987128084, 0.0633961145607001, 0.01137878979294617, 0.010566019093450016, 0.001910115133061902, 0.0878652961208475, 0.9092148033374653, 0.23643945163130062, 0.13066390748045562, 0.02986603599553271, 0.03982138132737695, 0.0049776726659221185, 0.27003874212627493, 0.27003874212627493, 0.0049776726659221185, 0.013688599831285826, 0.005530950826166554, 0.011061901652333108, 0.12721186900183074, 0.027654754130832772, 0.8241116730988166, 0.018852032476936773, 0.9312904043606766, 0.0414744714492609, 0.2241891066891301, 0.041709601244489324, 0.6934221206896349, 0.03649590108892816, 0.9974829178565223, 0.028207268277544415, 0.9696248470405893, 0.992421663729286, 0.008478491727264456, 0.2162015390452436, 0.06358868795448341, 0.3264219314996816, 0.002119622931816114, 0.38153212772690054, 0.002119622931816114, 0.13349804560784015, 0.2917578254816507, 0.3132897683216249, 0.035527705685957464, 0.1571831827318118, 0.06782561994591879, 0.04262579729021404, 0.10612953611032884, 0.1217879922577544, 0.43060754405420304, 0.07307279535465264, 0.07394270958506517, 0.033056740755676196, 0.05045502536392683, 0.06785330997217745, 0.8995224533871211, 0.007721222775855117, 0.08879406192233384, 0.0038606113879275585, 0.19717348878120514, 0.5812203587207166, 0.027957434976439533, 0.07062930941416302, 0.007357219730641983, 0.08534374887544699, 0.032371766814824725, 0.013834878879680835, 0.10030287187768605, 0.8646799299800522, 0.017293598599601043, 0.04235434859731221, 0.7279653665163036, 0.021177174298656103, 0.1455930733032607, 0.010588587149328052, 0.05029578895930825, 0.02942434927270524, 0.8716963472038928, 0.011034130977264465, 0.025746305613617085, 0.00735608731817631, 0.00735608731817631, 0.04781456756814601, 0.050492545283401295, 0.9425275119568242, 0.009497913436742613, 0.9782850839844891, 0.9936927815756851, 0.22089591659826505, 0.7731357080939276, 0.29595904254963984, 0.007398976063740997, 0.6955037499916537, 0.9912909718960454, 0.20373188042513565, 0.12387717352483123, 0.14947163086467238, 0.12797228669920582, 0.07371203713874255, 0.07780715031311713, 0.03890357515655857, 0.03685601856937128, 0.05323647126686962, 0.11466316888248841, 0.07589345378425225, 0.10841921969178893, 0.11112970018408365, 0.0738605934150312, 0.04607816836901029, 0.09622205747646267, 0.2391999034450093, 0.06640677206122071, 0.10367587883027315, 0.07860393427654697, 0.9963890950347261, 0.008237247221284202, 0.9884696665541042, 0.9909940935246155, 0.007654001616086073, 0.07909135003288942, 0.4107647533966193, 0.4745481001973365, 0.02551333872028691, 0.07520011867843024, 0.19349244019506207, 0.11829232151663183, 0.16307441466221387, 0.2408093688017148, 0.011829232151663182, 0.08111473475426183, 0.007604506383212046, 0.030418025532848184, 0.07773495413950092, 0.9957179935536954, 0.9902108730530882, 0.05907180264348449, 0.9377648669653162, 0.4468366744667201, 0.11147724646730284, 0.12322796870241381, 0.07406047303444946, 0.08859426106208672, 0.037262158666601916, 0.0906042530233557, 0.014224558495134343, 0.01329686989762558, 0.00046384429875438076, 0.022442044699068898, 0.9537868997104282, 0.016831533524301674, 0.11490877191766674, 0.09978919666534217, 0.1663153277755703, 0.6199025853453074, 0.9928219198081532, 0.9928978031197602, 0.9548379506688168, 0.039456113664000696]}, \"lambda.step\": 0.01, \"R\": 30, \"tinfo\": {\"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.6701, 1.6661, 1.666, 1.6646, 1.6521, 1.6053, 1.6036, 1.6003, 1.5967, 1.5754, 1.5743, 1.5684, 1.5641, 1.553, 1.5516, 1.5512, 1.5449, 1.5371, 1.5051, 1.4786, 1.4541, 1.4527, 1.45, 1.436, 1.4345, 1.422, 1.4148, 1.4058, 1.3954, 1.3852, 1.3346, 1.3527, 1.2837, 1.243, 1.1366, 1.0974, 1.2382, 1.2053, 0.8688, 0.9244, 1.3209, 0.8755, 1.1557, 0.8161, 0.8223, 1.0571, 1.1017, 1.0801, 0.7177, 0.8532, 1.0967, 0.1381, 0.2245, 0.7369, 0.8219, 0.3428, 0.9965, 0.6712, 1.6763, 1.6734, 1.6711, 1.6706, 1.6689, 1.6661, 1.6646, 1.6529, 1.6391, 1.6312, 1.6296, 1.6254, 1.6147, 1.6084, 1.6076, 1.5897, 1.5682, 1.5421, 1.5373, 1.5311, 1.5243, 1.5237, 1.5146, 1.5076, 1.4958, 1.481, 1.4408, 1.4393, 1.4369, 1.4231, 1.401, 1.3615, 1.3546, 1.1592, 1.2771, 1.143, 1.1764, 0.8493, 1.0675, 1.026, 0.6977, 0.8546, 1.198, 1.1368, 1.0891, 0.8202, 0.2307, 0.4059, 1.1034, 0.1567, -0.0309, 0.4714, 0.069, 0.0507, 0.2532, 0.2252, 0.468, -0.514, 0.3212, 0.2007, 0.6539, -0.1003, 1.9386, 1.9384, 1.9371, 1.9365, 1.9363, 1.9359, 1.9357, 1.9356, 1.9355, 1.935, 1.9345, 1.9341, 1.9334, 1.933, 1.9322, 1.9321, 1.9316, 1.9313, 1.9308, 1.9303, 1.9291, 1.9288, 1.9288, 1.928, 1.927, 1.9152, 1.8978, 1.8971, 1.8956, 1.8955, 1.8484, 1.7784, 1.7908, 1.7988, 1.8558, 1.8029, 1.736, 1.8683, 1.8309, 1.8011, 1.4701, 1.7523, 1.5059, 1.4348, 1.3728, 0.8919, 1.7535, -0.1542, 1.4358, 0.4353, 0.5948, 0.8299, 0.6681, 0.0698, 0.3808, 0.7804, -0.4986, 0.5629, 2.1664, 2.1661, 2.1656, 2.1656, 2.1655, 2.1652, 2.1645, 2.1644, 2.164, 2.163, 2.1617, 2.1611, 2.1609, 2.1606, 2.1602, 2.1599, 2.1597, 2.1593, 2.1581, 2.1578, 2.1577, 2.1576, 2.1571, 2.1569, 2.1564, 2.1562, 2.1549, 2.1527, 2.1456, 2.1437, 2.1268, 2.0093, 2.0112, 2.0609, 1.8963, 1.9932, 1.7783, 2.0659, 1.4904, 1.9941, 1.3692, 0.7571, 0.9359, 1.3251, 1.5897, 1.2368, 0.8552, 0.698, 0.9906, 0.1896, 0.4638, 0.0068, 0.6956, -0.4349, 0.0404, -0.6782, 2.4006, 2.3998, 2.3976, 2.3958, 2.3942, 2.3916, 2.3897, 2.3891, 2.3889, 2.3827, 2.3778, 2.3613, 2.3451, 2.3054, 2.2939, 2.2775, 2.2488, 2.1847, 2.1066, 2.0873, 2.0531, 2.0514, 1.9982, 1.9881, 1.9688, 1.9643, 1.9622, 1.9334, 1.8905, 1.8521, 1.846, 1.8503, 1.7669, 1.7765, 1.5906, 1.3976, 1.6494, 1.5188, 1.0383, 0.9396, 0.229, 0.4699, -0.0197, 1.0664, 1.1693, 0.9793, 1.1409, 0.9116, 0.1055, 0.2723, -0.4337, 0.4246, 2.4513, 2.448, 2.4449, 2.4253, 2.4219, 2.4215, 2.4075, 2.4032, 2.3754, 2.3519, 2.3467, 2.2826, 2.2789, 2.2572, 2.2239, 2.2131, 2.1928, 2.1869, 2.1644, 2.1629, 2.1176, 2.1093, 2.1046, 2.1002, 2.0924, 2.0908, 2.0799, 2.0622, 2.0488, 2.0461, 2.0437, 2.0301, 1.8155, 1.8612, 1.7918, 1.4637, 1.6808, 1.3975, 1.7447, 0.5571, 1.2329, 1.4952, 1.305, 1.3324, 0.5054, 0.9453, 0.7346, -0.3635, 0.4164, 0.2157, 1.1443, -0.2483, -0.8375, -0.2214, 2.6819, 2.6814, 2.6784, 2.6778, 2.6766, 2.6763, 2.6746, 2.6723, 2.6704, 2.6576, 2.64, 2.6389, 2.6344, 2.6319, 2.6191, 2.6008, 2.589, 2.5051, 2.4655, 2.412, 2.4047, 2.3823, 2.3689, 2.3453, 2.3196, 2.3001, 2.2389, 2.2308, 2.2126, 2.2065, 2.1308, 1.7893, 1.8178, 1.8252, 1.1629, 1.5082, 1.2546, 1.0629, 1.1679, 1.3704, 0.2837, 1.1173, 0.1737, 1.3974, 1.3762, 0.1988, 0.7395, 0.0978, -0.1815, 0.3637, 0.6303, 3.0812, 3.0803, 3.0791, 3.0765, 3.0757, 3.075, 3.0747, 3.0745, 3.0718, 3.0717, 3.0701, 3.0525, 3.0439, 3.0349, 3.014, 3.0093, 2.9822, 2.9705, 2.9596, 2.9583, 2.9572, 2.9358, 2.9131, 2.8952, 2.8871, 2.8446, 2.8372, 2.8307, 2.8291, 2.8206, 2.8032, 2.7778, 2.6869, 2.5228, 2.4115, 2.3396, 2.1182, 0.7252, 0.815, 1.1238, 1.8713, 0.6534, -0.3527, 1.0758, 0.5296, 3.1017, 3.1008, 3.1004, 3.0987, 3.0982, 3.098, 3.0967, 3.0963, 3.0891, 3.0888, 3.0875, 3.0855, 3.0818, 3.0729, 3.0722, 3.0697, 3.0674, 3.0551, 3.0506, 3.0323, 3.0169, 2.9803, 2.9651, 2.9412, 2.9281, 2.9156, 2.9037, 2.9017, 2.8797, 2.8758, 2.828, 2.8067, 2.4917, 2.4681, 2.5983, 1.3798, 2.5712, 1.1595, 0.3996, 0.0319, 1.6918, 0.8423, 1.1811, 0.7281, 3.4118, 3.4106, 3.4093, 3.4082, 3.4081, 3.4074, 3.4071, 3.4062, 3.4051, 3.4041, 3.401, 3.396, 3.3946, 3.3792, 3.3501, 3.3418, 3.3282, 3.2561, 3.2504, 3.2437, 3.2361, 3.2089, 3.1596, 3.1461, 3.0571, 2.8511, 2.8363, 2.8074, 2.8047, 2.7623, 2.591, 1.5946, 1.2921, 2.0868, 1.4274, 0.6503, 0.8678, 1.2465], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"Term\": [\"bank\", \"china\", \"kong\", \"hong\", \"percent\", \"beijing\", \"tonne\", \"market\", \"chinese\", \"profit\", \"price\", \"gold\", \"company\", \"bre\", \"share\", \"sale\", \"official\", \"quarter\", \"computer\", \"million\", \"canada\", \"corp\", \"service\", \"thomson\", \"government\", \"north\", \"deng\", \"financial\", \"internet\", \"ford\", \"franc\", \"quaker\", \"snapple\", \"kellogg\", \"cereal\", \"kmart\", \"item\", \"stg\", \"abnormal\", \"profit\", \"exceptional\", \"quarter\", \"pretax\", \"fourth\", \"margin\", \"earning\", \"retailer\", \"jones\", \"expectation\", \"net\", \"growth\", \"sale\", \"income\", \"pre\", \"forecast\", \"earn\", \"half\", \"improvement\", \"exclude\", \"outlook\", \"result\", \"food\", \"1996\", \"rise\", \"percent\", \"million\", \"increase\", \"1997\", \"year\", \"share\", \"store\", \"analyst\", \"1995\", \"billion\", \"expect\", \"fall\", \"strong\", \"revenue\", \"business\", \"report\", \"loss\", \"company\", \"market\", \"pound\", \"cost\", \"group\", \"continue\", \"good\", \"electric\", \"ce\", \"wmx\", \"hilton\", \"zealand\", \"lloyd\", \"electricity\", \"casino\", \"itt\", \"prudential\", \"steel\", \"telecom\", \"auction\", \"northern\", \"bzw\", \"takeover\", \"st\", \"shareholder\", \"pounds\", \"hotel\", \"float\", \"stake\", \"bid\", \"australian\", \"australia\", \"melbourne\", \"club\", \"franchise\", \"partnership\", \"attractive\", \"merger\", \"venture\", \"penny\", \"pound\", \"plc\", \"offer\", \"buy\", \"share\", \"british\", \"deal\", \"company\", \"group\", \"london\", \"value\", \"britain\", \"executive\", \"million\", \"billion\", \"acquisition\", \"market\", \"percent\", \"stock\", \"analyst\", \"new\", \"business\", \"make\", \"base\", \"year\", \"plan\", \"price\", \"chief\", \"expect\", \"deng\", \"beijing\", \"tung\", \"jiang\", \"handover\", \"hong\", \"human\", \"kong\", \"democracy\", \"taiwan\", \"xiaoping\", \"legislature\", \"li\", \"dissident\", \"wang\", \"colony\", \"xinhua\", \"zemin\", \"patten\", \"liberty\", \"mao\", \"chee\", \"hwa\", \"activist\", \"democratic\", \"lee\", \"corruption\", \"territory\", \"provisional\", \"freedom\", \"china\", \"chinese\", \"party\", \"political\", \"diplomat\", \"communist\", \"leader\", \"sino\", \"civil\", \"visit\", \"people\", \"death\", \"right\", \"law\", \"rule\", \"official\", \"army\", \"year\", \"july\", \"government\", \"state\", \"foreign\", \"british\", \"make\", \"tell\", \"united\", \"new\", \"day\", \"internet\", \"software\", \"apple\", \"microsoft\", \"att\", \"online\", \"mci\", \"aol\", \"intel\", \"pc\", \"3com\", \"windows\", \"mainframe\", \"computer\", \"web\", \"netscape\", \"server\", \"device\", \"robotics\", \"modem\", \"cellular\", \"sprint\", \"hp\", \"calif\", \"distance\", \"nynex\", \"wireless\", \"subscriber\", \"ibm\", \"phone\", \"bell\", \"network\", \"technology\", \"user\", \"customer\", \"communications\", \"service\", \"machine\", \"corp\", \"communication\", \"product\", \"company\", \"new\", \"use\", \"access\", \"revenue\", \"base\", \"plan\", \"long\", \"analyst\", \"business\", \"market\", \"industry\", \"year\", \"make\", \"million\", \"textile\", \"deficit\", \"fcc\", \"apec\", \"moscow\", \"gazprom\", \"surcharge\", \"russian\", \"russia\", \"cnb\", \"czech\", \"crown\", \"wage\", \"norilsk\", \"prague\", \"oil\", \"budget\", \"economist\", \"klaus\", \"soviet\", \"gas\", \"inflation\", \"penalty\", \"quota\", \"debate\", \"minister\", \"cabinet\", \"output\", \"prime\", \"skoda\", \"export\", \"import\", \"trade\", \"economy\", \"country\", \"government\", \"agreement\", \"economic\", \"state\", \"official\", \"percent\", \"billion\", \"year\", \"rate\", \"cut\", \"world\", \"talk\", \"foreign\", \"make\", \"plan\", \"market\", \"tell\", \"banking\", \"scandal\", \"nomura\", \"suez\", \"bank\", \"loan\", \"yen\", \"mgam\", \"natwest\", \"bond\", \"lending\", \"markets\", \"banker\", \"bankruptcy\", \"pension\", \"probe\", \"watchdog\", \"japanese\", \"tokyo\", \"brokerage\", \"investigation\", \"suspend\", \"institution\", \"fund\", \"client\", \"trust\", \"japan\", \"debt\", \"abn\", \"amro\", \"securities\", \"credit\", \"financial\", \"finance\", \"security\", \"firm\", \"asset\", \"investment\", \"insurance\", \"market\", \"foreign\", \"management\", \"capital\", \"problem\", \"business\", \"issue\", \"state\", \"company\", \"stock\", \"make\", \"trading\", \"analyst\", \"year\", \"billion\", \"crop\", \"tonne\", \"corn\", \"grain\", \"bean\", \"harvest\", \"lme\", \"ivory\", \"cocoa\", \"sugar\", \"index\", \"zinc\", \"rain\", \"cathay\", \"season\", \"port\", \"trader\", \"exporter\", \"coast\", \"farmer\", \"arrival\", \"coffee\", \"buying\", \"shanghai\", \"weather\", \"metal\", \"grade\", \"copper\", \"factory\", \"yuan\", \"mid\", \"price\", \"export\", \"point\", \"market\", \"source\", \"week\", \"stock\", \"high\", \"rate\", \"year\", \"rise\", \"percent\", \"investor\", \"trading\", \"analyst\", \"chinese\", \"china\", \"million\", \"expect\", \"add\", \"ford\", \"gm\", \"chrysler\", \"uaw\", \"automaker\", \"thomson\", \"detroit\", \"lagardere\", \"hughes\", \"csf\", \"bskyb\", \"tv\", \"film\", \"auto\", \"truck\", \"aerospatiale\", \"car\", \"electronics\", \"automotive\", \"optus\", \"broadcasting\", \"vehicle\", \"channel\", \"satellite\", \"broadcast\", \"mercury\", \"cable\", \"station\", \"radio\", \"sport\", \"television\", \"digital\", \"defence\", \"plant\", \"strike\", \"worker\", \"union\", \"new\", \"make\", \"corp\", \"production\", \"government\", \"company\", \"offer\", \"plan\", \"conrail\", \"attorney\", \"norfolk\", \"csx\", \"flight\", \"tobacco\", \"cigarette\", \"airbus\", \"jet\", \"plane\", \"aviation\", \"airline\", \"boeing\", \"gec\", \"lawsuit\", \"airlines\", \"alcatel\", \"747\", \"smoke\", \"aircraft\", \"airport\", \"judge\", \"air\", \"florida\", \"passenger\", \"trial\", \"philip\", \"lawyer\", \"morris\", \"drug\", \"cargo\", \"court\", \"case\", \"southern\", \"542\", \"state\", \"file\", \"industry\", \"new\", \"company\", \"general\", \"week\", \"talk\", \"deal\", \"bre\", \"busang\", \"gold\", \"freeport\", \"hwang\", \"toronto\", \"barrick\", \"pyongyang\", \"ounce\", \"strathcona\", \"indonesian\", \"walsh\", \"mining\", \"seoul\", \"xinjiang\", \"nickel\", \"korean\", \"canadian\", \"embassy\", \"riot\", \"indonesia\", \"canada\", \"korea\", \"ethnic\", \"kill\", \"north\", \"town\", \"police\", \"arrest\", \"south\", \"deposit\", \"official\", \"report\", \"project\", \"president\", \"government\", \"week\", \"wednesday\"], \"Total\": [2812.0, 3599.0, 2234.0, 2225.0, 5561.0, 1400.0, 975.0, 4237.0, 1647.0, 1929.0, 1997.0, 631.0, 5385.0, 613.0, 3390.0, 2140.0, 1708.0, 1445.0, 809.0, 5087.0, 550.0, 1619.0, 1552.0, 512.0, 1972.0, 617.0, 785.0, 1258.0, 682.0, 473.0, 319.83292435014016, 114.05290387891559, 112.10611405430978, 120.05600604566001, 134.71671152106313, 95.43760619748765, 118.4033632312299, 234.57246001178237, 95.89979769651465, 1929.4906775661902, 115.47403416174333, 1445.628984689283, 133.38308886601862, 494.64182314679726, 426.92814074049306, 912.929891825971, 154.10678874388552, 95.67269526879184, 235.0662097350975, 545.3076070712391, 1080.3351880885693, 2140.3204937554337, 286.20804037430213, 274.0200421935696, 553.7715208638176, 187.52672578744506, 811.8567183066878, 196.13347554772577, 129.64055971001397, 120.02341216369499, 1094.7176687552833, 384.1520042162228, 1623.1449769262942, 1543.3615724795982, 5561.504553371283, 5087.43376418365, 1073.2039621778308, 1101.2342570786716, 6467.687558209244, 3390.9626326358734, 467.0516722710196, 3768.1492641077075, 877.7497942566953, 3159.601468074781, 2449.2273729506196, 1056.6426144523464, 872.3610324669569, 921.9557827888063, 2416.485430145043, 1596.237573034256, 803.1831470216111, 5385.2235019538475, 4237.6198159863225, 1573.6569224277166, 1238.2312081011632, 2187.02732579893, 969.9111441197276, 1397.0304989783479, 284.61201629782323, 163.97713869709256, 117.05417898852356, 117.29747167211002, 109.78806755300336, 131.41391806219306, 195.6276802362052, 145.76926829469613, 173.00154885278172, 120.36189023071069, 101.60836916551789, 373.2719176402468, 213.154477947576, 360.15563548823843, 110.07341784787633, 372.79799251292314, 105.2044701772058, 865.9746314102764, 114.63900157483299, 224.53929863641562, 168.0251085729146, 605.0038912412368, 772.9681973668792, 319.38696201945345, 479.2975247699986, 106.0100119178548, 175.59841692492827, 119.34066812168771, 85.45958335165756, 86.41014187652763, 721.4062239596719, 377.76522434854195, 415.26553888574216, 1573.6569224277166, 556.7360367083763, 1164.807347007038, 877.6855633030087, 3390.9626326358734, 1233.304673166706, 1463.9363796459345, 5385.2235019538475, 2187.02732579893, 599.041424491542, 679.6045494163467, 727.4609836671681, 1252.6060579639511, 5087.43376418365, 3159.601468074781, 578.191638714785, 4237.6198159863225, 5561.504553371283, 2045.0746547169101, 3768.1492641077075, 3372.66425426231, 2416.485430145043, 2519.364611060389, 1681.9528097271202, 6467.687558209244, 2011.2458244507857, 1997.8065337945454, 1114.0653612555734, 2449.2273729506196, 785.6830350056172, 1400.0925459032833, 352.88824870946974, 287.55808355310313, 266.74543761896985, 2225.547659144283, 378.6638932668509, 2234.9376178576767, 219.31953907142224, 373.83149994341613, 176.75687407421444, 166.10527293749368, 156.0723951748089, 136.6345406814393, 121.76801748336337, 139.14741975020345, 112.09733504314788, 107.76537088086799, 102.30042030630615, 101.86037910773689, 86.2856850262701, 84.16838372611016, 84.16833489401651, 85.69064457126748, 164.24671628542092, 174.80182708279722, 106.81674144707397, 316.3947488698765, 99.93821903661771, 163.79095202075086, 3599.9418484536423, 1647.2763037759414, 865.2173573361449, 627.6794317225336, 236.2475286613151, 435.1218976015583, 739.7372359422096, 187.59784023557145, 233.66823343531846, 271.8836677017357, 1427.9621347531534, 316.4798409189721, 964.1456956376812, 640.5583113956034, 677.3231847769337, 1708.8476350659175, 228.96835106250106, 6467.687558209244, 394.2596878092379, 1972.7091322017006, 1461.1602084533822, 1000.5458122927413, 1233.304673166706, 2519.364611060389, 1629.0217092182802, 928.852549379327, 3372.66425426231, 1055.170584069337, 682.9641936929534, 536.1654493607462, 414.9886674097121, 415.1148154105867, 392.89419377824584, 345.6852632466879, 274.9122337769545, 263.5320352497416, 240.56314670107713, 188.5134593509791, 160.11903173122417, 136.49286275584956, 154.07979703250723, 809.5215935398913, 144.25633245351105, 115.34183488840412, 112.4495059310837, 113.28436807472428, 93.6439581089697, 90.71202531760565, 90.0713403353235, 100.08355535838908, 88.40390469304538, 83.53722553039536, 302.5354207024066, 102.63851566816601, 242.79955988600236, 175.17805249639915, 411.0630088822265, 338.7971498291008, 193.59115143805616, 772.0590520154877, 473.82508842965825, 259.0263301629064, 863.7951828514178, 348.3997259097021, 1552.6981539034791, 179.01500267108293, 1619.2004224280022, 196.21736649020764, 1165.1119703530544, 5385.2235019538475, 3372.66425426231, 1149.53861546302, 486.65284507172737, 921.9557827888063, 1681.9528097271202, 2011.2458244507857, 1192.6236695244593, 3768.1492641077075, 2416.485430145043, 4237.6198159863225, 1689.4821519444477, 6467.687558209244, 2519.364611060389, 5087.43376418365, 332.8645776075929, 237.74922171130933, 207.90210403328769, 124.41294114467753, 104.21214107794893, 101.50414202736805, 80.74761084599412, 216.58059412532074, 341.86874489209634, 79.70103466646314, 562.4256648196936, 489.7207336158748, 178.2441338515494, 181.7155014538363, 244.20422337743562, 541.809562847823, 256.0983629396231, 176.4506991051972, 123.28619363709134, 91.62298814962105, 249.4155700310273, 263.24184581729514, 122.05979189569099, 138.89971351268278, 142.1501177574308, 576.0868979072895, 86.2903521149505, 261.0531548892669, 215.1374759309236, 83.50473561042936, 768.1816118925517, 405.3963538690396, 1230.3593136660934, 379.7534877163035, 891.4587218034163, 1972.7091322017006, 524.215345735322, 657.4537463319094, 1461.1602084533822, 1708.8476350659175, 5561.504553371283, 3159.601468074781, 6467.687558209244, 1063.444141029657, 897.8950072965056, 1183.5087705191083, 915.2413218922018, 1000.5458122927413, 2519.364611060389, 2011.2458244507857, 4237.6198159863225, 1629.0217092182802, 563.7376813856118, 169.38988738743984, 379.59639886920155, 121.14666636142027, 2812.587449667612, 384.5677844639786, 178.2368787531181, 104.7890092662327, 229.4566481911195, 192.09647566861918, 101.42300018678762, 92.9922493735616, 151.8056207457839, 120.72200459083038, 128.423516809886, 124.17047552221035, 99.59441685837298, 321.29627646960523, 108.23582073779481, 286.57714794862113, 219.37875637100626, 207.97056019755843, 147.2507352232178, 754.807679518123, 233.29012517495624, 191.80236111839983, 471.92412468404626, 637.1990084074848, 97.59185257023474, 91.8280817792124, 444.72348137860604, 390.1629144780004, 1258.948873844673, 485.4459987253864, 540.0453711404915, 1069.0750611934825, 524.605872945052, 1049.5550746230106, 389.46896540677113, 4237.6198159863225, 1000.5458122927413, 597.2709605406751, 809.1126842071159, 771.0429106480137, 2416.485430145043, 1117.8880892359111, 1461.1602084533822, 5385.2235019538475, 2045.0746547169101, 2519.364611060389, 803.5883973216219, 3768.1492641077075, 6467.687558209244, 3159.601468074781, 350.2462621101521, 975.3748737188133, 146.3680109955818, 134.4904505606435, 122.85389055550681, 110.43427173245414, 101.15726961902311, 192.27329620942928, 231.89992502369492, 112.60003355492606, 281.3882594073247, 126.72307370611472, 128.47902009148544, 91.93315772344191, 130.13327832028722, 180.71578567372293, 523.5286515933763, 154.1251658785182, 252.81213534231233, 96.38572772567994, 117.8331590580438, 203.53435801295927, 93.19732303073776, 365.19963070200413, 135.15383633967184, 278.70395730306836, 88.45923232501794, 248.25095548700128, 167.833895093089, 330.6971205577532, 275.8259809333578, 1997.8065337945454, 768.1816118925517, 627.3349913664211, 4237.6198159863225, 934.0385056961501, 1475.7531040607325, 2045.0746547169101, 1555.6558271900956, 1063.444141029657, 6467.687558209244, 1543.3615724795982, 5561.504553371283, 804.1281528931944, 803.5883973216219, 3768.1492641077075, 1647.2763037759414, 3599.9418484536423, 5087.43376418365, 2449.2273729506196, 1687.5918675848138, 473.14762661027675, 344.8645357033907, 254.24543579251346, 143.08434125308952, 177.214252788678, 512.4569035634777, 124.55149126211298, 162.68469807624254, 99.59343710144883, 299.83118807664243, 82.33088863457182, 283.61484427645684, 146.59250368058497, 153.6723928993451, 265.223392019768, 140.08067240911237, 433.2275594446712, 111.34595676222982, 100.190458790591, 94.38611093971502, 100.54226279174159, 289.1243237318662, 216.1369986900792, 229.93406773696483, 117.44523583694765, 194.0741121267944, 353.7761269905853, 192.47482489771613, 203.47635110587984, 150.13401658178927, 495.5667855403096, 297.8915169322263, 362.5752163397837, 447.8238922186749, 446.28922335987676, 391.9518378066493, 471.78202546770433, 3372.66425426231, 2519.364611060389, 1619.2004224280022, 543.0325973668649, 1972.7091322017006, 5385.2235019538475, 1164.807347007038, 2011.2458244507857, 299.5762864413441, 178.19501233210386, 165.72748867117528, 184.2178210446453, 191.48090129300232, 336.17985408437545, 165.83038446398737, 138.37432833187205, 92.6890230964653, 152.66370201358032, 104.4320345785174, 261.3695672332171, 268.2692464498123, 92.36817193446703, 214.8244245123147, 91.36125155103983, 165.08761921808278, 85.66902475552428, 89.84253791963609, 219.2202575223213, 186.28480693308717, 143.0490558940855, 383.59079533470043, 86.33616314754119, 143.67117074871834, 180.80073958876432, 137.22881046608728, 150.156147864468, 111.39437560669643, 282.77581998469867, 276.08066705914007, 316.4917860732199, 446.83175246475446, 305.83057643397086, 199.38659254994062, 1461.1602084533822, 205.58968679312812, 1689.4821519444477, 3372.66425426231, 5385.2235019538475, 611.5857770341009, 1475.7531040607325, 915.2413218922018, 1463.9363796459345, 613.8142626682696, 356.84987911567663, 631.5641111315521, 180.67204550742574, 174.64202573987166, 382.17543194653365, 141.28135021049968, 126.6519597624087, 109.79889191795083, 100.78079743146128, 164.82042582923438, 105.286282788334, 278.0697446962616, 163.4200401762538, 135.42840478869974, 154.14146111859094, 219.87214959464495, 335.95120013208447, 83.41228348236777, 109.75151227779588, 174.3198491563731, 550.0903300263634, 251.7701402787696, 117.12181032668774, 156.17884352881117, 617.1126797878749, 132.0094787461062, 328.28423684860485, 94.1914055349356, 522.2686203805824, 277.03437149054287, 1708.8476350659175, 1596.237573034256, 426.60983493885186, 875.6449106807479, 1972.7091322017006, 1475.7531040607325, 976.7739814934146], \"Freq\": [2812.0, 3599.0, 2234.0, 2225.0, 5561.0, 1400.0, 975.0, 4237.0, 1647.0, 1929.0, 1997.0, 631.0, 5385.0, 613.0, 3390.0, 2140.0, 1708.0, 1445.0, 809.0, 5087.0, 550.0, 1619.0, 1552.0, 512.0, 1972.0, 617.0, 785.0, 1258.0, 682.0, 473.0, 318.47403245601635, 113.1144441358806, 111.16893690459574, 118.88366077824077, 131.74647301448238, 89.07043817097096, 110.31547353151824, 217.81703583353635, 88.73000397167154, 1747.7495955040026, 104.4821542971233, 1300.2383725001332, 119.4571764210999, 438.12366682611116, 377.61308937735055, 807.1501657633628, 135.3870242144249, 83.40335567038439, 198.46134037947644, 448.34885773520585, 866.746760275293, 1714.6930353803255, 228.69554913129386, 215.9024837084441, 435.6673242667129, 145.70114291466734, 626.2560805276286, 149.9410243204633, 98.08503393895381, 89.882632243645, 779.3537013255941, 278.48807953842424, 1098.2485958887683, 1002.5481193190628, 3248.2007100222013, 2857.178180895623, 693.8571678972107, 688.8865886741064, 2889.9395369924464, 1601.8835351564069, 327.9697151115917, 1695.1103680185993, 522.510803267366, 1339.3976126052696, 1044.694212000322, 569.9693690062562, 492.04520916758054, 508.8728953578891, 928.313710791973, 702.2020232959561, 450.7302853047784, 1158.7528064816768, 994.1674461675658, 616.256616968883, 527.9147374952195, 577.5046458904578, 492.42594327436154, 512.2919739418885, 283.44289406865505, 162.83157713822334, 115.96028145629427, 116.14196730022913, 108.52890573019685, 129.53487334444597, 192.55596378044413, 141.80333579775305, 165.9881148256029, 114.58153481351324, 96.57143639856639, 353.26972781793796, 199.58747587150972, 335.104777251589, 102.34056297020184, 340.44336927621146, 94.03444590070812, 754.11403327137, 99.35248229713798, 193.3835015437953, 143.73992265472867, 517.2472937211319, 654.8130200051503, 268.6744060765535, 398.46995511458715, 86.84462645554291, 138.17259079753086, 93.76780986713757, 66.98923903734776, 66.8012965464013, 545.5178989667588, 274.6117020073591, 299.77634113547623, 934.3817866249781, 371.93958279004886, 680.5266532974917, 530.204974985021, 1476.9449891893114, 668.1573991189144, 760.8153042015684, 2015.6653841697746, 957.6555678472769, 369.77813087824103, 394.5780541645202, 402.7008050814876, 529.9458552318501, 1193.636722369636, 883.3091171874763, 324.6794425568201, 923.3138501333157, 1004.5558304525479, 610.3888691113567, 752.1128122785929, 660.991471284984, 579.879586536935, 587.8670575048202, 500.30160608173003, 720.6039799075462, 516.6141755685667, 454.8986174187927, 399.09625409668894, 412.72109025614253, 784.7599106098986, 1398.1836061867912, 351.9553990017953, 286.6350502365728, 265.816298531261, 2217.030918894332, 377.12652234210964, 2225.683057675861, 218.40062368114621, 372.07075400318803, 175.8346522518228, 165.16832547565335, 155.09176934230675, 135.71234152032986, 120.85545475927536, 138.084710572008, 111.18812553213552, 106.85410613668626, 101.38893181426053, 100.9025779332096, 85.37402165768837, 83.25446400962669, 83.2539623485095, 84.69328299014451, 162.16256658624997, 170.56968343768014, 102.42612696962516, 303.17537676922456, 95.6190644404599, 156.70897895875999, 3285.683098300834, 1401.8669634200269, 745.5159341533007, 545.1830778740862, 217.22657463399855, 379.4646954647631, 603.363025924053, 174.66738033787027, 209.57407492339246, 236.68699972881922, 892.8393993005925, 262.3969637719596, 624.7630192403988, 386.61476618738624, 384.2043386315603, 599.2825703755678, 190.054107418991, 796.8284286497721, 238.18092411064012, 438.2333441305885, 380.70810226080613, 329.78131381800495, 345.7905862464657, 388.29849822387064, 342.687283122766, 291.37569563456805, 294.4528635262526, 266.2994993981469, 681.9825439137699, 535.2463282403387, 414.0651165586211, 414.17966518954245, 391.98326891969174, 344.7735527420441, 273.9956827121149, 262.61723410297884, 239.64794380155917, 187.60358231048338, 159.13361557747484, 135.57558028086808, 153.0146153967613, 803.6679566895718, 143.16025465305668, 114.4314179668739, 111.53953521531832, 112.32162364998, 92.73282701515677, 89.80183043776567, 89.15884396179361, 99.0654114330201, 87.46096361340146, 82.6273852362759, 299.0913771749799, 101.45271548809397, 239.6698353900957, 172.54789260862955, 402.03087666818834, 330.7243364049014, 185.8133490048289, 658.859190309909, 405.13718392819976, 232.75403858730743, 658.3862385857839, 292.57203851598706, 1051.7785243125443, 161.67226266663505, 822.4051898071515, 164.92712600338993, 524.2523770397228, 1313.8544943119425, 983.928134760442, 494.9060681494057, 272.977069562999, 363.3905119121178, 452.64689362158214, 462.5156535031356, 367.47066771581365, 521.2054723060131, 439.654673811354, 488.2090034764342, 387.5962036034425, 479.0582899163769, 300.1634843848493, 295.4613651843559, 331.7909046831512, 236.78265193952208, 206.6048362375901, 123.40862346313365, 103.21194946160101, 100.26989188209978, 79.61019484053826, 213.40056373634488, 336.7780174674494, 78.03114859261255, 547.9498844091937, 469.3157443818051, 168.07236645354132, 164.6724030990808, 218.76299129108233, 477.46791434294016, 219.30867451614458, 141.71680164340668, 91.5862520531379, 66.7584113368147, 175.6189039243058, 185.04399961692764, 81.35898273555365, 91.65306501959047, 92.00388500192706, 371.19606452111833, 55.480748969046, 163.08382012651848, 128.7504838738737, 48.09197711847263, 439.7183528937762, 233.05066724096167, 650.7310640764381, 202.7833991460415, 395.2746568086256, 721.1827544538694, 246.5071193485066, 271.3085910123623, 372.9490276233348, 395.17221701696764, 631.9201897872655, 456.8114174917005, 573.0415878783155, 279.1645975011761, 261.2524819425434, 284.778058078735, 258.85315091667115, 224.97366677512306, 253.00160811622564, 238.63186271240087, 248.1992473482101, 225.08498531330937, 562.5648578957745, 168.4741780937874, 376.3756277917934, 117.7840529362325, 2725.392962240616, 372.47628457184913, 170.23674575559207, 99.65335409421179, 212.24371312062587, 173.55285761967357, 91.16172126567601, 78.39024944427177, 127.5033980832299, 99.21298042555657, 102.0839605377684, 97.64173732584318, 76.74890973593732, 246.13018515046576, 81.07046565683358, 214.32045432360206, 156.809519144306, 147.42689088115617, 103.8934768132241, 530.1845327733275, 162.60158198891182, 133.46958639246873, 324.8388586521638, 430.9114834122417, 65.11901187300678, 61.105088122389844, 295.2326986678936, 255.5061550154103, 665.2435558480613, 268.50639071719894, 278.6675713219372, 397.37283990863, 242.25612544218458, 365.1268006551806, 191.72333241768573, 636.1421771305877, 295.2500022968093, 229.09166711327134, 256.59138440733204, 251.32277810099328, 344.49282460690665, 247.4119168251771, 261.9567167691191, 321.9796782713126, 266.72228059118305, 268.82048995467295, 217.01230612011113, 252.8119476077526, 240.72232312013222, 217.75428037370702, 349.23962428768056, 972.1214143466634, 145.43743633517056, 133.55662876633164, 121.85203166240737, 109.50874128622337, 100.136160633568, 189.89787443473256, 228.59248067859923, 109.5795676978388, 269.06222553529284, 121.04077835510218, 122.170577067807, 87.19737941084956, 121.86257506308925, 166.15652971414715, 475.717662020832, 128.78222572531863, 203.0328615135149, 73.37841618321515, 89.05171388023898, 150.41836604302162, 67.95797840419385, 260.09129564071327, 93.81358060520493, 189.70433738253922, 56.63759090897642, 157.66232929144195, 104.66725050395522, 204.99638245846896, 158.50462737776272, 815.9645857815779, 322.79953508480475, 265.5804731600035, 925.068162143391, 288.01260053623906, 353.11347375784, 403.97470709677754, 341.30830552677486, 285.6871843796105, 586.1232786747453, 321.8935447742363, 451.51315780890457, 221.94858015775242, 217.14210311748943, 313.69137697834543, 235.47542685492667, 270.88851060086233, 289.5417693849517, 240.44348213850688, 216.28700290188442, 472.2193253750165, 343.8858044897589, 253.20728204506605, 142.13791241290775, 175.88965663945328, 508.3073232132198, 123.50567822758173, 161.28863441662847, 98.46553466618232, 296.4185271166456, 81.26412434847919, 275.0566980728802, 140.94777004033796, 146.42970178969486, 247.49410840364874, 130.10922912708207, 391.6365076225791, 99.48347024710333, 88.54173654528603, 83.30417571944582, 88.64571160937388, 249.51376661372637, 182.3378802154529, 190.5354034704563, 96.53617395249269, 152.8876962311059, 276.6263528852663, 149.53096038302363, 157.83296303954108, 115.46108654472069, 374.56655962562814, 219.50797630318192, 243.9518041316155, 255.71638075319925, 227.99999049774735, 186.3339249946108, 179.74321732810532, 319.08742261772437, 260.7555955914767, 228.22821173750594, 161.6268417063159, 173.717348726286, 173.38727066760015, 156.47688198111592, 156.47592751071556, 298.27010300270564, 177.25678203300288, 164.78754373312242, 182.87764430220727, 189.97786315355168, 333.4908718430126, 164.29695150525544, 137.03881850839565, 91.13382185266703, 150.05857628770696, 102.51315549254814, 256.05672761886456, 261.85469295557243, 89.36018412736215, 207.68150755105785, 88.09927256626062, 158.83473734201624, 81.41742590201326, 84.99483580976677, 203.63147909621412, 170.40211254225403, 126.14641902861722, 333.1668579247596, 73.21531952482123, 120.25282272614501, 149.45339461131863, 112.0858609769029, 122.40412368980418, 88.8306064274908, 224.6112017263047, 209.05902149634838, 234.61795950683452, 241.74584192833467, 161.59354603591984, 119.9983177752535, 260.0101866178834, 120.42773282042414, 241.2100465658746, 225.21269006765817, 248.94856092417197, 148.6757221511512, 153.4204021811275, 133.5183324570755, 135.76149876910645, 612.7093590008526, 355.77339128112305, 628.8615418016814, 179.70037252172764, 173.6826724392718, 379.79280551618876, 140.35918785970884, 125.71979042054716, 108.8668418640278, 99.82743922009713, 162.75159240319147, 103.44942549436065, 272.8372176092498, 157.8979859842042, 127.09885948875903, 143.45946438916602, 201.87795513356085, 286.983285059333, 70.84907092917747, 92.60487477194857, 145.964191356442, 448.26824562883235, 195.2903712893251, 89.6288846034662, 109.34024826992047, 351.5990931873597, 74.10827285524215, 179.0411644021857, 51.233863911115186, 272.27879996809423, 121.6940431580195, 277.14688563809324, 191.30378289828587, 113.1924107202904, 120.14819791096824, 124.43949292158939, 115.71667425366893, 111.84611054922793], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.6906, -6.7257, -6.7431, -6.676, -6.5733, -6.9647, -6.7508, -6.0705, -6.9685, -3.9881, -6.8051, -4.2838, -6.6712, -5.3716, -5.5203, -4.7606, -6.546, -7.0304, -6.1635, -5.3486, -4.6894, -4.0071, -6.0217, -6.0793, -5.3773, -6.4726, -5.0144, -6.4439, -6.8683, -6.9556, -4.7957, -5.8248, -4.4527, -4.5438, -3.3683, -3.4965, -4.9119, -4.9191, -3.4851, -4.0752, -5.6612, -4.0186, -5.1955, -4.2542, -4.5027, -5.1086, -5.2556, -5.2219, -4.6208, -4.8999, -5.3433, -4.399, -4.5522, -5.0305, -5.1852, -5.0954, -5.2548, -5.2152, -5.801, -6.3553, -6.6948, -6.6932, -6.761, -6.5841, -6.1877, -6.4936, -6.3361, -6.7068, -6.8778, -5.5808, -6.1518, -5.6336, -6.8198, -5.6178, -6.9044, -4.8225, -6.8494, -6.1834, -6.4801, -5.1995, -4.9637, -5.8546, -5.4604, -6.9839, -6.5196, -6.9072, -7.2435, -7.2463, -5.1463, -5.8327, -5.745, -4.6082, -5.5293, -4.9252, -5.1748, -4.1503, -4.9435, -4.8137, -3.8394, -4.5836, -5.5352, -5.4702, -5.4499, -5.1753, -4.3633, -4.6644, -5.6652, -4.6201, -4.5358, -5.034, -4.8252, -4.9543, -5.0852, -5.0716, -5.2328, -4.868, -5.2008, -5.328, -5.4589, -5.4253, -4.5234, -3.9458, -5.3253, -5.5306, -5.606, -3.4848, -5.2562, -3.4809, -5.8024, -5.2697, -6.0192, -6.0818, -6.1448, -6.2782, -6.3942, -6.2609, -6.4775, -6.5173, -6.5698, -6.5746, -6.7417, -6.7669, -6.7669, -6.7497, -6.1002, -6.0496, -6.5596, -5.4745, -6.6284, -6.1344, -3.0914, -3.9432, -4.5747, -4.8876, -5.8078, -5.25, -4.7862, -6.0259, -5.8437, -5.722, -4.3944, -5.6189, -4.7514, -5.2313, -5.2376, -4.793, -5.9415, -4.5081, -5.7157, -5.106, -5.2467, -5.3903, -5.3429, -5.227, -5.3519, -5.5142, -5.5036, -5.6041, -4.4357, -4.6779, -4.9347, -4.9344, -4.9895, -5.1178, -5.3476, -5.39, -5.4815, -5.7263, -5.8909, -6.0511, -5.9301, -4.2715, -5.9967, -6.2207, -6.2463, -6.2393, -6.431, -6.4631, -6.4703, -6.3649, -6.4895, -6.5463, -5.2599, -6.3411, -5.4814, -5.81, -4.9641, -5.1594, -5.7359, -4.4702, -4.9564, -5.5107, -4.4709, -5.282, -4.0024, -5.8751, -4.2484, -5.8552, -4.6987, -3.78, -4.0691, -4.7563, -5.3513, -5.0652, -4.8456, -4.824, -5.054, -4.7045, -4.8747, -4.7699, -5.0007, -4.7889, -5.2563, -5.2721, -4.9201, -5.2575, -5.3938, -5.9091, -6.0879, -6.1168, -6.3475, -5.3615, -4.9052, -6.3675, -4.4185, -4.5734, -5.6002, -5.6207, -5.3366, -4.5561, -5.3342, -5.7708, -6.2074, -6.5236, -5.5563, -5.504, -6.3258, -6.2066, -6.2028, -4.8079, -6.7086, -5.6304, -5.8668, -6.8515, -4.6385, -5.2734, -4.2465, -5.4125, -4.7451, -4.1437, -5.2172, -5.1214, -4.8032, -4.7453, -4.2759, -4.6004, -4.3737, -5.0928, -5.1592, -5.0729, -5.1684, -5.3087, -5.1912, -5.2497, -5.2104, -5.3082, -4.3426, -5.5483, -4.7445, -5.9063, -2.7647, -4.7549, -5.5379, -6.0734, -5.3174, -5.5186, -6.1625, -6.3134, -5.827, -6.0778, -6.0493, -6.0938, -6.3346, -5.1692, -6.2798, -5.3076, -5.6201, -5.6818, -6.0317, -4.4019, -5.5838, -5.7812, -4.8918, -4.6092, -6.4989, -6.5625, -4.9873, -5.1319, -4.175, -5.0822, -5.0451, -4.6902, -5.1851, -4.7749, -5.4191, -4.2197, -4.9873, -5.241, -5.1276, -5.1484, -4.833, -5.1641, -5.1069, -4.9006, -5.0889, -5.0811, -5.2952, -5.1425, -5.1915, -5.2917, -4.588, -3.5643, -5.464, -5.5492, -5.641, -5.7478, -5.8372, -5.1973, -5.0118, -5.7471, -4.8488, -5.6476, -5.6383, -5.9756, -5.6409, -5.3308, -4.2789, -5.5856, -5.1304, -6.1481, -5.9545, -5.4303, -6.2249, -4.8827, -5.9025, -5.1983, -6.4071, -5.3833, -5.793, -5.1208, -5.378, -3.7394, -4.6667, -4.8618, -3.6139, -4.7808, -4.577, -4.4424, -4.611, -4.7889, -4.0702, -4.6695, -4.3312, -5.0413, -5.0632, -4.6954, -4.9822, -4.8421, -4.7755, -4.9613, -5.0672, -3.8879, -4.205, -4.5111, -5.0886, -4.8755, -3.8143, -5.2291, -4.9622, -5.4556, -4.3536, -5.6476, -4.4284, -5.097, -5.0588, -4.534, -5.177, -4.075, -5.4454, -5.5619, -5.6229, -5.5607, -4.5258, -4.8395, -4.7955, -5.4754, -5.0157, -4.4227, -5.0379, -4.9838, -5.2964, -4.1196, -4.654, -4.5484, -4.5013, -4.616, -4.8178, -4.8538, -4.2799, -4.4818, -4.615, -4.9601, -4.8879, -4.8898, -4.9924, -4.9925, -4.3245, -4.8449, -4.9178, -4.8137, -4.7756, -4.2129, -4.9208, -5.1022, -5.5101, -5.0114, -5.3925, -4.4771, -4.4547, -5.5298, -4.6865, -5.544, -4.9546, -5.6229, -5.5799, -4.7062, -4.8843, -5.185, -4.2138, -5.7291, -5.2329, -5.0155, -5.3032, -5.2151, -5.5357, -4.6081, -4.6799, -4.5645, -4.5346, -4.9374, -5.235, -4.4618, -5.2314, -4.5368, -4.6054, -4.5052, -5.0207, -4.9893, -5.1282, -5.1116, -3.297, -3.8406, -3.271, -4.5236, -4.5577, -3.7753, -4.7707, -4.8808, -5.0248, -5.1115, -4.6227, -5.0758, -4.106, -4.6529, -4.8699, -4.7488, -4.4072, -4.0555, -5.4543, -5.1866, -4.7315, -3.6095, -4.4404, -5.2192, -5.0204, -3.8524, -5.4094, -4.5273, -5.7785, -4.1081, -4.9134, -4.0904, -4.461, -4.9858, -4.9262, -4.8911, -4.9638, -4.9978]}, \"topic.order\": [9, 1, 3, 7, 5, 2, 8, 4, 10, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el13432476860674764248518930798\", ldavis_el13432476860674764248518930798_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el13432476860674764248518930798\", ldavis_el13432476860674764248518930798_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el13432476860674764248518930798\", ldavis_el13432476860674764248518930798_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics         x         y\n",
       "topic                                                \n",
       "8      18.742531        1       1 -0.199457 -0.028758\n",
       "0      18.628928        1       2 -0.072734  0.048877\n",
       "2      14.374059        1       3  0.090754 -0.027557\n",
       "6      11.442473        1       4 -0.074860  0.088662\n",
       "4       9.036709        1       5 -0.024172 -0.020448\n",
       "1       8.600028        1       6 -0.050096  0.016569\n",
       "7       6.823840        1       7 -0.144744 -0.134166\n",
       "3       4.581432        1       8  0.162894  0.173882\n",
       "9       4.477801        1       9  0.091465  0.112647\n",
       "5       3.292198        1      10  0.220951 -0.229708, topic_info=     Category         Freq        Term        Total  loglift  logprob\n",
       "term                                                                 \n",
       "183   Default  2812.000000        bank  2812.000000  30.0000  30.0000\n",
       "287   Default  3599.000000       china  3599.000000  29.0000  29.0000\n",
       "766   Default  2234.000000        kong  2234.000000  28.0000  28.0000\n",
       "673   Default  2225.000000        hong  2225.000000  27.0000  27.0000\n",
       "990   Default  5561.000000     percent  5561.000000  26.0000  26.0000\n",
       "196   Default  1400.000000     beijing  1400.000000  25.0000  25.0000\n",
       "1384  Default   975.000000       tonne   975.000000  24.0000  24.0000\n",
       "844   Default  4237.000000      market  4237.000000  23.0000  23.0000\n",
       "288   Default  1647.000000     chinese  1647.000000  22.0000  22.0000\n",
       "1065  Default  1929.000000      profit  1929.000000  21.0000  21.0000\n",
       "1049  Default  1997.000000       price  1997.000000  20.0000  20.0000\n",
       "630   Default   631.000000        gold   631.000000  19.0000  19.0000\n",
       "331   Default  5385.000000     company  5385.000000  18.0000  18.0000\n",
       "219   Default   613.000000         bre   613.000000  17.0000  17.0000\n",
       "1238  Default  3390.000000       share  3390.000000  16.0000  16.0000\n",
       "1200  Default  2140.000000        sale  2140.000000  15.0000  15.0000\n",
       "937   Default  1708.000000    official  1708.000000  14.0000  14.0000\n",
       "1097  Default  1445.000000     quarter  1445.000000  13.0000  13.0000\n",
       "340   Default   809.000000    computer   809.000000  12.0000  12.0000\n",
       "873   Default  5087.000000     million  5087.000000  11.0000  11.0000\n",
       "248   Default   550.000000      canada   550.000000  10.0000  10.0000\n",
       "369   Default  1619.000000        corp  1619.000000   9.0000   9.0000\n",
       "1230  Default  1552.000000     service  1552.000000   8.0000   8.0000\n",
       "1373  Default   512.000000     thomson   512.000000   7.0000   7.0000\n",
       "632   Default  1972.000000  government  1972.000000   6.0000   6.0000\n",
       "924   Default   617.000000       north   617.000000   5.0000   5.0000\n",
       "426   Default   785.000000        deng   785.000000   4.0000   4.0000\n",
       "575   Default  1258.000000   financial  1258.000000   3.0000   3.0000\n",
       "727   Default   682.000000    internet   682.000000   2.0000   2.0000\n",
       "593   Default   473.000000        ford   473.000000   1.0000   1.0000\n",
       "...       ...          ...         ...          ...      ...      ...\n",
       "956   Topic10   108.866842       ounce   109.798892   3.4051  -5.0248\n",
       "1317  Topic10    99.827439  strathcona   100.780797   3.4041  -5.1115\n",
       "708   Topic10   162.751592  indonesian   164.820426   3.4010  -4.6227\n",
       "1449  Topic10   103.449425       walsh   105.286283   3.3960  -5.0758\n",
       "874   Topic10   272.837218      mining   278.069745   3.3946  -4.1060\n",
       "1224  Topic10   157.897986       seoul   163.420040   3.3792  -4.6529\n",
       "1491  Topic10   127.098859    xinjiang   135.428405   3.3501  -4.8699\n",
       "917   Topic10   143.459464      nickel   154.141461   3.3418  -4.7488\n",
       "768   Topic10   201.877955      korean   219.872150   3.3282  -4.4072\n",
       "249   Topic10   286.983285    canadian   335.951200   3.2561  -4.0555\n",
       "495   Topic10    70.849071     embassy    83.412283   3.2504  -5.4543\n",
       "1179  Topic10    92.604875        riot   109.751512   3.2437  -5.1866\n",
       "707   Topic10   145.964191   indonesia   174.319849   3.2361  -4.7315\n",
       "248   Topic10   448.268246      canada   550.090330   3.2089  -3.6095\n",
       "767   Topic10   195.290371       korea   251.770140   3.1596  -4.4404\n",
       "521   Topic10    89.628885      ethnic   117.121810   3.1461  -5.2192\n",
       "761   Topic10   109.340248        kill   156.178844   3.0571  -5.0204\n",
       "924   Topic10   351.599093       north   617.112680   2.8511  -3.8524\n",
       "1389  Topic10    74.108273        town   132.009479   2.8363  -5.4094\n",
       "1011  Topic10   179.041164      police   328.284237   2.8074  -4.5273\n",
       "151   Topic10    51.233864      arrest    94.191406   2.8047  -5.7785\n",
       "1274  Topic10   272.278800       south   522.268620   2.7623  -4.1081\n",
       "430   Topic10   121.694043     deposit   277.034371   2.5910  -4.9134\n",
       "937   Topic10   277.146886    official  1708.847635   1.5946  -4.0904\n",
       "1145  Topic10   191.303783      report  1596.237573   1.2921  -4.4610\n",
       "1070  Topic10   113.192411     project   426.609835   2.0868  -4.9858\n",
       "1041  Topic10   120.148198   president   875.644911   1.4274  -4.9262\n",
       "632   Topic10   124.439493  government  1972.709132   0.6503  -4.8911\n",
       "1466  Topic10   115.716674        week  1475.753104   0.8678  -4.9638\n",
       "1465  Topic10   111.846111   wednesday   976.773981   1.2465  -4.9978\n",
       "\n",
       "[548 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "23        1  0.595842      1995\n",
       "23        2  0.091142      1995\n",
       "23        3  0.063800      1995\n",
       "23        4  0.043293      1995\n",
       "23        5  0.086585      1995\n",
       "23        6  0.051267      1995\n",
       "23        7  0.064939      1995\n",
       "23        9  0.003418      1995\n",
       "25        1  0.676465      1996\n",
       "25        2  0.041894      1996\n",
       "25        3  0.016634      1996\n",
       "25        4  0.034501      1996\n",
       "25        5  0.097958      1996\n",
       "25        6  0.025260      1996\n",
       "25        7  0.106583      1996\n",
       "27        1  0.625662      1997\n",
       "27        2  0.047220      1997\n",
       "27        3  0.061749      1997\n",
       "27        4  0.061749      1997\n",
       "27        5  0.049944      1997\n",
       "27        7  0.146200      1997\n",
       "27        9  0.008173      1997\n",
       "48        4  0.993011      3com\n",
       "56        1  0.305938       542\n",
       "56        2  0.070215       542\n",
       "56        7  0.020062       542\n",
       "56        9  0.601846       542\n",
       "63        4  0.011673       747\n",
       "63        7  0.023346       747\n",
       "63        9  0.945499       747\n",
       "...     ...       ...       ...\n",
       "1484      6  0.011829     world\n",
       "1484      7  0.081115     world\n",
       "1484      8  0.007605     world\n",
       "1484      9  0.030418     world\n",
       "1484     10  0.077735     world\n",
       "1489      3  0.995718  xiaoping\n",
       "1490      3  0.990211    xinhua\n",
       "1491      3  0.059072  xinjiang\n",
       "1491     10  0.937765  xinjiang\n",
       "1492      1  0.446837      year\n",
       "1492      2  0.111477      year\n",
       "1492      3  0.123228      year\n",
       "1492      4  0.074060      year\n",
       "1492      5  0.088594      year\n",
       "1492      6  0.037262      year\n",
       "1492      7  0.090604      year\n",
       "1492      8  0.014225      year\n",
       "1492      9  0.013297      year\n",
       "1492     10  0.000464      year\n",
       "1493      1  0.022442       yen\n",
       "1493      6  0.953787       yen\n",
       "1493      7  0.016832       yen\n",
       "1496      3  0.114909      yuan\n",
       "1496      5  0.099789      yuan\n",
       "1496      6  0.166315      yuan\n",
       "1496      7  0.619903      yuan\n",
       "1497      2  0.992822   zealand\n",
       "1498      3  0.992898     zemin\n",
       "1499      7  0.954838      zinc\n",
       "1499     10  0.039456      zinc\n",
       "\n",
       "[1813 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[9, 1, 3, 7, 5, 2, 8, 4, 10, 6])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(lda, tf_large, vectorizer, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Neural Networks</span><a id='nn_ml'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Interested? Check out the Stanford course CS224n ([Syllabus](http://web.stanford.edu/class/cs224n/syllabus.html))!   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
